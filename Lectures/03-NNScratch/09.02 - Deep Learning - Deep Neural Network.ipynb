{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.2 Deep Learning - Deep Neural Network\n",
    "\n",
    "### Readings\n",
    "\n",
    "- [WEIDMAN] Ch3\n",
    "- [CHARU] Ch2-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap, last time, we have inputted our data into a linear function which we got some decent result.  To improve, we inserted a non-linear function in between follow by a linear function, which obviously increase the result since it help model the non-linearity.  We can summarize that neural newtork has basically the following key things that make it work:\n",
    "\n",
    "1. **Activation function**: these functions help model input data into a non-linear relationship\n",
    "\n",
    "2. **Chain rule / Backpropagation**: they are essential for us to improve the neural network\n",
    "\n",
    "3. **Layers of neurons**: they are performing some sequential processes that spit out desired output.\n",
    "\n",
    "Putting together, the typial procedure of training a neural network is as followss:\n",
    "\n",
    "1. Feed observations/samples/records (X) into the model.  This step we called \"**forward pass**\"\n",
    "\n",
    "2. Calculate the loss \n",
    "\n",
    "3. Calculate gradients based on how each parameters (e.g., W, B) affect the loss by using chain rule.  This step was called \"**backward pass**\"\n",
    "\n",
    "4. Update the parameters (e.g., W, B) so that the loss will be hopefully be reduced in the next iteration.   This step was called \"**training**\"\n",
    "\n",
    "5. Stop when the loss does not decrease further by some tolerance level (e.g., 0.00001) or when it exceeds the specified maximum iteration.  We called this \"**early stopping**\"\n",
    "\n",
    "In fact, you are now very close to understanding Deep Neural Networks.  In this lesson, we have several objectives:\n",
    "\n",
    "- From our low-level understandings of neural network, we shall code them up as a Python class, so they are resuable.  They will be essential for understanding deep neural network, CNN, and RNN.  You will be so surprised that all these fancy terms are simply layers after layers.\n",
    "\n",
    "- When we code our work, we want to make sure these classes resemble PyTorch as much as possible, so you will understand PyTorch right away.\n",
    "\n",
    "- Of course, we shall also understand what is \"deep\" neural network.  Here, we shall simply say that \"deep\" neural network is simply neural network that has more than \"one\" hidden layers (which we did not yet define what is \"hidden\" layers).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Operations\n",
    "\n",
    "Let's first code up the first building block, the class <code>Operation</code>,  which is the operations/functions.  \n",
    "\n",
    "Each function has a **forward** and **backward** methods.  Forward methods for running the function and backward for calculating its gradients.\n",
    "\n",
    "Each of these functions receives an <code>ndarray</code> as input and outputs an <code>ndarray</code>.  In some operations such as matrix multiplication, we receive <code>ndarray</code> as <code>params</code>, thus we probably should have another class inheriting from <code>Operation</code> and allow for params as another instance variable.\n",
    "\n",
    "We also need to note that the shape of the output may vary.  For example, in matrix multiplication, the shape of output will be different from shape of input.  In sigmoid, input and output shares the same shape.  To make sure the shape is consistent, we can follow these facts:\n",
    "\n",
    "1. Each Operation will send outputs forward on the forward pass and will receive an “output gradient” on the backward pass, which will represent the partial derivative of the loss with respect to every element of the Operation’s output.  Thus **the shape of the output gradient ndarray must match the shape of the output.**\n",
    "\n",
    "2. On the backward pass, each Operation will send an “input gradient” backward, representing the partial derivative of the loss with respect to each element of the input.  **The shape of the input gradient that the Operation sends backward during the backward pass must match the shape of the Operation’s input.**\n",
    "\n",
    "![](figures/input_output_grad.png)\n",
    "\n",
    "![](figures/param_grad.png)\n",
    "\n",
    "Based on this, we can write the class Operation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "#Abstract class, inheriting object\n",
    "class Operation(object):\n",
    "  \n",
    "    #nothing to init\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    #forward receive ndarray as input\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        #put trailing _ to avoid naming conflict\n",
    "        self.input_ = input_\n",
    "\n",
    "        #this _output will use self.input_ to calculate the ouput\n",
    "        #_  here means internal use\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "        \n",
    "        #perform input grad based on output_grad\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        #input grad must have same shape as input\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        \n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add also another class that inherits from <code>Operation</code> that we’ll use specifically for Operations that involve parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract class, inheriting Operation\n",
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__()  #inherit from parent if any\n",
    "        self.param = param  #this will be used in _output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        #perform gradients for both input and param\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        assert self.param.shape == self.param_grad.shape\n",
    "        \n",
    "        #return only input_grad because param_grad is not needed in the previous layer\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement some functions that we implement in last class, including:\n",
    "1. Matrix multiplication\n",
    "2. Addition of bias term\n",
    "3. Sigmoid activation function\n",
    "\n",
    "Lets start with matrix multiplication.  Since the input has two params, X and W, we inherit from <code>ParamOperation</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        #initialize Operation with self.param = W\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ @ self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad @ self.param.T  #same as last class\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        return self.input_.T @ output_grad  #same as last class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the BiasAdd operation where the gradients are simply one.  Since it is an operation between X and B, we inherit from <code>ParamOperation</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self, B: ndarray):\n",
    "        #initialize Operation with self.param = B.\n",
    "        assert B.shape[0] == 1  #make sure it's only B\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do sigmoid.  Since sigmoid is simply a operation that maps to another value, it inherits from Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return 1.0/(1.0 + np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also code up Linear activation function which does nothing.  We can use this for Linear Regression since it does not have any activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Layers\n",
    "\n",
    "In terms of <code>Operations</code>, <code>layers</code> are a series of linear operations followed by a nonlinear operation. For example, our neural network from the last chapter could be said to have had five total operations: two linear operations — a weight multiplication and the addition of a bias term — followed the sigmoid function and then two more linear operations.\n",
    "\n",
    "![](figures/layers.png)\n",
    "\n",
    "Here, we define the input as **input layer**, Layer 1 is typically called **hidden layer** because it is the only layer whose values we don't typically see explicitly during the course of training.  Layer 2 is typically called **output layer** which outputs the desired value.\n",
    "\n",
    "By abstraction, we can make neural network look much simpler as follows:\n",
    "\n",
    "![](figures/layers2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer can be said to have a certain number of neurons equal to the dimensionality of the vector that represents each observation in the layer’s output. The neural network from the last class can thus be thought of as having 13 neurons in the input layer (i.e., 13 features), then 13 neurons (again) in the hidden layer, and one neuron in the output layer.\n",
    "\n",
    "Neurons in the brain have the property that they can receive inputs from many other neurons and will “fire” and send a signal forward only if the signals they receive cumulatively reach a certain “activation energy.” Neurons in the context of neural networks have a loosely analogous property: they do indeed send signals forward based on their inputs, but the inputs are transformed into outputs simply via a nonlinear function. Thus, this nonlinear function is called the activation function, and the values that come out of it are called the activations for that layer.\n",
    "\n",
    "**Building on the context of layers, deep learning models are simply neural networks with more than one hidden layer.**\n",
    "\n",
    "Now leaving all the theory behind, let's code the Layer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.first = True   #first layer is true for init\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int):\n",
    "        #setup the series of operations\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        #setup self.operations if haven't\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        #run the series of operations\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    #if the operation is a subclass of ParamOperatio\n",
    "    #append param_grad to self.param_grads\n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our layer.  Remember that we have three layers:\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer\n",
    "3. Output layer\n",
    "\n",
    "We don't really need to implement the input layer since it's only the input.  \n",
    "\n",
    "As for our hidden layer, it composes of WeightMultiply, then BiasAdd, then sigmoid.   What name should we give to this layer?  How about LinearNonLinear layer.  In fact, there is a common name for this is \"**Dense/Fully-Connected Layer**\" which refers to layer where each output neuron is a function of all of the input neurons.   Imagine thirteen circles, each circle connected to all circles...(that's why it's called fully-connected)\n",
    "\n",
    "Our output layer is very similar to the hidden layer but without the hidden layer.  We consider this still as a **Dense** layer because each output neuron is again connected to all input neurons.\n",
    "\n",
    "To code this is simple, we simply inherit **Layers** and define the series of operations in <code>_setup_layer</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        #define the desired non-linear function as activation\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        #in case you want reproducible results\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # randomize weights of shape (num_feature, num_neurons)\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # randomize bias of shape (1, num_neurons)\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss Class\n",
    "\n",
    "The next thing we have to code up is the loss function (forward) and its gradients (backward).  We gonna make a parent class called <code>Loss</code> and a child class called <code>MeanSquaredError</code>  The code is quite straightforward, similar to Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert prediction.shape == target.shape\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        #self._output will hold the loss function\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert self.prediction.shape == self.input_grad.shape\n",
    "\n",
    "        #input_grad will hold the gradient of the loss function\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Loss/Objective/Cost function, let's make the concrete loss function.  Here we will be using the <code>MeanSquaredError</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NeuralNetwork \n",
    "\n",
    "Now that we have abstracted low-level <code>Operations</code> into <code>Layers</code>, we can also further abstract bunch of <code>Layers</code> into a <code>NeuralNetwork</code> class.  We can also plug in our <code>Loss</code> into our <code>NeuralNetwork</code> class.\n",
    "\n",
    "For coding implementation:  \n",
    "\n",
    "The structure can be summarized as follows:\n",
    "\n",
    "1. A <code>NeuralNetwork</code> will have a list of Layers as an attribute. The Layers would be as defined previously, with forward and backward methods. These methods take in ndarray objects and return ndarray objects.\n",
    "\n",
    "2. Each <code>Layer</code> will have a list of <code>Operations</code> saved in the operations attribute of the layer during the <code>_setup_layer function</code>.\n",
    "\n",
    "3. These <code>Operations</code>, just like the <code>Layer</code> itself, have <code>forward</code> and <code>backward</code> methods that take in <code>ndarray</code> objects as arguments and return <code>ndarray</code> objects as outputs.\n",
    "\n",
    "4. In each operation, the shape of the <code>output_grad</code> received in the backward method must be the same as the shape of the output attribute of the Layer. The same is true for the shapes of the <code>input_grad</code> passed backward during the backward method and the input_ attribute.\n",
    "\n",
    "5. Some operations have parameters (stored in the <code>param</code> attribute); these operations inherit from the <code>ParamOperation</code> class. The same constraints on input and output shapes apply to <code>Layers</code> and their forward and backward methods as well—they take in <code>ndarray</code> objects and output <code>ndarray</code> objects, and the shapes of the input and output attributes and their corresponding gradients must match.\n",
    "\n",
    "6. A <code>NeuralNetwork</code> will also have a Loss. This class will take the output of the last operation from the <code>NeuralNetwork</code> and the target, check that their shapes are the same, and calculate both a loss value (a number) and an <code>ndarray</code> loss_grad that will be fed into the output layer, starting backpropagation.\n",
    "\n",
    "In terms of processes, we can describe as follows:\n",
    "\n",
    "1. Receive <code>X</code> and <code>y</code> as inputs, both <code>ndarrays</code>.\n",
    "\n",
    "2. Feed <code>X</code> successively forward through each Layer.\n",
    "\n",
    "3. Use the <code>Loss</code> to produce loss value and the loss gradient to be sent backward.\n",
    "\n",
    "4. Use the loss gradient as input to the backward method for the network, which will calculate the <code>param_grads</code> for each layer in the network.\n",
    "\n",
    "5. Call the <code>update_params</code> function on each layer, which will use the overall learning rate for the <code>NeuralNetwork</code> as well as the newly calculated <code>param_grads</code>.\n",
    "\n",
    "Without further ado, let's code it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "        #need to return so that I can calculate loss in the trainer\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        #do not need to return since what I am interested are param_grads, not all the grads\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        \n",
    "        #return so that I can perform some early stopping in the trainer\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        #get the parameters for the network\n",
    "        #use for updating w and b\n",
    "        for layer in self.layers:\n",
    "            #yield is different from return is that\n",
    "            #it will return a generator\n",
    "            #what's amazing is that you can manipulate\n",
    "            #the values and change\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        #get the gradient of the loss with respect to the parameters\n",
    "        #for the network\n",
    "        #use for updating w and b\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this NeuralNetwork class, we can implement the models in a more modular, flexible way and define other models to represent complex nonlinear relationships between input and output. For example, here’s how to easily instantiate the two models we covered in the last chapter— the linear regression, the neural network, and the deep neural network like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20200720\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20200720\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20200720\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trainer and Optimizer\n",
    "\n",
    "To make this process cleaner and easier to extend to the more complicated deep learning scenarios, it will help us to define another class <code>Trainer</code> that carries out the **training**, as well as an additional class <code>Optimizer</code> that carries out the **learning** or the actual updating of the NeuralNetwork parameters given the gradients computed on the backward pass. Let’s quickly define these two classes.  \n",
    "\n",
    "Let's start with the easier one - the <code>Optimizer</code>\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "By making a separate <code>Optimizer</code>, it allows us to incorporate different ways of updating the parameters based on gradients in the future.  \n",
    "\n",
    "For now, we will make something simple, which is basically a <code>-learning_rate</code> multiplies with the gradient.  Since we want to use many different <code>Optimizer</code> in the future, we shall make a parent class as well.\n",
    "\n",
    "The code is simple like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parent class\n",
    "class Optimizer(object):\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        #learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        #how parameters are updated\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochasitc gradient descent optimizer.  \n",
    "class SGD(Optimizer): \n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        #params hold w and b\n",
    "        #param_grads hold their gradients\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Let's create a <code>Trainer</code>.  Here this class will be wrapping everything, with <code>NeuralNetwork</code> and <code>Optimizer</code> as attributes in this class.  \n",
    "\n",
    "For training, we gonna keep this simple where the training works like this:\n",
    "\n",
    "1. Shuffle the data at the beginning of the epoch (epoch is like iterations)\n",
    "2. Feed the data through the newtork in batches, updating the parameters after each batch has been fed through\n",
    "3. Epoch ends when we have fed the entire training set through the <code>Trainer</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    #NeuralNetwork and Optimizer as attributes\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer):\n",
    "        #Requires a neural network and an optimizer in order for \n",
    "        #training to occur. \n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9  #use for comparing the least amount of loss\n",
    "        \n",
    "        #Assign the neural network as an instance variable to \n",
    "        #the optimizer when the code runs\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "\n",
    "    # helper function for shuffling\n",
    "    def permute_data(self, X, y):\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        return X[perm], y[perm]\n",
    "\n",
    "    # helper function for generating batches\n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        #X and y should have same number of rows\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for i in range(0, N, size):\n",
    "            X_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            #return a generator that can be loop\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        #for resetting\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "        \n",
    "        #Fits the neural network on the training data for a certain \n",
    "        #number of epochs.\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                # deepcopy is a hardcopy function that make sure it construct a new object (copy() is a shallow copy)\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = self.permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for (X_batch, y_batch) in batch_generator:\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "            \n",
    "            #Every \"eval_every\" epochs, it evaluated the neural network \n",
    "            #on the testing data.\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                #if the validation loss is not lower, it stop and perform early stopping\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let's run it!\n",
    "\n",
    "Let's load the boston data and test our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "s = StandardScaler()\n",
    "X = s.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#since our train function assumes y to be shape of (n, 1)\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 26.549\n",
      "Validation loss after 20 epochs is 24.722\n",
      "Validation loss after 30 epochs is 22.859\n",
      "Validation loss after 40 epochs is 22.766\n",
      "Validation loss after 50 epochs is 22.648\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.647618498359957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#getting the MSE with testing data\n",
    "preds = lr.forward(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 23.608\n",
      "Validation loss after 20 epochs is 21.098\n",
      "Validation loss after 30 epochs is 15.430\n",
      "Validation loss after 40 epochs is 13.955\n",
      "Validation loss after 50 epochs is 13.393\n",
      "NN MSE:  13.39263889450387\n",
      "Validation loss after 10 epochs is 31.474\n",
      "Validation loss after 20 epochs is 19.227\n",
      "Validation loss after 30 epochs is 15.901\n",
      "Validation loss after 40 epochs is 14.381\n",
      "Validation loss after 50 epochs is 12.854\n",
      "DL MSE:  12.85354351825392\n"
     ]
    }
   ],
   "source": [
    "#Let's try neural network and deep neural network\n",
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20200720)\n",
    "\n",
    "#getting the MSE with testing data\n",
    "preds = nn.forward(X_test)\n",
    "print(\"NN MSE: \", mean_squared_error(y_test, preds))\n",
    "\n",
    "trainer = Trainer(dl, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20200720)\n",
    "\n",
    "#getting the MSE with testing data\n",
    "preds = dl.forward(X_test)\n",
    "print(\"DL MSE: \", mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it!  My intention is to make sure you are no longer \"scared\" of the complexity of neural networks.  In fact, this is all about neural network.  CNN, RNN and other improvements are simply variants of this codebase (adding more layers for normalization, for preventing overfitting, of course, we can do some dimensionality here as well)\n",
    "\n",
    "In the next class, let's work out how we can further improve this codebase.  Obviously, our deep neural net is only good enough for teaching.  If you try to change the seed or change some hyperparameters, you may be amazed that the deep neural net cannot beat the simple neural net.   \n",
    "\n",
    "See you next class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
