{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.3 Deep Learning -  Extending Deep Neural Network\n",
    "\n",
    "### Readings\n",
    "\n",
    "- [WEIDMAN] Ch4\n",
    "- [CHARU] Ch2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from last time work so we can extend further\n",
    "from neuralnet.first_version import *\n",
    "\n",
    "#To make this simple, I have stored all our stuff \n",
    "#that we have develop so far in the folder \n",
    "#<code>/neuralnet/first_version.py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two classes, we focus on understanding deep neural net and I hope it's going fantastic.  Anyhow, what we have been discovering so far is the most basic form of it and most of the time, it is insufficient for most real-world problems.  \n",
    "\n",
    "Today, we shall explore some well-understood techniques that make neural network training more likely to succeed.\n",
    "\n",
    "Before doing that, let's review the intuition behind neural network:\n",
    "\n",
    "In a high-level view, we can say that Neural Network is trying to most optimal parameters which is commonly defined as W that **minimizes the loss**, which can be described using the figure like this:\n",
    "\n",
    "<img src=\"figures/4-1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "Now, if we use non-linear activation function like sigmoid, **each W will have a non-linear relationship with the loss**.  If we plot one W against loss, while keeping everything constant, we get this oversimplistic graph:\n",
    "\n",
    "<img src=\"figures/4-2.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "At the beginning, we will probably randomize a W value, and **we iterate to update our W by finding the gradients**.  How large should we move along the slope then?  We use **learning rate**.  Small learning rate means small step of update which can be slow but risk ending up in a **local minimia**, while large learning mean large step of update what will be faster but risk **hopping over\"** the global minimum\n",
    "\n",
    "<img src=\"figures/4-3.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "Now we can imagine there are many areas we can perhaps improve on:\n",
    "\n",
    "1.  The **loss function**.  We used MSE as our loss function.  We chose this because it is convex (i.e., imagining a shape of U), meaning that when the loss is huge, the gradient is steeper (imagine the curve is very steep on the top of U but become less toward the bottom of U), allowing the model to quickly improve.  The good question we can ask here is **whether we can find a loss function that is steeper** and also is differentiable so gradient can be easily find, in order to accelerate the learning.\n",
    "\n",
    "2. The **activation function**.  We used sigmoid as our activation function, but the gradient of sigmoid, *at best*, can have derivative of 0.25 which is quite small.   The good question we can ask here is **whether we can find a activation function that can provide larger range of derivative** and thus can accelerate the learning.\n",
    "\n",
    "3. The **update rule**.  Now we only simply multiply the learning rate with the current gradient.  However, our batch X and y keeps changing.  It may be nice if we can update the params based on histories of gradients, not only on the current gradient.\n",
    "\n",
    "4. The **learning rate**.  Currently, we put a static learning rate, but it does not make so much sense.  In fact, it can be safely assumed that our randomized weight is greatly far from the optimal weight and thus learning rate should be large.  However, as iterations run, learning rate should be slowly reduce so we do not keep hopping over and over again, not finding the minimum.\n",
    "\n",
    "5. The **weight initialization** We currently simply randomize our Ws but the good question is whether we can improve this process a bit.\n",
    "\n",
    "6. Last, the **overfitting**.  You may already realize that we so far is hesistant to add more layers.  Why?  Because more layers though may be more accurate but could potentially overfit.  Thus, we need to add some mechanisms to counter the act of adding more layers, in order to prevent overfitting.\n",
    "\n",
    "Phew....they may look a lot but that's the point of the beauty of deep neural net.  Let's start with the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #1. Softmax Cross Entropy Function\n",
    "\n",
    "So recall the question whether we can find **a loss function that is steeper**.\n",
    "\n",
    "Indeed, if we consider a **classification problem**, **softmax cross entropy** as a loss function has a steeper gradients, exploiting the fact that we know the predicted results are probabilities that sum up to 1\n",
    "\n",
    "The **Softmax Cross Entropy** function has two components: (1) softmax function, and (2) cross entropy function.\n",
    "\n",
    "Let's first focus on the softmax function:\n",
    "\n",
    "\n",
    "#### The Softmax Function\n",
    "\n",
    "For example, let's have a classification problem with N classes, let's say 3 classes.  Then for sample 1, the predicted values can be written as:\n",
    "\n",
    "Vector of probabilities for sample 1 = <code> [5, 3, 2] </code>\n",
    "\n",
    "5, 3, 2 represent the regressed probabilities of each class.  For example, 5 represent the probability of sample 1 to belong to class 1, 3 for class 2, and 2 for class 3.   \n",
    "\n",
    "To make it more clear, it is desirable to convert <code> [5, 3, 2] </code> to something like <code> [0.5, 0.3, 0.2] </code> so they sum up to 1 and are really probabilities. \n",
    "\n",
    "Indeed, this can be easily done by simply normalizing them like this:\n",
    "\n",
    "$$ \\text{Normalize}(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix})  = \\begin{bmatrix} \\frac{x_1}{x_1 + x_2 + x_3} \\\\ \n",
    "\\frac{x_2}{x_1 + x_2 + x_3} \\\\\n",
    "\\frac{x_3}{x_1 + x_2 + x_3}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "However, there turns out a way that both produces **steeper** gradients, and at the same time, **has elegant mathematical properties**.  This is called **softmax function** like this:\n",
    "\n",
    "$$ \\text{Softmax}(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix})  = \\begin{bmatrix} \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}} \\\\ \n",
    "\\frac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3}} \\\\\n",
    "\\frac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3}}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "The good properties include:\n",
    "1. The softmax function makes the bigger value much bigger, forcing the neural network to be \"less neutral\".  This is doable since we are most interested in the class with biggest probability anyway.  If we apply <code>softmax([5, 3, 2])</code>, we get <code>[0.84, 0.11, 0.04]</code> which is different with the more neutral method of normalization which get <code>[0.5, 0.3, 0.2]</code>\n",
    "\n",
    "2. The softmax function has a steeper gradients, comparing to simple normalization, since derivative of $e^x$ is $e^x$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss\n",
    "\n",
    "Recall that loss function in classification problems take a vector of predicted probabilities \n",
    "\n",
    "$$ \\begin{bmatrix} p_1 \\\\ . \\\\ . \\\\ p_n \\end{bmatrix} $$\n",
    "\n",
    "For N=3, we can have like this <code>[0.84, 0.11, 0.04]</code>\n",
    "\n",
    "The loss function will then calculate the loss based on the differences between this vector of predicted probabilities and a vector of actual values that look like this:\n",
    "\n",
    "$$ \\begin{bmatrix} y_1 \\\\ . \\\\ . \\\\ y_n \\end{bmatrix} $$\n",
    "\n",
    "If sample 1 belongs to class 1, then this vector will look like this: <code>[1, 0, 0]</code>\n",
    "\n",
    "The cross entropy loss function, for each index $i$ in these vectors, is \n",
    "\n",
    "$$ \\text{CE}(p_i, y_i) = - y_i * \\text{log}(p_i) - (1 - y_i) * \\text{log}(1-p_i) $$\n",
    "\n",
    "Why this loss function make sense?  Here is the breakdown situation when $y_i = 0$ and $y_i = 1$\n",
    "\n",
    "$$\n",
    "CE(p,y_i)=\n",
    "\\begin{cases}\n",
    "-log(1-p_i) & \\text{if }  y_i = 0\\\\\n",
    "-log(p_i) & \\text{if }  y_i = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If our $y_i = 0$, \n",
    "\n",
    "- When our $p_i$ is near 0 which means we are correct, the loss become \n",
    "\n",
    "$$-log(1) = 0$$\n",
    "\n",
    "- otherwise, if our $p_i$ is near 1 which means we are incorrect, the loss becomes \n",
    "\n",
    "$$-log(0) = \\infty$$\n",
    "\n",
    "If our $y_i = 1$\n",
    "\n",
    "- When our $p_i$ is near 0 which means we are incorrect, the loss become \n",
    "\n",
    "$$-log(0) = \\infty$$\n",
    "\n",
    "- otherwise, if our $p_i$ is near 1 which means we are correct, the loss becomes \n",
    "\n",
    "$$-log(1) = 0$$\n",
    "\n",
    "If we were to plot the situation when $y_i = 0$, here is the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss values')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0.01, 0.99, 99)  #range of p\n",
    "y1 = -1.0 * np.log(1 - x)  #the cross entropy\n",
    "y2 = (x - 0) ** 2  #the MSE\n",
    "plt.plot(x, y1);\n",
    "plt.plot(x, y2);\n",
    "plt.legend(['Cross entropy loss', 'Mean squared error'])\n",
    "\n",
    "plt.title(\"Cross entropy loss vs. MSE when $y = 0$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Loss values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that **cross entrophy** has a steeper gradient when the prediction is very far from the actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can plot when $y_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y = -1.0 * np.log((x))\n",
    "\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Log loss for $y = 1$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real magic happens when we combine this loss with the softmax function like this:\n",
    "\n",
    "$$ {SCE}_1 = - y_1 * log(\\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}}) - (1 - y_1) * log(1-\\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}}) $$\n",
    "\n",
    "This **softmax cross entropy** function first converts $x_1...x_n$ into probabilities, and then insert these probabilities into the cross entrophy function.\n",
    "\n",
    "It turns out that the gradient can also be very easily calculated as follows:\n",
    "\n",
    "$$\\frac{\\partial SCE_1}{\\partial x_1} = \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}} - y_1$$\n",
    "\n",
    "That means that the total gradient is as follows:\n",
    "\n",
    "$$ \\text{softmax}(\\begin{bmatrix} p_1 \\\\ p_2 \\\\ p_3 \\end{bmatrix}) - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} $$\n",
    "\n",
    "Enough talk.  Let's code this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = self.softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        #numpy.clip(a, a_min, a_max, out=None, **kwargs)\n",
    "        #To prevent extremely large loss values that could lead to numeric instability, \n",
    "        #we’ll clip the output of the softmax function to be no less than 10–7 and no greater than 10^7\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "        \n",
    "        #return average loss\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        #return average grad\n",
    "        return (self.softmax_preds - self.target) / self.prediction.shape[0]\n",
    "\n",
    "    def softmax(self, x, axis=None):\n",
    "        #keepdims so that this number can be broadcasted and divided\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #2. Activation Function\n",
    "\n",
    "We use sigmoid in previous classes because we say that sigmoid was:\n",
    "- non-linear and monotonic (one to one mapping)\n",
    "- actually help overfitting by forcing all values to be between 0 and 1\n",
    "\n",
    "Nevertheless, sigmoid produces **relatively flat gradients with maximum slopoe of only 0.25, i.e., any gradient that gets passed to the sigmoid function is at best, be divided by 4 when sent backward.  Worse still, when the input to the sigmoid is less than -2 or greater than 2, the gradient is almost zero**. See this graph for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x116020850>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid_derivative(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that any parameters influencing these inputs will receive small gradients, and our network could elarn slowly as a result.  Worse yet, if we have multiple sigmoid functions in successive layers, it will further diminish the gradients.\n",
    "\n",
    "So the question is, is there any other activation function that has bigger derivative.  Let's explore other activation function including **ReLu**, **Tanh**, and **Relu Leaky**.\n",
    "\n",
    "#### ReLu (Rectified Linear Unit)\n",
    "\n",
    "ReLu is simply defined to be 0 if x is less than 0, and x otherwise.  It's derivative is simply if x is more than 0, then the derivative is 1, otherwise is 0.\n",
    "\n",
    "This is a valid activation function because it is monotonic and non-linear.  It produces much greater gradients.  However, the downside is that it draws a somewhat strange distinction between values less than or greater than 0. That can be fixed with **Leaky ReLu** which is simply using some alpha to make sure values smaller than zero does not completely gone.  Another one is **ReLu6** which is simply having the positive cap at 6 (it will not exceed 6), adding even more non-linearity into the network.\n",
    "\n",
    "Nevertheless, if we are dealing with simple problems such as MNIST, then adding more sophiscated activation functions are NOT encouraged, since it adds more complexity and make the network harder to learn.\n",
    "\n",
    "Luckily, there is a simple activation function that also produces strong gradients but is also simple: Tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$ReLU(x)$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.array([el if el > 0 else 0 for el in x])\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "plt.plot(x, relu(x))\n",
    "# plt.plot(x, relu_derivative(x))\n",
    "plt.title(\"ReLU activation function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"$ReLU(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "\n",
    "The Tanh function is shaped similarly to the sigmoid function but maps inputs to values between -1 and 1.  It produces strong gradient with maximum of 1.  It also has a simple derivative of $$1 - tanh(x)^2$$\n",
    "\n",
    "Given that tanh has strong gradient of 1, it is commonly a natural replacement of sigmoid since tanh is better than sigmoid in the gradient, but share the same downside of sigmoid, i.e., having gradient of 0 after x is around the left/right side of the curve.\n",
    "\n",
    "Tanh is also considered a balance between sigmoid and ReLu.  Sigmoid is very conservative, while Relu is very progressive - always producing gradient of 1 when x is greater than 0.   In common problem, Tahn is sufficient to improve the performance over sigmoid.  In more sophiscated problems with many non-linearity, we may use ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1160c2f90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-5, 5, 0.01)\n",
    "plt.plot(x, np.tanh(x), label=\"tanh\")\n",
    "plt.plot(x, 1 - (np.tanh(x) **2), label=\"tanh'\")\n",
    "plt.title(\"Tanh activation \")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the Tanh function as a class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad * (1 - self.output * self.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Before we dive into other possible extensions.  Let's try out our techniques using MNIST dataset, which consists of black and white images of handwritten digits that are 28 x 28 pixels, with the value of each pixel range from 0 to 255.\n",
    "\n",
    "Here we gonna have 60,000 images on training and 10,000 images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.target = mnist.target.astype(int)\n",
    "\n",
    "X_train = mnist['data'][:60000]\n",
    "y_train = mnist['target'][:60000]\n",
    "\n",
    "X_test = mnist['data'][60000:]\n",
    "y_test = mnist['target'][60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Always help to standardize our data   The reason we do this is because in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model.  We'd like in this process for each feature to have a similar range so that our gradients don't go out of control (and that we only need one global learning rate multiplier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We need to convert our y which current is a single digit showing the class (e.g., 9), we need to convert to a vector of probabilities like this <code>[0, 0, 0, 0, 0, 0, 0, 0, 1]</code>  which indicates that this sample is of class 9.  This process is also commonly known as *one hot encoding*  We can use <code>sklearn.preprocessing</code> to make our life easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "\n",
    "#sklearn expects a 2D array thus we have to reshape to (-1, 1)\n",
    "y_train_encode = onehot.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encode = onehot.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(y_train_encode.shape, y_test_encode.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple accuracy function\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calc_accuracy(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sigmoid activation function**\n",
    "\n",
    "We gonna first use Tanh activation on the first Dense layer.  Then on the second layer (i.e., last layer), since the output has 10 numbers with number in range of [0, 1], thus the last layer has 10 neurons with Sigmoid activation to squeeze the result into that range of [0, 1]\n",
    "\n",
    "**A key question is how many neurons should be our hidden layer which is the Tanh layer.**. This is a entire field of its own and there are really tough to determine the right number.  However, the one consensus is that **number of neurons should be between the number of neurons in the input layer (e.g., 784) and the number of neurons in the output layer (e.g., 10)**\n",
    "\n",
    "Anyhow, this leave us still doubtful what number to choose between 10 and 784.   Another useful technique is to take the **number of sample / (alpha * number of neurons in both input and output layers)**.  This can often prevent overfitting.  For example, let alpha be 1 (typically around 2 - 10), it can be calculated as:\n",
    "\n",
    "$$ 60,000 / (1 * 784 + 10) = 75.6 $$\n",
    "\n",
    "Thus we can use around 76 neurons.  Another technique is to use the mean.  Since we are talking about images, we can take the **geometric mean of 784 and 10** which can be calculated as:\n",
    "\n",
    "$$ \\sqrt{784 * 10} = 88.54 $$\n",
    "\n",
    "Thus we can use around 89 neurons.\n",
    "\n",
    "Anyhow, talking about this probably require all of us to read tons of research papers!  Here is some discussion you can read (https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw).  The general idea is that the more neurons you have, it usually helps in model more complex relationships, but it also come with a downside that it takes much more time to learn.\n",
    "\n",
    "Also note that all what we discuss only apply to feed-forward neural network...\n",
    "\n",
    "Let's stick with 89 neurons for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.300\n",
      "Validation loss after 20 epochs is 0.245\n",
      "Validation loss after 30 epochs is 0.225\n",
      "Validation loss after 40 epochs is 0.216\n",
      "Validation loss after 50 epochs is 0.210\n",
      "Accuracy:  0.8758\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Softmax Cross Entropy Loss**\n",
    "\n",
    "Let's prove our claim that softmax cross entropy can help our model learn faster.  \n",
    "\n",
    "Similarly, we gonna use Tanh activation on the beginning.  Since we are now feeding our results to the SoftmaxCrossEntropy which has a softmax function to make things into probabilities, we simply use a Linear activation function in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.683\n",
      "Validation loss after 20 epochs is 0.645\n",
      "Validation loss after 30 epochs is 0.636\n",
      "Validation loss after 40 epochs is 0.635\n",
      "Validation loss after 50 epochs is 0.633\n",
      "Accuracy:  0.8924\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we see a 2\\% increase.  Although this is minimal, they stack up with other improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #3: Momentum\n",
    "\n",
    "So far, we’ve been using only one update rule for our weights at each time step. Simply take the derivative of the loss with respect to the weights and move the weights in the resulting correct direction.\n",
    "\n",
    "Here we gonna introduce a new way based on using the history of gradients called **momentum**\n",
    "\n",
    "Basing our parameter updates on momentum means that the parameter update at each time step will be a weighted average of the parameter updates at past time steps, with the weights decayed exponentially. There will thus be a second parameter we have to choose, the momentum parameter, which will determine the degree of this decay; the higher it is, the more the weight update at each time step will be based on the parameter’s accumulated momentum as opposed to its current velocity\n",
    "\n",
    "Mathematically, if our momentum parameter is $\\mu$, and the gradient at each time step is $\\nabla_t$, our weight update is \n",
    "\n",
    "$$\\text{update} = \\nabla_t + \\mu * \\nabla_{t-1} + \\mu^2 * \\nabla_{t-2} + \\ldots$$\n",
    "\n",
    "If our momentum parameter was $0.9$, for example, we would multiply the gradient from one time step ago by $0.9$, the one from two time steps ago by $0.9^2 = 0.81$, the one from three time steps ago by $0.9^3 = 0.729$, and so on, and then finally add all of these to the gradient from the current time step to get the overall weight update for the current time step.\n",
    "\n",
    "#### Why momentum?\n",
    "\n",
    "The problem of the old way is that it is possible that it stucks at the local minima, just like the picture below.\n",
    "\n",
    "<img src = \"figures/momentum.png\" width=\"300\">\n",
    "\n",
    "To avoid this situation, we use a momentum term in the objective function, which is a value between 0 and 1 that increases the size of the steps taken towards the minimum by trying to jump from a local minima. If the momentum term is large then the learning rate should be kept smaller. A large value of momentum also means that the convergence will happen fast. But if both the momentum and learning rate are kept at large values, then you might skip the minimum with a huge step. A small value of momentum cannot reliably avoid local minima, and can also slow down the training of the system. \n",
    "\n",
    "Momentum also helps in smoothing out the variations, if the gradient keeps changing direction, just like in this graph\n",
    "\n",
    "<img src = \"figures/momentum2.png\" width=\"500\">\n",
    "\n",
    "**A right value of momentum can be either learned by hit and trial or through cross-validation.**\n",
    "\n",
    "To implement this, a simple way is just to keep track of what you have added so far, and simply add the current gradient on top of it, like this:\n",
    "\n",
    "1. $\\nabla_1$\n",
    "2. $\\nabla_2 + \\mu * \\nabla_1$\n",
    "2. $\\nabla_3 + \\mu * (\\nabla_2 + \\mu * \\nabla_1)$\n",
    "\n",
    "We can call the right side after the first time, called velocity which is the history of momentum * gradients.  In every iteration, we simply multiply the velocity by momentum again, and add up the current grad.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 momentum: float = 0.9):\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param)\n",
    "                               for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for (param, param_grad, velocity) in zip(self.net.params(),\n",
    "                                                 self.net.param_grads(),\n",
    "                                                 self.velocities):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad,\n",
    "                              velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs):            \n",
    "            # Update velocity\n",
    "            kwargs['velocity'] *= self.momentum\n",
    "            kwargs['velocity'] += self.lr * kwargs['grad']\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= kwargs['velocity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this replacing SGD with SGDMomentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.549\n",
      "Loss increased after epoch 20, final loss was 0.549, using the model from epoch 10\n",
      "Accuracy:  0.9154\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.1, momentum=0.9))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it improves another 2\\%!  These improvements slowly stack up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #4: Learning rate decay\n",
    "\n",
    "In previous classes, we fix the learning rate.  However, learning rate decay is the idea that while we want to take big steps toward the beginning of training, we want to smoothly declines so it does not skip over the minimum.\n",
    "\n",
    "There are different types.  The simplest is **linear decay**.  At time step $t$ with $N$ iterations (epochs), if the learning rate we want to start with is \n",
    "$\\alpha_{start}$, and our final learning rate is $\\alpha_{end}$, then our learning rate at each time step is:\n",
    "\n",
    "$$\\alpha_{t} = \\alpha_{start} - (\\alpha_{start} - \\alpha_{end}) * \\frac{t}{N}$$\n",
    "\n",
    "Another simple method is **exponential decay** where the formula is:\n",
    "\n",
    "$$\\alpha_{t} = \\alpha_{start} * ((\\frac{\\alpha_{end}}{\\alpha_{start}})^\\frac{1}{N})^t$$\n",
    "\n",
    "Implementing this requires us to update our <code>Optimizer</code> function as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,\n",
    "                 decay_type: str = 'exponential'):\n",
    "        self.lr = lr\n",
    "        self.final_lr = final_lr  #<----added\n",
    "        self.decay_type = decay_type #<----added\n",
    "\n",
    "    def _setup_decay(self):  #<----added\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == 'exponential':\n",
    "            self.decay_per_epoch = np.power(self.final_lr / self.lr,\n",
    "                                       1.0 / (self.max_epochs - 1))\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs - 1)\n",
    "\n",
    "    def _decay_lr(self): #<----added\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "\n",
    "        if self.decay_type == 'exponential':\n",
    "            self.lr *= self.decay_per_epoch\n",
    "\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.lr -= self.decay_per_epoch\n",
    "\n",
    "    def step(self, epoch: int = 0):  #<----added epoch info\n",
    "\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our <code>SGDMomentum</code> class is also updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,   #<----added\n",
    "                 decay_type: str = None,   #<------added\n",
    "                 momentum: float = 0.9):\n",
    "        super().__init__(lr, final_lr, decay_type)   #<---changed\n",
    "        self.momentum = momentum\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param)\n",
    "                               for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for (param, param_grad, velocity) in zip(self.net.params(),\n",
    "                                                 self.net.param_grads(),\n",
    "                                                 self.velocities):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad,\n",
    "                              velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "\n",
    "            # Update velocity\n",
    "            kwargs['velocity'] *= self.momentum\n",
    "            kwargs['velocity'] += self.lr * kwargs['grad']\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= kwargs['velocity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update our <code>Trainer</code> to call this decay function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    #NeuralNetwork and Optimizer as attributes\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer):\n",
    "        #Requires a neural network and an optimizer in order for \n",
    "        #training to occur. \n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9  #use for comparing the least amount of loss\n",
    "        \n",
    "        #Assign the neural network as an instance variable to \n",
    "        #the optimizer when the code runs\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "\n",
    "    # helper function for shuffling\n",
    "    def permute_data(self, X, y):\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        return X[perm], y[perm]\n",
    "\n",
    "    # helper function for generating batches\n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        #X and y should have same number of rows\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for i in range(0, N, size):\n",
    "            X_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            #return a generator that can be loop\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 20200720,\n",
    "            restart: bool = True):\n",
    "        \n",
    "        \n",
    "        setattr(self.optim, 'max_epochs', epochs)  #<----added\n",
    "        self.optim._setup_decay() #<----added\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        #for resetting\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "        \n",
    "        #Fits the neural network on the training data for a certain \n",
    "        #number of epochs.\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                # deepcopy is a hardcopy function that make sure it construct a new object (copy() is a shallow copy)\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = self.permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for (X_batch, y_batch) in batch_generator:\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "            \n",
    "            #Every \"eval_every\" epochs, it evaluated the neural network \n",
    "            #on the testing data.\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                #if the validation loss is not lower, it stop and perform early stopping\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break\n",
    "            \n",
    "            #call this at the end of each epoch\n",
    "            if self.optim.final_lr:  #<------added\n",
    "                self.optim._decay_lr()   #<-----added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.505\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, another 1.5\\% increase!!!  Hopefully we can reach 0.95..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #5: Weight Initialization\n",
    "\n",
    "As we mentioned in the section on activation functions, several activation functions, such as sigmoid and Tanh, have their steepest gradients when their inputs are 0, with the functions quickly flattening out as the inputs move away from 0. This can potentially limit the effectiveness of these functions, because if many of the inputs have values far from 0, the weights attached to those inputs will receive very small gradients on the backward pass.\n",
    "\n",
    "This turns out to be a major problem in the neural networks we are now dealing with. Consider the hidden layer in the MNIST network we’ve been looking at. This layer will receive 784 inputs and then multiply them by a weight matrix, ending up with some number n of neurons (and then optionally add a bias to each neuron). \n",
    "\n",
    "This figure shows the distribution of these n values in the hidden layer of our neural network (with 784 inputs) before and after feeding them through the Tanh activation function.\n",
    "\n",
    "<img src=\"figures/distribution.png\" width=\"300\">\n",
    "\n",
    "After being fed through the activation function, most of the activations are either –1 or 1!\n",
    "\n",
    "The reason is because we initialized each weight to have variance 1 (Recall that we use <code>random.rand(num_feature, num_neurons)</code> which basically create a uniform distribution of <code>[0, 1)</code> with variance 1.  Also for bias, we use <code>random.rand(1, num_neurons)</code>):\n",
    "\n",
    "$$ \\text{Var}(w_{i,j}) = 1 $$\n",
    "\n",
    "Thus, if we have 785 features (784 + 1 for b), each having one variance, this gives us a standard deviation of:\n",
    "\n",
    "$$ \\sqrt{785} = 28.02 $$\n",
    "\n",
    "This is pretty huge .  So what should be the right distribution of weights?  Well, it certainly should depend on the number of neurons in the in and out layer.  The weight matrix should have a roughly normal distribution across all neurons.  But now we have in- and out- layer, which should we use as basis?  Let's think each of them first.  \n",
    "\n",
    "For in-layer, the standard deviation of each weight on the forward pass should be\n",
    "\n",
    "$$ \\frac{1}{n_{in} } $$ \n",
    "\n",
    "At the same time, for the next layer, \n",
    "\n",
    "$$ \\frac{1}{n_{out} } $$ \n",
    "\n",
    "As a compromise between these, what is most often called **Glorot initialization (also known as Xavier)**, the idea is to initialize each weight with a small Gaussian value with mean = 0.0 and variance based on the fan-in and fan-out of the weight like this:\n",
    "\n",
    "$$ \\frac{2}{n_{in} + n_{out}} $$ \n",
    "\n",
    "where $n_{in}$ is the number of input neurons, and $n_{out}$ is the number of output neurons.\n",
    "\n",
    "By doing this, we make sure the distribution of the weights in the beginning is uniformed across all neurons in the beginning.\n",
    "\n",
    "To do this is simple, we add a <code>weight_init</code> argument to each layer, and we add the following to our <code>_setup_layer</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int,\n",
    "                 activation: Operation = Sigmoid(),\n",
    "                 weight_init: str = \"glorot\"): #<---added\n",
    "        #define the desired non-linear function as activation\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init #<----added\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        #in case you want reproducible results\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        #---------->added section\n",
    "        num_in = input_.shape[1]\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2/(num_in + self.neurons)\n",
    "        else:\n",
    "            scale = 1.0   \n",
    "        #--------------------------------------\n",
    "            \n",
    "        #---------->revised section\n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.normal(loc=0,\n",
    "                                            scale=scale,\n",
    "                                            size=(num_in, self.neurons)))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.normal(loc=0,\n",
    "                                            scale=scale,\n",
    "                                            size=(1, self.neurons)))\n",
    "        #--------------------------------------\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then try our models with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, another 1\\% increase!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension #6: Dropout\n",
    "\n",
    "Recall the discussion in the beginning of the class that adding more layers are prone to overfitting, and thus we are quite hesitant to add more layers than we need.  Indeed, with one hidden layer, we are doing good job so far.\n",
    "\n",
    "The good question to ask is what if we want to add more layers in the future, for example, for more complex data?\n",
    "\n",
    "The answer is yes, with **Dropout**.  **Dropout** is a mechanism to make our neural network less likely to overfit\n",
    "\n",
    "**Dropout** is actually a simple idea of randomly choosing some proportion $p$ of the neurons in a layer and setting them equal to 0 during each forward pass of training. By randomly setting some neurons to null in each iteration, the other neurons become more generalized in learning.\n",
    "\n",
    "Though dropout can help our network avoid overfitting during training, we still want to give our network its *best shot* of making correct predictions when it comes time to predict. So, the Dropout operation will have **two modes**: a **training** mode in which dropout is applied, and an **inference** mode, in which it is not. \n",
    "\n",
    "To make sure the distribution of the values during inference time is close to that during training time, we should multiply values by $P_{keep}$.  For example, say we have a vector of $x = {1, 2, 3, 4, 5}$.  Let's set $p=0.8$ which means 20\\% of data will be turn to 0.  In training, $x_{train} = {1, 0, 3, 4, 5}$ ; do not confuse why I turn off 2 and not others, I just turn 20\\% off randomly.  In inference, we turn off dropout, but to make sure the distribution remains similar, we multiply the values by 0.8, which becomes $x_{inference} = {0.8, 1.6, 2.4, 3.2, 4.0}$\n",
    "\n",
    "We can implement **Dropout** as an <code>Operation</code>, that we can attach onto the end of each layer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Operation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 keep_prob: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:\n",
    "        if inference:\n",
    "            return self.input_ * self.keep_prob  #multiply input by probability\n",
    "        else:\n",
    "            #binomial will give us list of 0 and 1s with 1s of probability equal to keep_prob\n",
    "            self.mask = np.random.binomial(1, self.keep_prob,\n",
    "                                           size=self.input_.shape)  \n",
    "            return self.input_ * self.mask\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        #since gradient of 0 is nothing, thus the input_grad is simply whatever output_grad multiply with self.mask\n",
    "        return output_grad * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we included an inference flag in the _output method that affects whether dropout is applied or not. For this flag to be called properly, we actually have to add it in several other places throughout training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The  **NeuralNetwork** > **Layer** > **Operation** forward methods will take in inference as an argument (False by default) and pass the flag into each Operation, so that every Operation will behave differently in training mode than in inference mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "  \n",
    "    def forward(self, X_batch: ndarray,\n",
    "                inference=False) ->  ndarray:   #<----added inference as param\n",
    "\n",
    "        X_out = X_batch\n",
    "        for layer in self.layers:\n",
    "            X_out = layer.forward(X_out, inference)  #<----added inference as param\n",
    "\n",
    "        return X_out\n",
    "    \n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "            \n",
    "            #you may wonder why I did not return anything\n",
    "            #it's because in Layer.backward, it is appending this value to param_grads to each layer\n",
    "            #this return \"grad\" is simply something it returns\n",
    "  \n",
    "    def train_batch(self,\n",
    "                    X_batch: ndarray,\n",
    "                    y_batch: ndarray,\n",
    "                    inference: bool = False) -> float:  #<-----added inference as param\n",
    "\n",
    "        prediction = self.forward(X_batch, inference)  #<----added inference as param\n",
    "\n",
    "        batch_loss = self.loss.forward(prediction, y_batch)\n",
    "        loss_grad = self.loss.backward()\n",
    "\n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        return batch_loss\n",
    "    \n",
    "    def params(self):\n",
    "        #get the parameters for the network\n",
    "        #use for updating w and b\n",
    "        for layer in self.layers:\n",
    "            #equivalent for-loop yield\n",
    "            #yield is different from return is that\n",
    "            #it will return a sequence of values\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        #get the gradient of the loss with respect to the parameters\n",
    "        #for the network\n",
    "        #use for updating w and b\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int) -> None:\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray,\n",
    "                inference=False) -> ndarray:   #<--------added\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_, inference)   #<------added inference as param\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        for operation in self.operations[::-1]:\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        assert self.input_.shape == input_grad.shape\n",
    "\n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> None:\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> None:\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,\n",
    "                input_: ndarray,\n",
    "                inference: bool=False) -> ndarray:  #<----inference\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output(inference) #<----inference\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        #input grad must have same shape as input\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:  #<----inference\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Linear(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:   #<----inference\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad\n",
    "\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:   #<----inference\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class Tanh(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:  #<----inference\n",
    "        return np.tanh(self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad * (1 - self.output * self.output)\n",
    "\n",
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__()  #inherit from parent if any\n",
    "        self.param = param  #this will be used in _output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        #perform gradients for both input and param\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        assert self.param.shape == self.param_grad.shape\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()    \n",
    "\n",
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        #initialize Operation with self.param = W\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: #<----inference\n",
    "        return self.input_ @ self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad @ self.param.T  #same as last class\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        return self.input_.T @ output_grad  #same as last class\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self, B: ndarray):\n",
    "        #initialize Operation with self.param = B.\n",
    "        assert B.shape[0] == 1  #make sure it's only B\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: #<----inference\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])\n",
    "\n",
    "#we have to define Dropout again, so it refers to the new Operation class\n",
    "class Dropout(Operation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 keep_prob: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: \n",
    "        if inference:\n",
    "            return self.input_ * self.keep_prob  #multiply input by probability\n",
    "        else:\n",
    "            #binomial will give us list of 0 and 1s with 1s of probability equal to keep_prob\n",
    "            self.mask = np.random.binomial(1, self.keep_prob,\n",
    "                                           size=self.input_.shape)  \n",
    "            return self.input_ * self.mask\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        #since gradient of 0 is nothing, thus the input_grad is simply whatever output_grad multiply with self.mask\n",
    "        return output_grad * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recall that in the Trainer, we evaluate the trained model on the testing set every eval_every epochs. Now, every time we do that, we’ll evaluate with the inference flag equal to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    #NeuralNetwork and Optimizer as attributes\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer):\n",
    "        #Requires a neural network and an optimizer in order for \n",
    "        #training to occur. \n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9  #use for comparing the least amount of loss\n",
    "        \n",
    "        #Assign the neural network as an instance variable to \n",
    "        #the optimizer when the code runs\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "\n",
    "    # helper function for shuffling\n",
    "    def permute_data(self, X, y):\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        return X[perm], y[perm]\n",
    "\n",
    "    # helper function for generating batches\n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        #X and y should have same number of rows\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for i in range(0, N, size):\n",
    "            X_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            #return a generator that can be loop\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 20200720,\n",
    "            restart: bool = True):\n",
    "        \n",
    "        \n",
    "        setattr(self.optim, 'max_epochs', epochs)\n",
    "        self.optim._setup_decay()\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        #for resetting\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "        \n",
    "        #Fits the neural network on the training data for a certain \n",
    "        #number of epochs.\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                # deepcopy is a hardcopy function that make sure it construct a new object (copy() is a shallow copy)\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = self.permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for (X_batch, y_batch) in batch_generator:\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "            \n",
    "            #Every \"eval_every\" epochs, it evaluated the neural network \n",
    "            #on the testing data.\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test, inference=True) #<----inference   #<---make sure validation does not use dropout\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                #if the validation loss is not lower, it stop and perform early stopping\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break\n",
    "            \n",
    "            #call this at the end of each epoch\n",
    "            if self.optim.final_lr:\n",
    "                self.optim._decay_lr() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add a dropout keyword to the Dense class and we append the dropout operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int,\n",
    "                 activation: Operation = Sigmoid(),\n",
    "                 dropout: float = 1.0,  #<---add default dropout as 1.0 which means all values are kept\n",
    "                 weight_init: str = \"glorot\"):\n",
    "        #define the desired non-linear function as activation\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init \n",
    "        self.dropout = dropout  #<----added\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        #in case you want reproducible results\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_in = input_.shape[1]\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2/(num_in + self.neurons)\n",
    "        else:\n",
    "            scale = 1.0   \n",
    "            \n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.normal(loc=0,\n",
    "                                            scale=scale,\n",
    "                                            size=(num_in, self.neurons)))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.normal(loc=0,\n",
    "                                            scale=scale,\n",
    "                                            size=(1, self.neurons)))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        \n",
    "        #---------added this section\n",
    "        if self.dropout < 1.0:\n",
    "            self.operations.append(Dropout(self.dropout))\n",
    "        #---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the previous one but with dropout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...very similar accuracy. The power of Dropout is best worked with deep models! Let's try add one more hidden layer, and use with and without dropout!  For the first layer, let's have twice as many (178) and the second hidden layer has half as many (46).  This will make sure our distribution still looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=178, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=46, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, even without dropout, it still performs quite well...This is not always the case but when you are lucky, with more layers, even without Dropout, you can get better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=178, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=46, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well....unluckily, Dropout does not show that much of a performance.  However, it should be noted that Boston dataset is quite small and thus  Dropout has less effect.  Dropout is especially good when training in a really deep network with huge datasets.\n",
    "\n",
    "Indeed, dropout was an essentially component of the ImageNet-winning model from 2012 that kicked off the modern deep learning era!\n",
    "\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
