{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.5 Deep Learning - Recurrent Neural Networks\n",
    "\n",
    "- [WEIDMAN] Ch6\n",
    "- [CHARU] Ch7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are designed to handle data that appears in sequences: instead of size **(batch_size, features)**, now we add additional dimension which is **time steps**\n",
    "\n",
    "<img src = \"figures/rnn.png\" width=300>\n",
    "\n",
    "For example, one observation could have the features from time t = 1 with the value of the target from time t = 2, the next observation could have the features from time t = 2 with the value of the target from time t = 3, and so on. If we wanted to use data from multiple time steps to make each prediction rather than data from just one time step, we could use the features from t = 1 and t = 2 to predict the target at t = 3, the features from t = 2 and t = 3 to predict the target at t = 4, and so on.\n",
    "\n",
    "However, **treating each time step as independent ignores the fact that the data is ordered sequentially**. How would we ideally want to use the sequential nature of the data to make better predictions? The solution would look something like this:\n",
    "\n",
    "1. Use features from time step t = 1 to make predictions for the corresponding target at t = 1.\n",
    "\n",
    "2. Use features from time step t = 2 as well as the information from t = 1, including the value of the target at t = 1, to make predictions for t = 2.\n",
    "\n",
    "3. Use features from time step t = 3 as well as the accumulated information from t = 1 and t = 2 to make predictions at t = 3.\n",
    "\n",
    "4. And so on, at each step using the information from all prior time steps to make a prediction.\n",
    "\n",
    "To do this, it seems that we'd want to pass our data through the neural network one sequence element at a time, with the data from the first time step being passed through first, then the data from the next time step, and so on. In addition, we’ll want our neural network to **accumulate information** about what it has seen before as the new sequence elements are passed through. \n",
    "\n",
    "In a more concrete fashion, consider the following steps and figure:\n",
    "\n",
    "1. In the first time step, t = 1, we would pass through the observation from the first time step (along with randomly initialized representations, perhaps). We would output a prediction for t = 1, along with representations at each layer.\n",
    "\n",
    "2. In the next time step, we would pass through the observation from the second time step, t = 2, **along with the representations computed during the first time step** (which, again, are just the outputs of the neural network’s layers), and combine these somehow (it is in this combining step that the variants of RNNs we'll learn about differ). We would use these two pieces of information to output a prediction for t = 2 as well as the updated representations at each layer, which are now a function of the inputs passed in at both t = 1 and t = 2.\n",
    "\n",
    "3. In the third time step, we would pass through the observation from t = 3, as well as the representations that now incorporate the information from t = 1 and t = 2, and use this information to make predictions for t = 3, as well as additional updated representations at each layer, which now incorporate information from time steps 1–3.\n",
    "\n",
    "This process is depicted in this figure:\n",
    "\n",
    "<img src = \"figures/rnn2.png\" width=450>\n",
    "\n",
    "We see that each layer has a representation that is **persistent** getting updated over time as new observations are passed through.\n",
    "\n",
    "### The First Class for RNNs: RNNLayer\n",
    "\n",
    "RNNs will deal with data in which each observation is two-dimensional, with dimensions `(sequence_length, num_features)` - order is not important; and since it is always more efficient computationally to pass data forward in batches, `RNNLayer` will have to take in three-dimensional ndarrays, of size `(batch_size, sequence_length, num_features)`. \n",
    "\n",
    "However, we want to feed our data through our `RNNLayers` one sequence element at a time as shown in the previous picture; how can we do this if our input, data, is `(batch_size, sequence_length, num_features)`? Here’s how:\n",
    "\n",
    "1. Select a two-dimensional array from the second axis, starting with `data[:, 0, :]`. This ndarray will have shape `(batch_size, num_features)`.\n",
    "\n",
    "2. Initialize a **hidden state** for the `RNNLayer` that will continually get updated with each sequence element passed in, this time of shape `(batch_size, hidden_size)`. This ndarray will represent the layer’s **accumulated information** about the data that has been passed in during the prior time steps.\n",
    "\n",
    "3. Pass these two ndarrays forward through the first time step in this layer. We'll end up designing `RNNLayer` to output `ndarrays` of different dimensionality than the inputs, just like regular Dense layers can, so the output will be of shape `(batch_size, num_outputs)`. In addition, update the neural network’s representation for **each observation**: at **each time step**, our `RNNLayer` should also output an ndarray of shape `(batch_size, hidden_size)`.\n",
    "\n",
    "4. Select the next two-dimensional array from data: `data[:, 1, :]`.\n",
    "\n",
    "5. Pass this data, as well as the values of the `RNN`’s representations outputted at the first time step, into the second time step at this layer to get another output of shape `(batch_size, num_outputs)`, as well as updated representations of shape `(batch_size, hidden_size)`.\n",
    "\n",
    "6. Continue until all sequence_length time steps have been passed through the layer. Then concatenate all the results together to get an output from that layer of shape `(batch_size, sequence_length, num_outputs)`.\n",
    "\n",
    "This gives us an idea of how our `RNNLayers` should work—and we'll solidify this understanding when we code it up—but it also hints that we'll need another class to handle receiving the data and updating the layer's hidden state at each time step. For this we'll use the `RNNNode`, the next class we’ll cover.\n",
    "\n",
    "### The Second Class for RNNs: RNNNode\n",
    "\n",
    "Based on the description from the prior section, an RNNNode should have a forward method with the following inputs and outputs:\n",
    "\n",
    "Two ndarrays as inputs:\n",
    "\n",
    "- One for the data inputs to the network, of shape `[batch_size, num_features]`\n",
    "- One for the representations of the observations at that time step, of shape `[batch_size, hidden_size]`\n",
    "\n",
    "Two ndarrays as outputs:\n",
    "\n",
    "- One for the outputs of the network at that time step, or shape `[batch_size, num_outputs]`\n",
    "- One for the updated representations of the observations at that time step, of shape: `[batch_size, hidden_size]`\n",
    "\n",
    "Next, we’ll show how the two classes, RNNNode and RNNLayer, fit together.\n",
    "\n",
    "### Putting These Two Classes Together\n",
    "\n",
    "The RNNLayer class will wrap around a list of `RNNNodes` and will (at least) contain a `forward` method that has the following inputs and outputs:\n",
    "\n",
    "- Input: a batch of sequences of observations of shape `[batch_size, sequence_length, num_features]`\n",
    "\n",
    "- Output: the neural network output of those sequences of shape `[batch_size, sequence_length, num_outputs]`\n",
    "\n",
    "The figure below shows the order that data would move forward through an `RNN` with two `RNNLayers` with five `RNNNodes` each. At each time step, inputs initially of dimension `feature_size` are passed successively forward through the first `RNNNode` in each `RNNLayer`, with the network ultimately outputting a prediction at that time step of dimension output_size. In addition, each RNNNode passes a **hidden state** forward to the next `RNNNode` within each layer. Once data from each of the five time steps has been passed forward through all the layers, we will have a final set of predictions of shape `(5, output_size)`, where `output_size` should be the same dimension as the targets. These predictions would then be compared to the target, and the loss gradient would be computed, kicking off the backward pass.\n",
    "\n",
    "<img src = \"figures/rnn3.png\" width=600>\n",
    "\n",
    "Alternatively, data could flow through the RNN in the order shown below. Whatever the order, the following must occur:\n",
    "\n",
    "- Each layer needs to process its data at a given time step before the next layer—for example, in above, 2 can’t happen before 1, and 4 can’t happen before 3.\n",
    "\n",
    "- Similarly, each layer has to process all of its time steps in order—in figure above, for example, 4 can’t happen before 2, and 3 can’t happen before 1.\n",
    "\n",
    "- The last layer has to output dimension feature_size for each observation.\n",
    "\n",
    "<img src = \"figures/rnn4.png\" width=600>\n",
    "\n",
    "### The Backward Pass\n",
    "\n",
    "Backpropagation through recurrent neural networks is often described as a separate algorithm called **backpropagation through time.** While this does indeed describe what happens during backpropagation, it makes things sound a lot more complicated than they are. Keeping in mind the explanation of how data flows forward through an RNN, we can describe what happens on the backward pass this way: we pass data backward through the `RNN` by passing gradients backward through the network in reverse of the order that we passed inputs forward on the forward pass—which, indeed, is the same thing we do in regular feed-forward networks.\n",
    "\n",
    "Looking at the diagrams in figures above on the forward pass:\n",
    "\n",
    "1. We start with a batch of observations, each of shape `(feature_size, sequence_length)`.\n",
    "\n",
    "2. These inputs are broken up into the individual `sequence_length` elements and passed into the network one at a time.\n",
    "\n",
    "3. Each element gets passed through all the layers, ultimately getting transformed into an output of size `output_size`.\n",
    "\n",
    "4. At the same time, the layer passes the hidden state forward into the layer’s computation at the next time step.\n",
    "\n",
    "5. This continues for all `sequence_length` time steps, resulting in a total output of size `(output_size, sequence_length)`.\n",
    "\n",
    "Backpropagation simply works the same way, but in reverse:\n",
    "\n",
    "1. We start with a gradient of shape `[output_size, sequence_length]`, representing how much each element of the output (also of size `[output_size, sequence_length]`) ultimately impacts the loss computed for that batch of observations.\n",
    "\n",
    "2. These gradients are broken up into the individual `sequence_length` elements and passed backward through the layers in reverse order.\n",
    "\n",
    "3. The gradient for an individual element is passed backward through all the layers.\n",
    "\n",
    "4. At the same, the layers pass the gradient of the loss with respect to the hidden state at that time step backward into the layers’ computations at the prior time steps.\n",
    "\n",
    "5. This continues for all `sequence_length` time steps, until the gradients have been passed backward to every layer in the network, thus allowing us to compute the gradient of the loss with respect to each of the weights, just as we do in the case of regular feed-forward networks.\n",
    "\n",
    "This parallelism between the backward and forward pass is highlighted in the figure below, which shows how data flows through an RNN during the backward pass. You’ll notice, of course, that it is the very similar as figures above but with reversed arrows.\n",
    "\n",
    "<img src = \"figures/rnn5.png\" width=600>\n",
    "\n",
    "This highlights that, at a high level, the forward and backward passes for an RNNLayer are very similar to those of a layer in a normal neural network: they both receive `ndarrays` of a certain shape as input, output `ndarrays` of another shape, and on the backward pass receive an **output gradient of the same shape as their output and produce an input gradient of the same shape as their input**. There is a key difference in the way the weight gradients are handled in `RNNLayers` versus other layers, however, so we’ll briefly cover that before we shift to coding this all up.\n",
    "\n",
    "### Accumulating gradients for the weights in an RNN\n",
    "\n",
    "In recurrent neural networks, just as in regular neural networks, each layer will have one set of weights. That means that the same set of weights will affect the layer's output at all sequence_length time steps; during backpropagation, therefore, the same set of weights will receive `sequence_length` different gradients. For example, in the circle labeled *1* in the backpropagation shown in figure above, the second layer will receive a gradient for the last time step, while in the circle labeled *3*, the layer will receive a gradient for the second-to-last time step; both of these will be driven by the same set of weights. Thus, during backpropagation, we'll have to accumulate gradients for the weights over a series of time steps, which means that however we choose to store the weights, we'll have to update their gradients using something like the following:\n",
    "\n",
    "`weight_grad += grad_from_time_step`\n",
    "\n",
    "This is different from the `Dense` and `Conv2D` layers, in which we just stored the parameters in a param_grad argument.\n",
    "\n",
    "We’ve laid out how `RNNs` work and the classes we want to build to implement them; now let’s start figuring out the details.\n",
    "\n",
    "## RNNs: The Code\n",
    "\n",
    "Firstly, let's code up all the activation functions so our layers can make use of that.  Also some helper function.  To make our learning a bit smooth, we shall first cover things that remain unchanged, comparing to the our NeuralNetwork we learn - including Loss\n",
    "\n",
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x: ndarray):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x: ndarray):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x: ndarray):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "        #keepdims so that this number can be broadcasted and divided\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "def batch_softmax(input_array: ndarray):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(a: np.ndarray,\n",
    "                      b: np.ndarray):\n",
    "    assert a.shape == b.shape\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "The Loss for RNNs is the same as before: an `ndarray` output is produced by the last Layer and compared with `y_batch`, a single value is computed, and a gradient of this value with respect to the input to the Loss is returned with the same shape as output. We’ll have to modify the softmax function to work appropriately with ndarrays of shape `[batch_size, sequence_length, feature_size]`, but we can handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,\n",
    "                prediction: ndarray,\n",
    "                target: ndarray) -> float:\n",
    "\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        #-----revised here----to handle sequences----\n",
    "        out = []\n",
    "        for row in self.prediction:\n",
    "            out.append(softmax(row, axis=1))\n",
    "        softmax_preds = np.stack(out)\n",
    "        #--------------------------------------------\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)\n",
    "\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "\n",
    "        return self.softmax_preds - self.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Layer\n",
    "\n",
    "The Layers themselves are where things get interesting.\n",
    "\n",
    "Previously, we gave `Layers` a set of `Operations` that passed data forward and sent gradients backward. `RNNLayers` will be completely different; they now must maintain a *hidden state* that continually gets updated as new data gets fed in and gets *combined* with the data somehow at each time step. How exactly should this work? \n",
    "\n",
    "The forward pass figures suggest that each `RNNLayer` should have a `List` of `RNNNodes` as an attribute, and then each sequence element from the layer’s input should get passed through each `RNNNode`, one element at a time. Each `RNNNode` will take in this sequence element, as well as the *hidden state* for that layer, and produce an output for the layer at that time step as well as updating the layer’s hidden state.\n",
    "\n",
    "To clarify all this, let's dive in and start coding it up: we'll cover, in order, how an `RNNLayer` should be initialized, how it should send data forward during the forward pass, and how it should send data backward during the backward pass.\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "Each `RNNLayer` will start with:\n",
    "\n",
    "- An int hidden_size\n",
    "- An int output_size\n",
    "- An ndarray start_H of shape (1, hidden_size), representing the layer’s hidden state\n",
    "\n",
    "In addition, just like in regular neural networks, we'll set `self.first = True` when we initialize the layer; the first time we pass data into the forward method we’ll pass the ndarray we receive into an `_init_params` method, initialize the parameters, and set `self.first = False`.\n",
    "\n",
    "**Forward**\n",
    "\n",
    "The bulk of the forward method will consist of taking in an `ndarray x_seq_in` of shape `(batch_size, sequence_length, feature_size)` and feeding it through the layer’s `RNNNodes` in sequence. In the following code, `self.nodes` are the `RNNNodes` for the layer, and `H_in` is the hidden state for the layer:\n",
    "\n",
    "<code>   sequence_length = x_seq_in.shape[1]\n",
    "    x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "    for t in range(sequence_length):\n",
    "        x_in = x_seq_in[:, t, :]\n",
    "        y_out, H_in = self.nodes[t].forward(x_in, H_in, self.params)\n",
    "        x_seq_out[:, t, :] = y_out\n",
    "</code>\n",
    "\n",
    "One note on the hidden state `H_in`: the hidden state for an `RNNLayer` is typically represented in a vector, but the operations in each `RNNNode` require the hidden state to be an `ndarray` of size `(batch_size, hidden_size)`. So, at the beginning of each forward pass, we simply *repeat* the hidden state:\n",
    "\n",
    "<code>batch_size = x_seq_in.shape[0]\n",
    "H_in = np.copy(self.start_H)\n",
    "H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "</code>\n",
    "    \n",
    "After the forward pass, we take the average value across the observations making up the batch to get the updated hidden state for that layer:\n",
    "\n",
    "`self.start_H = H_in.mean(axis=0, keepdims=True)`\n",
    "\n",
    "As you can see,  an RNNNode will have to have a forward method that takes in two arrays of shapes:\n",
    "\n",
    "- (batch_size, feature_size)\n",
    "- (batch_size, hidden_size)\n",
    "\n",
    "and returns two arrays of shapes:\n",
    "\n",
    "- (batch_size, output_size)\n",
    "- (batch_size, hidden_size)\n",
    "\n",
    "**Backward**\n",
    "\n",
    "Since the forward method outputted `x_seq_out`, the backward method will receive a gradient of the same shape as `x_seq_out` called `x_seq_out_grad`. Moving in the opposite direction from the `forward` method, we feed this gradient backward through the `RNNNodes`, ultimately returning `x_seq_in_grad` of shape `(batch_size, sequence_length, self.feature_size)` as the gradient for the entire layer:\n",
    "\n",
    "<code>h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "sequence_length = x_seq_out_grad.shape[1]\n",
    "x_seq_in_grad = np.zeros((batch_size, sequence_length, self.feature_size))\n",
    "for t in reversed(range(sequence_length)):\n",
    "    x_out_grad = x_seq_out_grad[:, t, :]\n",
    "    grad_out, h_in_grad = self.nodes[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "    x_seq_in_grad[:, t, :] = grad_out\n",
    "</code>\n",
    "\n",
    "From this, we see that `RNNNodes` should have a backward method that, following the pattern, is the opposite of the forward method, taking in two arrays of shapes:\n",
    "\n",
    "- (batch_size, output_size)\n",
    "- (batch_size, hidden_size)\n",
    "\n",
    "and returning two arrays of shapes:\n",
    "\n",
    "- (batch_size, feature_size)\n",
    "- (batch_size, hidden_size)\n",
    "\n",
    "And that’s the working of an `RNNLayer`. Now it seems like the only thing left is to describe the core of recurrent neural networks: the `RNNNodes` where the actual computations happen. Before we do, let’s clarify the role of `RNNNodes` and their variants within `RNNs` as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = None):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.first = True\n",
    "\n",
    "\n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "        \n",
    "        if not self.weight_scale:\n",
    "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc = 0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc = 0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.output_size))    \n",
    "        \n",
    "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
    "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
    "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
    "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
    "        \n",
    "        self.cells = [RNNNode() for x in range(input_.shape[1])]\n",
    "\n",
    "    \n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "        \n",
    "\n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, output_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        sequence_length = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(sequence_length)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNModel\n",
    "\n",
    "An RNN still passes data forward through a series of layers, which send outputs forward on the forward pass and gradients backward on the backward pass. Thus, for example, whatever the equivalent of our `NeuralNetwork` class ends up being will still have a list of `RNNLayers` as a layers attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[RNNLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss.backward()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "The `Trainer` is mostly the same: we cycle through our training data, selecting batches of input data and batches of output data, and continually feed them through our model, producing loss values that tell us whether our model is learning and updating the weights after each batch has been fed through.\n",
    "\n",
    "One important changes is regarding the representation of input - i.e., how to represent text data in a form that will allow us to feed it into our RNNs.  \n",
    "\n",
    "Language modeling is one of the most common tasks RNNs are used for. How can we reshape a sequence of characters into a training dataset so that an RNN can be trained to predict the next character? The simplest method is to use one-hot encoding. This works as follows: first, each letter is represented as a vector with dimension equal to the size of the vocabulary or the number of letters in the overall corpus of text we’ll train the network on (this is calculated beforehand and hardcoded as a hyperparameter in the network). Then each letter is represented as a vector with a 1 in the position representing that letter and 0s everywhere else. Finally, the vectors for each letter are simply concatenated together to get an overall representation for the sequence of letters.\n",
    "\n",
    "Here’s a simple example of how this would look with a vocabulary of four letters, a, b, c, and d, where we arbitrarily call a the first letter, b the second letter, and so on:\n",
    "\n",
    "<img src = \"figures/onehot.png\" width=\"400\">\n",
    "\n",
    "This 2D array would take the place of one observation of shape (sequence_length, num_features) = (5, 4) in a batch of sequences. So if our text was “abcdba”—of length 6—and we wanted to feed sequences of length 5 into our array, the first sequence would be transformed into the preceding matrix, and the second sequence would be:\n",
    "\n",
    "<img src = \"figures/onehot2.png\" width=\"400\">\n",
    "\n",
    "These would be then concatenated together to create an input to the RNN of shape (batch_size, sequence_length, vocab_size) = (2, 5, 4). Continuing in this way, we can take raw text and transform it into a batch of sequences to be fed into an RNN.\n",
    "\n",
    "Thus, once we select a batch of data to be fed through—with each element of the batch simply a string—we must first preprocess it, one-hot encoding each letter and concatenating the resulting vectors into a sequence to transform each string of length sequence_length into an ndarray of shape (sequence_length, vocab_size). To form the batches that get will fed into our RNN, these ndarrays will then be concatenated together to form the batch of size (sequence_length, vocab_size, batch_size).\n",
    "\n",
    "But once the data has been preprocessed and the model defined, RNNs are trained in the same way as other neural networks we’ve seen: batches are iteratively fed through, the model’s predictions are compared to the targets to generate the loss, and the loss is backpropagated through the operations that make up the model to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 text_file: str, \n",
    "                 model: RNNModel,\n",
    "                 optim: RNNOptimizer,\n",
    "                 batch_size: int = 32):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "    \n",
    "\n",
    "    def _generate_inputs_targets(self, \n",
    "                                 start_pos: int):\n",
    "        \n",
    "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
    "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
    "\n",
    "        return inputs_indices, targets_indices\n",
    "\n",
    "\n",
    "    def _generate_one_hot_array(self, \n",
    "                                indices: ndarray):\n",
    "        '''\n",
    "        param indices: numpy array of shape (batch_size, sequence_length)\n",
    "        return batch - numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        ''' \n",
    "        batch = []\n",
    "        for seq in indices:\n",
    "            \n",
    "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
    "            \n",
    "            for i in range(self.sequence_length):\n",
    "                one_hot_sequence[i, seq[i]] = 1.0\n",
    "\n",
    "            batch.append(one_hot_sequence) \n",
    "\n",
    "        return np.stack(batch)\n",
    "\n",
    "\n",
    "    def sample_output(self, \n",
    "                      input_char: int, \n",
    "                      sample_length: int):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model, one character at a time.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
    "            \n",
    "            input_char_batch[0, 0, input_char] = 1.0\n",
    "            \n",
    "            x_batch_out = sample_model.forward(input_char_batch)\n",
    "            \n",
    "            x_softmax = batch_softmax(x_batch_out)\n",
    "            \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "    def train(self, \n",
    "              num_iterations: int, \n",
    "              sample_every: int=100):\n",
    "        '''\n",
    "        Trains the \"character generator\" for a number of iterations. \n",
    "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
    "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
    "        '''\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
    "\n",
    "            inputs_batch, targets_batch = \\\n",
    "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
    "            \n",
    "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            self.optim.step()\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.batch_size\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Our `Optimizer` remains the same as well. As we'll see, we'll have to update how we extract the `params` and `param_grads` at each time step, but the *update rules* (which we captured in the _update_rule function in our class) remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 gradient_clipping: bool = True) -> None:\n",
    "        self.lr = lr\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.first = True\n",
    "\n",
    "    def step(self) -> None:\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            for key in layer.params.keys():\n",
    "\n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'])\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "class SGD(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "        update = self.lr*kwargs['grad']\n",
    "        kwargs['param'] -= update\n",
    "        \n",
    "class AdaGrad(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = {}\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.sum_squares[i] = {}\n",
    "                for key in layer.params.keys():\n",
    "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
    "            \n",
    "            self.first = False\n",
    "\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                \n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "                \n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'],\n",
    "                                  sum_square=self.sum_squares[i][key])\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "            # Update running sum of squares\n",
    "            kwargs['sum_square'] += (self.eps +\n",
    "                                     np.power(kwargs['grad'], 2))\n",
    "\n",
    "            # Scale learning rate by running sum of squareds=5\n",
    "            lr = np.divide(self.lr, np.sqrt(kwargs['sum_square']))\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= lr * kwargs['grad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNNodes\n",
    "\n",
    "In most treatments of RNNs, the first thing discussed is the workings of what we are here calling `RNNNodes`. However, we cover these last because this is the most important concepts to understand about RNNs: **the way the data is structured and the way it and the hidden states are routed between layers and through time. As it turns out, there are multiple ways we can implement RNNNodes, the actual processing of the data from a given time step, and updating of the layer’s hidden state**\n",
    "\n",
    "One way produces what is usually thought of as a *regular* recurrent neural network, which we'll refer to here by another common term: a **vanilla RNN.** However, there are other, more complicated ways that produce different variants of RNNs; one of these, for example, is a variant with RNNNodes called **GRUs**, which stands for**Gated Recurrent Units.** Often, GRUs and other RNN variants are described as being significantly different from vanilla RNNs; however, it is important to understand that all RNN variants share the structure of the layers that we’ve seen so far—for example, they all pass data forward in time in the same way, updating their hidden state(s) at each time step. The only way they differ is in the internal workings of these *nodes*.\n",
    "\n",
    "Another variant is **LSTMs**, or **Long Short Term Memory** cells. The only difference with these is that `LSTMLayers` require two quantities to be *remembered* by the layer and updated as sequence elements are passed forward through time: in addition to a *hidden state*, there is a *cell state* stored in the layer that allows it to better model long-term dependencies. This leads to some minor differences in how we would implement an `LSTMLayer` as opposed to an `RNNLayer`; for example, an `LSTMLayer` would have two `ndarrays` to store the layer’s state throughout the time steps:\n",
    "\n",
    "- An `ndarray start_H` of shape `(1, hidden_size)`, representing the layer’s hidden state\n",
    "\n",
    "- An ndarray start_C of shape `(1, cell_size)`, representing the layer’s cell state\n",
    "\n",
    "Each `LSTMNode`, therefore, should take in the input, as well as both the hidden state and the cell state. On the forward pass, this will look like:\n",
    "\n",
    "`y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in self.params)`\n",
    "\n",
    "as well as:\n",
    "\n",
    "`grad_out, h_in_grad, c_in_grad =\n",
    "    self.nodes[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)`\n",
    "    \n",
    "in the backward method.\n",
    "\n",
    "There are many more variants than the three mentioned here, some of which, such as LSTMs with *peephole connections* have a cell state in addition to only a hidden state, and some of which maintain only a hidden state.  Still, a layer made up of `LSTMPeepholeConnectionNodes` would fit into an `RNNLayer` in the same way as the variants we've seen so far and would thus have the same forward and backward methods. This basic structure of RNN—the way data is routed forward through layers, as well as forward through time steps, and then routed in the opposite direction during the backward pass—is what makes recurrent neural networks unique. The actual structural differences between a vanilla RNN and an LSTM-based RNN, for example, are relatively minor, even though they can have dramatically different performance.\n",
    "\n",
    "With that, let’s look at the implementation of an RNNNode.\n",
    "\n",
    "#### “Vanilla” RNNNodes\n",
    "\n",
    "RNNs receive data one sequence element at a time; for example, if we are predicting the price of oil, at each time step, the `RNN` will receive information about the features we are using to predict the price at that time step. In addition, the `RNN` will have in its *hidden state* an encoding representing cumulative information about what has happened at prior time steps. We want to combine these two pieces of data—the features at the time step, and the cumulative information from all the prior time steps—into a prediction at that time step, as well as an updated hidden state.\n",
    "\n",
    "To understand how `RNNs` should accomplish this, recall what happens in a regular neural network. In a feed-forward neural network, each layer receives a set of *learned features* from the prior layer, each of which is a combination of the original features that the network has learned is useful. The layer then multiplies these features by a weight matrix that allows the layer to learn features that are combinations of the features the layer received as input. To level set and normalize the output, respectively, we add a **bias** to these new features and feed them through an activation function.\n",
    "\n",
    "In recurrent neural networks, we want our updated hidden state to be a combination of both the input and the old hidden state. Thus, similar to what happens in regular neural networks:\n",
    "\n",
    "We first concatenate the input and the hidden state. Then we multiply this value by a weight matrix, add a bias, and feed the result through the Tanh activation function. This is our updated hidden state.\n",
    "\n",
    "Next, we multiply this new hidden state by a weight matrix that transforms the hidden state into an output with the dimension that we want. For example, if we are using this RNN to predict a single continuous value at each time step, we’ll multiply the hidden state by a weight matrix of size `(hidden_size, 1)`.\n",
    "\n",
    "Thus, our updated hidden state will be a function of both the input received at that time step as well as the prior hidden state, and the output will be the result of feeding this updated hidden state through the operations of a fully connected layer.\n",
    "\n",
    "Let’s code this up.\n",
    "\n",
    "#### The backward pass\n",
    "\n",
    "The backward pass through an `RNNNode` simply computes the value of the gradients of the loss with respect to the inputs to the RNNNode, given gradients of the loss with respect to the outputs of the `RNNNode`. Since we can represent an `RNNNode` as a series of operations, we can simply compute the derivative of each operation evaluated at its input, and successively multiply these derivatives together with the ones that have come before (taking care to handle matrix multiplication correctly) to end up with ndarrays representing the gradients of the loss with respect to each of the inputs. The following code accomplishes this:\n",
    "\n",
    "<code>def forward(self,\n",
    "            x_in: ndarray,\n",
    "            H_in: ndarray,\n",
    "            params_dict: Dict[str, Dict[str, ndarray]]\n",
    "            ) -> Tuple[ndarray]:\n",
    "    '''\n",
    "    param x: numpy array of shape (batch_size, vocab_size)\n",
    "    param H_prev: numpy array of shape (batch_size, hidden_size)\n",
    "    return self.x_out: numpy array of shape (batch_size, vocab_size)\n",
    "    return self.H: numpy array of shape (batch_size, hidden_size)\n",
    "    '''\n",
    "    self.X_in = x_in\n",
    "    self.H_in = H_in\n",
    "    self.Z = np.column_stack((x_in, H_in))\n",
    "    self.H_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "    self.H_out = tanh(self.H_int)\n",
    "    self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "    return self.X_out, self.H_out</code>\n",
    "    \n",
    "Note that just as in our Operations from before, the shapes of the inputs to the backward function must match the shapes of the outputs of the forward function, and the shapes of the outputs of the backward function must match the shapes of the inputs to the forward function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,\n",
    "                x_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]\n",
    "                ) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param x: numpy array of shape (batch_size, vocab_size)\n",
    "        param H_prev: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.x_out: numpy array of shape (batch_size, vocab_size)\n",
    "        return self.H: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = x_in\n",
    "        self.H_in = H_in\n",
    "    \n",
    "        self.Z = np.column_stack((x_in, H_in))\n",
    "        \n",
    "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) \\\n",
    "                                    + params_dict['B_f']['value']\n",
    "        \n",
    "        self.H_out = tanh(self.H_int)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) \\\n",
    "                                        + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray,\n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param x_out_grad: numpy array of shape (batch_size, vocab_size)\n",
    "        param h_out_grad: numpy array of shape (batch_size, hidden_size)\n",
    "        param RNN_Params: RNN_Params object\n",
    "        return x_in_grad: numpy array of shape (batch_size, vocab_size)\n",
    "        return h_in_grad: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        \n",
    "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
    "        dh += H_out_grad\n",
    "        \n",
    "        dH_int = dh * dtanh(self.H_int)\n",
    "        \n",
    "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)     \n",
    "        \n",
    "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
    "\n",
    "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
    "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
    "\n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        \n",
    "        return X_in_grad, H_in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r mito Zooto.g\n",
      "\n",
      "sFVt, urd the y fosU\n",
      "bues yy Ioutt,\n",
      "AiWl theri'lmeyOe so,v\n",
      "M!\n",
      "Al?tt\n",
      "andUsas mod.\n",
      "An\n",
      "me\n",
      "Thith:\n",
      "hace shennemman moe ho., oot hy whauwdso harethaL fotvtle keath the thiiche nor;, as ooy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [RNNLayer(hidden_size=256, output_size=62)]\n",
    "mod = RNNModel(layers=layers,\n",
    "               vocab_size=62, sequence_length=10,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.001, gradient_clipping=True)\n",
    "trainer = RNNTrainer('resources/input.txt', mod, optim)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of “Vanilla” RNNNodes\n",
    "\n",
    "Remember: the purpose of RNNs is to model dependencies in sequences of data. Thinking of modeling the price of oil as our canonical example, this means that we should be able to uncover the relationship between the sequence of features we’ve seen in the last several time steps and what will happen with the price of oil in the next time step. But how long should *several* be? For the price of oil, we might imagine that the relationship between what happened yesterday—one time step before—would be most important for predicting the price of oil tomorrow, with the day before being less important, and the importance generally decaying as we move backward in time.\n",
    "\n",
    "While this is true for many real-world problems, there are domains to which we’d like to apply RNNs where we would want to learn extremely long-range dependencies. Language modeling is the canonical example here—that is, building a model that can predict the next character, word, or word part, given a theoretically extremely long series of past words or characters (since this is a particularly prevalent application, we’ll discuss some details specific to language modeling later in this chapter). For this, vanilla RNNs are usually insufficient. Now that we’ve seen their details, we can understand why: at each time step, the hidden state is multiplied by the same weight matrix across all time steps in the layer. **Consider what happens when we multiply a number by a value x over and over again: if x < 1, the number decreases exponentially to 0, and if x > 1, the number increases exponentially to infinity**. \n",
    "\n",
    "Recurrent neural networks have the same issues: over long time horizons, because the same set of weights is multiplied by the hidden state at each time step, the gradient for these weights tends to become either extremely small or extremely large. The former is known as the vanishing gradient problem and the latter is known as the exploding gradient problem. Both make it hard to train RNNs to model the very long term dependencies (50–100 time steps) needed for high-quality language modeling. The two commonly used modifications of the vanilla RNN architectures we’ll cover next both significantly mitigate this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n",
    "Recall this is how LSTM looks like:\n",
    "\n",
    "<img src = \"figures/rnn_all.png\" width=\"500\">\n",
    "\n",
    "Vanilla RNNs can be described as taking the input and hidden state, combining them, and using the matrix multiplication to determine how to “weigh” the information contained in the hidden state against the information in the new input to predict the output. The insight that motivates more advanced RNN variants is that to model long-term dependencies, such as those that exist in language, we sometimes receive information that tells us we need to “forget” or “reset” our hidden state. A simple example is a period “.” or a colon “:”—if a language model receives one of these, it knows that it should forget the characters that came before and begin modeling a new pattern in the sequence of characters.\n",
    "\n",
    "A first, simple variant on vanilla RNNs that leverages this insight is GRUs or Gated Recurrent Units, so named because the input and the prior hidden state are passed through a series of “gates.”\n",
    "\n",
    "- The first gate is similar to the operations that take place in vanilla RNNs: the input and hidden state are concatenated together, multiplied by a weight matrix, and then passed through a sigmoid operation. We can think of the output of this as the “update” gate.   This can be formulated as equation like this:\n",
    "\n",
    "$$z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$$\n",
    "\n",
    "- The second gate is interpreted as a “reset” gate: the input and hidden state are concatenated, multiplied by a weight matrix, passed through a sigmoid operation.  This can be formulated as equation like this:\n",
    "\n",
    "$$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$$\n",
    "\n",
    "\n",
    "- The output from the second gate is then multiplied by the prior hidden state. This allows the network to “learn to forget” what was in the hidden state, given the particular input that was passed in.   It is then added with the input and passed through the Tanh function, with the output being a “candidate” for the new hidden state.\n",
    "\n",
    "$$h^{'}_t = tanh(Wx_i + r_i \\odot Uh_{t-1})$$\n",
    "\n",
    "- Finally, the hidden state is updated to be the update gate times the “candidate” for the new hidden state, plus the old hidden state times 1 minus the update gate.\n",
    "\n",
    "$$h_t = z_t \\odot h_{t-1} + (1-z_t) \\odot h^{'}_t$$\n",
    "\n",
    "This whole process can be summarized in this simple picture:\n",
    "\n",
    "<img src = \"figures/gru.png\" width=\"350\">\n",
    "\n",
    "\n",
    "### GRUNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
    "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.X_out: numpy array of shape (batch_size, vocab_size)\n",
    "        return self.H_out: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = X_in\n",
    "        self.H_in = H_in        \n",
    "        \n",
    "        # reset gate\n",
    "        self.X_r = np.dot(X_in, params_dict['W_xr']['value'])\n",
    "        self.H_r = np.dot(H_in, params_dict['W_hr']['value'])\n",
    "\n",
    "        # update gate        \n",
    "        self.X_u = np.dot(X_in, params_dict['W_xu']['value'])\n",
    "        self.H_u = np.dot(H_in, params_dict['W_hu']['value'])        \n",
    "        \n",
    "        # gates   \n",
    "        self.r_int = self.X_r + self.H_r + params_dict['B_r']['value']\n",
    "        self.r = sigmoid(self.r_int)\n",
    "        \n",
    "        self.u_int = self.X_r + self.H_r + params_dict['B_u']['value']\n",
    "        self.u = sigmoid(self.u_int)\n",
    "\n",
    "        # new state        \n",
    "        self.h_reset = self.r * H_in\n",
    "        self.X_h = np.dot(X_in, params_dict['W_xh']['value'])\n",
    "        self.H_h = np.dot(self.h_reset, params_dict['W_hh']['value']) \n",
    "        self.h_bar_int = self.X_h + self.H_h + params_dict['B_h']['value']\n",
    "        self.h_bar = tanh(self.h_bar_int)        \n",
    "        \n",
    "        self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        \n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        du = self.H_in * H_out_grad - self.h_bar * H_out_grad \n",
    "        dh_bar = (1 - self.u) * H_out_grad\n",
    "        \n",
    "        dh_bar_int = dh_bar * dtanh(self.h_bar_int)\n",
    "        params_dict['B_h']['deriv'] += dh_bar_int.sum(axis=0)\n",
    "        params_dict['W_xh']['deriv'] += np.dot(self.X_in.T, dh_bar_int)\n",
    "        \n",
    "        dX_in = np.dot(dh_bar_int, params_dict['W_xh']['value'].T)\n",
    " \n",
    "        params_dict['W_hh']['deriv'] += np.dot(self.h_reset.T, dh_bar_int)\n",
    "        dh_reset = np.dot(dh_bar_int, params_dict['W_hh']['value'].T)   \n",
    "        \n",
    "        dr = dh_reset * self.H_in\n",
    "        dH_in = dh_reset * self.r        \n",
    "        \n",
    "        # update branch\n",
    "        du_int = dsigmoid(self.u_int) * du\n",
    "        params_dict['B_u']['deriv'] += du_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(du_int, params_dict['W_xu']['value'].T)\n",
    "        params_dict['W_xu']['deriv'] += np.dot(self.X_in.T, du_int)\n",
    "        \n",
    "        dH_in += np.dot(du_int, params_dict['W_hu']['value'].T)\n",
    "        params_dict['W_hu']['deriv'] += np.dot(self.H_in.T, du_int)        \n",
    "\n",
    "        # reset branch\n",
    "        dr_int = dsigmoid(self.r_int) * dr\n",
    "        params_dict['B_r']['deriv'] += dr_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(dr_int, params_dict['W_xr']['value'].T)\n",
    "        params_dict['W_xr']['deriv'] += np.dot(self.X_in.T, dr_int)\n",
    "        \n",
    "        dH_in += np.dot(dr_int, params_dict['W_hr']['value'].T)\n",
    "        params_dict['W_hr']['deriv'] += np.dot(self.H_in.T, dr_int)   \n",
    "        \n",
    "        return dX_in, dH_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))       \n",
    "        self.first = True\n",
    "\n",
    "        \n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W_xr'] = {}\n",
    "        self.params['W_hr'] = {}\n",
    "        self.params['B_r'] = {}\n",
    "        self.params['W_xu'] = {}\n",
    "        self.params['W_hu'] = {}\n",
    "        self.params['B_u'] = {}\n",
    "        self.params['W_xh'] = {}\n",
    "        self.params['W_hh'] = {}\n",
    "        self.params['B_h'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_xr']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hr']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.hidden_size, self.hidden_size))        \n",
    "        self.params['B_r']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_xu']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hu']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size, self.hidden_size))\n",
    "        self.params['B_u']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_xh']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hh']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size, self.hidden_size))\n",
    "        self.params['B_h']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(1, self.output_size))    \n",
    "        \n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        self.cells = [GRUNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "\n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)      \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)     \n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Long Short-Term Memory)\n",
    "\n",
    "Long Short Term Memory cells, or LSTMs, are the most popular variant of vanilla RNN cells. Part of the reason for this is that they were invented in the early days of deep learning, back in 1997.  Whereas investigation into LSTM alternatives such as GRUs has just accelerated in the last several years (GRUs were proposed in 2014, for example).\n",
    "\n",
    "Like GRUs, LSTMs are motivated by the desire to give the RNN the ability to “reset” or “forget” its hidden state as it receives new input. In GRUs, this is achieved by feeding the input and hidden state through a “series of gates, as well as computing a “proposed” new hidden state using these gates—self.h_bar, computed using the gate self.r—and then computing the final hidden state using a weighted average of the proposed new hidden state and the old hidden state, controlled by an update gate:\n",
    "\n",
    "<code>self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar</code>\n",
    "\n",
    "LSTMs, by contrast, use a separate “state” vector, the “cell state,” to determine whether to “forget” what is in the hidden state. They then use two other gates to control the extent to which they should reset or update what is in the cell state, and a fourth gate to determine the extent to which the hidden state gets updated based on the final cell state.\n",
    "\n",
    "Let's study step by step, so we can later put this understanding into the code.  Firstly, this is how LSTM looks like.  Instead for single neural network layer, there are four, interacting in a special way.\n",
    "\n",
    "<img src = \"figures/lstm_all.png\" width=\"600\">\n",
    "\n",
    "The key to LSTMs is the **cell state**, the horizontal line running through the top of the diagram (see below).  The cell state is kind of like a conveyor belt.  To interact with this cell state, LSTM has the ability to remove or add information to this cell state, which of course is regulated by **gates**\n",
    "\n",
    "<img src = \"figures/lstm-cell.png\" width=\"600\">\n",
    "\n",
    "Gates, similar to GRU, receive input from a **sigmoid layer** which squeezes numbers between zero and one, which describe how much of each component should be let through.  For example, 0 means nothing will be through and opposite for 1.  To apply the gates, we perform a simple **elementwise multiplication**.  This works simply because 0 multiplies something will diminish the impact and opposite effect for 1.\n",
    "\n",
    "LSTM has **three of these gates** to protect and control the cell state.\n",
    "\n",
    "<img src = \"figures/gate.png\" width=\"100\">\n",
    "\n",
    "<img src = \"figures/lstm1.png\" width=\"600\">\n",
    "\n",
    "<img src = \"figures/lstm2.png\" width=\"600\">\n",
    "\n",
    "<img src = \"figures/lstm3.png\" width=\"600\">\n",
    "\n",
    "<img src = \"figures/lstm4.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray, \n",
    "                C_in: ndarray, \n",
    "                params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
    "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
    "        param C_in: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.X_out: numpy array of shape (batch_size, output_size)\n",
    "        return self.H: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.C: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 C_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        assert_same_shape(C_out_grad, self.C_out)\n",
    "\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
    "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
    "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
    "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))        \n",
    "        self.first = True\n",
    "\n",
    "        \n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_i'] = {}\n",
    "        self.params['B_i'] = {}\n",
    "        self.params['W_c'] = {}\n",
    "        self.params['B_c'] = {}\n",
    "        self.params['W_o'] = {}\n",
    "        self.params['B_o'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.output_size))\n",
    "        \n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "        C_in = np.repeat(C_in, batch_size, axis=0)        \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad, c_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[LSTMLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 hidden_size: int,\n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss.backward()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### One LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arhvkhoFrNFE:kYKATodamnnU;uFtyaAg:awSnnlc wemy,A mhty d.e .rlylSdc id cicT :fn:uRn'RbRNs;ul\n",
      "EUBnnTWslveincOYSH\n",
      "Fd\n",
      "RKT\n",
      "oMe tthalGucouwwpach\n",
      "Brwy Me,sce\n",
      "IoRKLxnSh u\n",
      "\n",
      "RhRjRn urthtossbByeAAS k d:o urpaiDw\n"
     ]
    }
   ],
   "source": [
    "layers1 = [LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers1,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('resources/input.txt', mod, optim, batch_size=3)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il, Pluy trit ind roriiu m antl s alaris Rnve oronngeXthe sawor yalf ger 'onr: hee.\n",
      ":g\n",
      "\n",
      "AndEINhesleu: aey anash sacr\n",
      ",ermeld\n",
      "\n",
      "\n",
      "ixAI: purouar brdeagy dod swey uend, uiwnl o in thee tonhreal thist im, i\n"
     ]
    }
   ],
   "source": [
    "layers3 = [GRULayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers3,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('resources/input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
