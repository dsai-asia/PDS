{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.8 Deep Learning -  PyTorch - Variational Autoencoders\n",
    "\n",
    "- Variational Autoencoders - https://arxiv.org/abs/1312.6114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders\n",
    "\n",
    "Variational autoencoders (VAEs) are a group of generative models in the field of deep learning.\n",
    "\n",
    "#### Standard Autoencoders\n",
    "\n",
    "Before we move to variational autoencoders, let's review what is a standard autoencoders.  A standard autoencoder consists of an encoder and a decoder. Let the input data be X. The encoder produces the latent space vector z from X. Then the decoder tries to reconstruct the input data X from the latent vector z.  Hence, the network serves the purpose of dimensionality reduction.\n",
    "\n",
    "The problem is that they can only reconstruct those types of images on which they are trained. In other words, they compress the data while producing the latent vector and try to replicate the output to the input.  Another limitation is that the latent space vectors are not continuous. This means that we can only replicate the output images to input images. But we cannot generate new images from the latent space vector. This is where variational autoencoders work much better than standard autoencoders.\n",
    "\n",
    "#### Variational Autoencoders\n",
    "\n",
    "The concept of variational autoencoders was introduced by Diederik P Kingma and Max Welling in their paper Auto-Encoding Variational Bayes.\n",
    "\n",
    "Variational autoencoders or VAEs are really good at generating new images from the latent vector. Although, they also reconstruct images similar to the data they are trained on, but they can generate many variations of the images.\n",
    "\n",
    "Moreover, the latent vector space of variational autoencoders is continous which helps them in generating new images.\n",
    "\n",
    "In architecture, VAEs resemble a standard autoencoder. VAEs also consist of an encoder and a decoder. The major difference – the latent vector generated by VAEs is continuous (that is, it generates a distribution, not a latent vector) which makes them a part of the generative model family.\n",
    "\n",
    "#### Types of Variational Autoencoders\n",
    "\n",
    "VAEs also allow us to control or condition the outputs of the decoder to some extent. This conditioning of the decoder’s actions leads to the concept of **Conditional Variational Autoencoders (CVAEs)**.\n",
    "\n",
    "We can also have variational autoencoders that learn from latent vectors which have more disentanglement. As such, disentanglement can lead to learning a broader set of features from the input data to the latent vectors. This, we can control through a parameter called beta ($\\beta$). Such VAEs are called $\\beta$-VAEs.\n",
    "\n",
    "Here, we will take a look at the simple VAE.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "In the case of an autoencoder, we have $z$ as the latent vector, $x$ as the input, $P(x)$ be the probability distribution of input data, $P(z)$ be the probability distribution of the latent variable, $Q(z|x)$ as the encoder, and $P(x|z)$ be the distribution of generating data given latent variable, or the decoder.  \n",
    "\n",
    "The loss function of variational autoencoders consist of two parts: (a) the KL divergence, and (b) the reconstruction loss:\n",
    "\n",
    "$$\\mathcal{L}(\\theta, \\phi;x^{(i)}) = -D_{KL}(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) + \\mathbb{E}_{z{\\tilde{}}q}[logp_{\\theta}(x|z)]$$\n",
    "\n",
    "For the first term KL divergence, it measures the similarity between two distributions where $D_{KL}$ outputs a big number when two distibution is close.   What we want is to make sure the learned distribution ($q_{\\phi}(z|x^{(i)}$) is not far from actual distribution ($p_{\\theta}(z)$ assumed to be normal for easy calculation) where $\\mu=0$ and $\\sigma=1$.  The reason is because if it is not normal, it will be like standard autoencoders ($\\mu=$scalar and $\\sigma=0$) which does not output a distribution but instead a non-continuous latent vector.\n",
    "\n",
    "The loss function of this KL divergence that we need to minimize can be written as:\n",
    "\n",
    "$$-D_{KL}(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) = -\\frac{1}{2}\\sum_{j=1}^{N}{(1+log(\\sigma_j)^2-(\\mu_j)^2-(\\sigma_j)^2)}$$\n",
    "\n",
    "Here, $\\sigma_j$ is the standard deviation and $\\mu_j$ is the mean.  It's minimized when $m_j = 0$ and $\\sigma_j = 1$.\n",
    "\n",
    "For the second term, it is simply maximize the expectation of the reconstruction of real data points from the latent vector.  In another words, we simply compare the real data points with generated data points, and minimize the reconstruction loss.  Commonly, the second term can be denoted $\\mathcal{L}_R$ and commonly calculated using the Binary Cross-Entrophy Loss (BCELoss).\n",
    "\n",
    "So the final VAE loss that we need to optimize is:\n",
    "\n",
    "$$\\mathcal{L}_{VAE} = \\mathcal{L}_R + \\mathcal{L}_{KL}$$\n",
    "\n",
    "Finally, we need to sample from the input space using the following formula.\n",
    "\n",
    "$$Sample = \\mu + \\epsilon\\sigma$$\n",
    "\n",
    "Here, $\\epsilon\\sigma$ is element-wise multiplication. And the above formula is called the **reparameterization trick** in VAE. This perhaps is the most important part of a variational autoencoder. This makes it look like as if the sampling is coming from the input space instead of the latent vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Epoch[1/15], Step [10/469], Reconst Loss: 35997.9648, KL Div: 3528.9819\n",
      "Epoch[1/15], Step [20/469], Reconst Loss: 29627.0117, KL Div: 1011.9289\n",
      "Epoch[1/15], Step [30/469], Reconst Loss: 26666.1836, KL Div: 1153.9070\n",
      "Epoch[1/15], Step [40/469], Reconst Loss: 25800.6367, KL Div: 715.3208\n",
      "Epoch[1/15], Step [50/469], Reconst Loss: 25302.5469, KL Div: 741.5033\n",
      "Epoch[1/15], Step [60/469], Reconst Loss: 25220.3906, KL Div: 931.3972\n",
      "Epoch[1/15], Step [70/469], Reconst Loss: 24315.3418, KL Div: 1034.2733\n",
      "Epoch[1/15], Step [80/469], Reconst Loss: 23148.3418, KL Div: 1108.7080\n",
      "Epoch[1/15], Step [90/469], Reconst Loss: 23523.0215, KL Div: 1347.5452\n",
      "Epoch[1/15], Step [100/469], Reconst Loss: 21146.2910, KL Div: 1451.1224\n",
      "Epoch[1/15], Step [110/469], Reconst Loss: 21475.1055, KL Div: 1522.2439\n",
      "Epoch[1/15], Step [120/469], Reconst Loss: 21172.3984, KL Div: 1563.0320\n",
      "Epoch[1/15], Step [130/469], Reconst Loss: 19318.7832, KL Div: 1613.6377\n",
      "Epoch[1/15], Step [140/469], Reconst Loss: 19856.7520, KL Div: 1654.1300\n",
      "Epoch[1/15], Step [150/469], Reconst Loss: 17899.4883, KL Div: 1755.7971\n",
      "Epoch[1/15], Step [160/469], Reconst Loss: 19319.0469, KL Div: 1828.2179\n",
      "Epoch[1/15], Step [170/469], Reconst Loss: 18686.7109, KL Div: 1820.7501\n",
      "Epoch[1/15], Step [180/469], Reconst Loss: 17865.4043, KL Div: 1774.9282\n",
      "Epoch[1/15], Step [190/469], Reconst Loss: 18852.3047, KL Div: 1926.3187\n",
      "Epoch[1/15], Step [200/469], Reconst Loss: 18098.5176, KL Div: 2063.0305\n",
      "Epoch[1/15], Step [210/469], Reconst Loss: 17352.6895, KL Div: 2052.8108\n",
      "Epoch[1/15], Step [220/469], Reconst Loss: 16276.5225, KL Div: 2054.1057\n",
      "Epoch[1/15], Step [230/469], Reconst Loss: 16430.9512, KL Div: 2093.8989\n",
      "Epoch[1/15], Step [240/469], Reconst Loss: 16611.4355, KL Div: 2092.0488\n",
      "Epoch[1/15], Step [250/469], Reconst Loss: 17164.4355, KL Div: 2170.1274\n",
      "Epoch[1/15], Step [260/469], Reconst Loss: 16407.2988, KL Div: 2057.6040\n",
      "Epoch[1/15], Step [270/469], Reconst Loss: 16239.6152, KL Div: 2008.6499\n",
      "Epoch[1/15], Step [280/469], Reconst Loss: 16289.2344, KL Div: 2148.6035\n",
      "Epoch[1/15], Step [290/469], Reconst Loss: 16392.8262, KL Div: 2123.4275\n",
      "Epoch[1/15], Step [300/469], Reconst Loss: 15658.5820, KL Div: 2266.1592\n",
      "Epoch[1/15], Step [310/469], Reconst Loss: 17259.8223, KL Div: 2506.1089\n",
      "Epoch[1/15], Step [320/469], Reconst Loss: 16445.2617, KL Div: 2348.4944\n",
      "Epoch[1/15], Step [330/469], Reconst Loss: 15760.2617, KL Div: 2224.5916\n",
      "Epoch[1/15], Step [340/469], Reconst Loss: 14718.7891, KL Div: 2353.4028\n",
      "Epoch[1/15], Step [350/469], Reconst Loss: 15448.9043, KL Div: 2438.0984\n",
      "Epoch[1/15], Step [360/469], Reconst Loss: 14358.6699, KL Div: 2409.1169\n",
      "Epoch[1/15], Step [370/469], Reconst Loss: 14469.4102, KL Div: 2382.4370\n",
      "Epoch[1/15], Step [380/469], Reconst Loss: 14031.4180, KL Div: 2385.2324\n",
      "Epoch[1/15], Step [390/469], Reconst Loss: 14360.9131, KL Div: 2542.8608\n",
      "Epoch[1/15], Step [400/469], Reconst Loss: 15072.5234, KL Div: 2523.3101\n",
      "Epoch[1/15], Step [410/469], Reconst Loss: 14175.0029, KL Div: 2462.8560\n",
      "Epoch[1/15], Step [420/469], Reconst Loss: 14385.1367, KL Div: 2453.5198\n",
      "Epoch[1/15], Step [430/469], Reconst Loss: 14899.7744, KL Div: 2576.5898\n",
      "Epoch[1/15], Step [440/469], Reconst Loss: 14529.9785, KL Div: 2644.2014\n",
      "Epoch[1/15], Step [450/469], Reconst Loss: 14505.2656, KL Div: 2679.0010\n",
      "Epoch[1/15], Step [460/469], Reconst Loss: 13376.2129, KL Div: 2627.6399\n",
      "Epoch[2/15], Step [10/469], Reconst Loss: 13587.7598, KL Div: 2621.5061\n",
      "Epoch[2/15], Step [20/469], Reconst Loss: 13948.2666, KL Div: 2541.4644\n",
      "Epoch[2/15], Step [30/469], Reconst Loss: 13761.8369, KL Div: 2470.5439\n",
      "Epoch[2/15], Step [40/469], Reconst Loss: 13598.5391, KL Div: 2726.3438\n",
      "Epoch[2/15], Step [50/469], Reconst Loss: 13706.1719, KL Div: 2790.6421\n",
      "Epoch[2/15], Step [60/469], Reconst Loss: 14159.0713, KL Div: 2596.5276\n",
      "Epoch[2/15], Step [70/469], Reconst Loss: 13107.5508, KL Div: 2668.1318\n",
      "Epoch[2/15], Step [80/469], Reconst Loss: 13277.4658, KL Div: 2797.3289\n",
      "Epoch[2/15], Step [90/469], Reconst Loss: 13447.3994, KL Div: 2642.7275\n",
      "Epoch[2/15], Step [100/469], Reconst Loss: 13571.4932, KL Div: 2649.5186\n",
      "Epoch[2/15], Step [110/469], Reconst Loss: 13443.5137, KL Div: 2879.8701\n",
      "Epoch[2/15], Step [120/469], Reconst Loss: 13068.3506, KL Div: 2836.1997\n",
      "Epoch[2/15], Step [130/469], Reconst Loss: 13363.6914, KL Div: 2936.0034\n",
      "Epoch[2/15], Step [140/469], Reconst Loss: 13008.6973, KL Div: 2716.3274\n",
      "Epoch[2/15], Step [150/469], Reconst Loss: 13159.7324, KL Div: 2849.9871\n",
      "Epoch[2/15], Step [160/469], Reconst Loss: 12523.9043, KL Div: 2792.7539\n",
      "Epoch[2/15], Step [170/469], Reconst Loss: 13317.6699, KL Div: 2858.8875\n",
      "Epoch[2/15], Step [180/469], Reconst Loss: 12753.6172, KL Div: 2857.6816\n",
      "Epoch[2/15], Step [190/469], Reconst Loss: 12327.4219, KL Div: 2874.2441\n",
      "Epoch[2/15], Step [200/469], Reconst Loss: 12153.9590, KL Div: 2781.1357\n",
      "Epoch[2/15], Step [210/469], Reconst Loss: 13115.9795, KL Div: 2877.7505\n",
      "Epoch[2/15], Step [220/469], Reconst Loss: 13382.8916, KL Div: 2778.0249\n",
      "Epoch[2/15], Step [230/469], Reconst Loss: 12453.2773, KL Div: 2962.9009\n",
      "Epoch[2/15], Step [240/469], Reconst Loss: 12500.0664, KL Div: 2968.8721\n",
      "Epoch[2/15], Step [250/469], Reconst Loss: 12848.9170, KL Div: 2890.6021\n",
      "Epoch[2/15], Step [260/469], Reconst Loss: 12117.5361, KL Div: 2844.9802\n",
      "Epoch[2/15], Step [270/469], Reconst Loss: 13114.1836, KL Div: 2946.9639\n",
      "Epoch[2/15], Step [280/469], Reconst Loss: 12483.4521, KL Div: 2862.4995\n",
      "Epoch[2/15], Step [290/469], Reconst Loss: 13038.8340, KL Div: 2910.9006\n",
      "Epoch[2/15], Step [300/469], Reconst Loss: 12314.7705, KL Div: 2824.9536\n",
      "Epoch[2/15], Step [310/469], Reconst Loss: 12563.4287, KL Div: 2867.5061\n",
      "Epoch[2/15], Step [320/469], Reconst Loss: 12385.3838, KL Div: 2916.1714\n",
      "Epoch[2/15], Step [330/469], Reconst Loss: 12830.0889, KL Div: 2976.6113\n",
      "Epoch[2/15], Step [340/469], Reconst Loss: 12897.6973, KL Div: 2977.2917\n",
      "Epoch[2/15], Step [350/469], Reconst Loss: 12556.0117, KL Div: 3038.8799\n",
      "Epoch[2/15], Step [360/469], Reconst Loss: 11702.1182, KL Div: 2906.0952\n",
      "Epoch[2/15], Step [370/469], Reconst Loss: 12212.1836, KL Div: 2977.8728\n",
      "Epoch[2/15], Step [380/469], Reconst Loss: 12398.0000, KL Div: 2955.7605\n",
      "Epoch[2/15], Step [390/469], Reconst Loss: 12655.1992, KL Div: 2938.1113\n",
      "Epoch[2/15], Step [400/469], Reconst Loss: 11730.3623, KL Div: 2836.5342\n",
      "Epoch[2/15], Step [410/469], Reconst Loss: 12210.3750, KL Div: 2932.9304\n",
      "Epoch[2/15], Step [420/469], Reconst Loss: 12101.3486, KL Div: 2892.8286\n",
      "Epoch[2/15], Step [430/469], Reconst Loss: 11997.2725, KL Div: 3007.8125\n",
      "Epoch[2/15], Step [440/469], Reconst Loss: 12034.4561, KL Div: 2883.0273\n",
      "Epoch[2/15], Step [450/469], Reconst Loss: 11463.8477, KL Div: 3080.3882\n",
      "Epoch[2/15], Step [460/469], Reconst Loss: 12363.2236, KL Div: 2877.4434\n",
      "Epoch[3/15], Step [10/469], Reconst Loss: 12082.8867, KL Div: 3102.1562\n",
      "Epoch[3/15], Step [20/469], Reconst Loss: 12212.3623, KL Div: 2914.5698\n",
      "Epoch[3/15], Step [30/469], Reconst Loss: 11574.4502, KL Div: 2962.0955\n",
      "Epoch[3/15], Step [40/469], Reconst Loss: 11613.5098, KL Div: 2870.0764\n",
      "Epoch[3/15], Step [50/469], Reconst Loss: 11825.4395, KL Div: 3030.5684\n",
      "Epoch[3/15], Step [60/469], Reconst Loss: 11973.0518, KL Div: 2945.2148\n",
      "Epoch[3/15], Step [70/469], Reconst Loss: 12165.3535, KL Div: 2962.2087\n",
      "Epoch[3/15], Step [80/469], Reconst Loss: 11697.1758, KL Div: 2995.9822\n",
      "Epoch[3/15], Step [90/469], Reconst Loss: 11543.5781, KL Div: 2902.6509\n",
      "Epoch[3/15], Step [100/469], Reconst Loss: 11867.0322, KL Div: 3107.7969\n",
      "Epoch[3/15], Step [110/469], Reconst Loss: 11605.2930, KL Div: 3072.7173\n",
      "Epoch[3/15], Step [120/469], Reconst Loss: 11782.7812, KL Div: 2994.5039\n",
      "Epoch[3/15], Step [130/469], Reconst Loss: 11776.3320, KL Div: 3095.8586\n",
      "Epoch[3/15], Step [140/469], Reconst Loss: 11777.7314, KL Div: 3147.4626\n",
      "Epoch[3/15], Step [150/469], Reconst Loss: 11307.5312, KL Div: 2985.7000\n",
      "Epoch[3/15], Step [160/469], Reconst Loss: 11533.4736, KL Div: 2898.8281\n",
      "Epoch[3/15], Step [170/469], Reconst Loss: 11879.4424, KL Div: 3195.4077\n",
      "Epoch[3/15], Step [180/469], Reconst Loss: 11912.4580, KL Div: 2958.3237\n",
      "Epoch[3/15], Step [190/469], Reconst Loss: 12206.6250, KL Div: 3083.7058\n",
      "Epoch[3/15], Step [200/469], Reconst Loss: 11635.5898, KL Div: 3015.2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/15], Step [210/469], Reconst Loss: 11635.3604, KL Div: 3144.9187\n",
      "Epoch[3/15], Step [220/469], Reconst Loss: 11702.9619, KL Div: 3065.3311\n",
      "Epoch[3/15], Step [230/469], Reconst Loss: 11753.8125, KL Div: 3040.0229\n",
      "Epoch[3/15], Step [240/469], Reconst Loss: 11721.6045, KL Div: 3043.5205\n",
      "Epoch[3/15], Step [250/469], Reconst Loss: 12098.2383, KL Div: 3077.1523\n",
      "Epoch[3/15], Step [260/469], Reconst Loss: 11703.6641, KL Div: 3160.4749\n",
      "Epoch[3/15], Step [270/469], Reconst Loss: 11087.0938, KL Div: 2960.7227\n",
      "Epoch[3/15], Step [280/469], Reconst Loss: 11588.9531, KL Div: 3056.7163\n",
      "Epoch[3/15], Step [290/469], Reconst Loss: 11843.6113, KL Div: 3065.3877\n",
      "Epoch[3/15], Step [300/469], Reconst Loss: 11823.7510, KL Div: 3017.7693\n",
      "Epoch[3/15], Step [310/469], Reconst Loss: 11397.3057, KL Div: 3147.2952\n",
      "Epoch[3/15], Step [320/469], Reconst Loss: 11551.8701, KL Div: 3165.4055\n",
      "Epoch[3/15], Step [330/469], Reconst Loss: 11111.6357, KL Div: 3018.5281\n",
      "Epoch[3/15], Step [340/469], Reconst Loss: 11756.5215, KL Div: 3148.3420\n",
      "Epoch[3/15], Step [350/469], Reconst Loss: 11716.5518, KL Div: 3168.0605\n",
      "Epoch[3/15], Step [360/469], Reconst Loss: 11804.3115, KL Div: 3119.8516\n",
      "Epoch[3/15], Step [370/469], Reconst Loss: 11464.1338, KL Div: 3114.9055\n",
      "Epoch[3/15], Step [380/469], Reconst Loss: 11560.7295, KL Div: 3193.6416\n",
      "Epoch[3/15], Step [390/469], Reconst Loss: 11199.0146, KL Div: 3016.7651\n",
      "Epoch[3/15], Step [400/469], Reconst Loss: 11539.7861, KL Div: 3108.3555\n",
      "Epoch[3/15], Step [410/469], Reconst Loss: 11678.9678, KL Div: 3055.7427\n",
      "Epoch[3/15], Step [420/469], Reconst Loss: 11215.5566, KL Div: 3093.2253\n",
      "Epoch[3/15], Step [430/469], Reconst Loss: 11222.8047, KL Div: 3179.9709\n",
      "Epoch[3/15], Step [440/469], Reconst Loss: 10893.6816, KL Div: 3106.2192\n",
      "Epoch[3/15], Step [450/469], Reconst Loss: 11584.7764, KL Div: 3156.1528\n",
      "Epoch[3/15], Step [460/469], Reconst Loss: 11578.4316, KL Div: 3067.3682\n",
      "Epoch[4/15], Step [10/469], Reconst Loss: 11097.5439, KL Div: 3063.6045\n",
      "Epoch[4/15], Step [20/469], Reconst Loss: 11318.4053, KL Div: 3074.1338\n",
      "Epoch[4/15], Step [30/469], Reconst Loss: 11021.7188, KL Div: 3132.0913\n",
      "Epoch[4/15], Step [40/469], Reconst Loss: 10976.6533, KL Div: 3112.8096\n",
      "Epoch[4/15], Step [50/469], Reconst Loss: 11257.0869, KL Div: 3016.4731\n",
      "Epoch[4/15], Step [60/469], Reconst Loss: 11204.2061, KL Div: 3131.0559\n",
      "Epoch[4/15], Step [70/469], Reconst Loss: 11534.0244, KL Div: 3181.6528\n",
      "Epoch[4/15], Step [80/469], Reconst Loss: 11347.1221, KL Div: 3070.4106\n",
      "Epoch[4/15], Step [90/469], Reconst Loss: 11461.9746, KL Div: 3085.8872\n",
      "Epoch[4/15], Step [100/469], Reconst Loss: 11129.5049, KL Div: 3158.0344\n",
      "Epoch[4/15], Step [110/469], Reconst Loss: 11083.0508, KL Div: 3205.2385\n",
      "Epoch[4/15], Step [120/469], Reconst Loss: 11686.0850, KL Div: 3078.4810\n",
      "Epoch[4/15], Step [130/469], Reconst Loss: 11103.9736, KL Div: 3222.8157\n",
      "Epoch[4/15], Step [140/469], Reconst Loss: 11328.2588, KL Div: 3085.3486\n",
      "Epoch[4/15], Step [150/469], Reconst Loss: 11384.8955, KL Div: 3199.6909\n",
      "Epoch[4/15], Step [160/469], Reconst Loss: 11384.8105, KL Div: 3090.9734\n",
      "Epoch[4/15], Step [170/469], Reconst Loss: 10761.3867, KL Div: 3089.4700\n",
      "Epoch[4/15], Step [180/469], Reconst Loss: 10932.3711, KL Div: 3148.7778\n",
      "Epoch[4/15], Step [190/469], Reconst Loss: 11444.7129, KL Div: 3115.8457\n",
      "Epoch[4/15], Step [200/469], Reconst Loss: 11069.4111, KL Div: 3117.9392\n",
      "Epoch[4/15], Step [210/469], Reconst Loss: 10864.3418, KL Div: 3106.2683\n",
      "Epoch[4/15], Step [220/469], Reconst Loss: 10598.8623, KL Div: 3123.2646\n",
      "Epoch[4/15], Step [230/469], Reconst Loss: 11610.3447, KL Div: 3115.2583\n",
      "Epoch[4/15], Step [240/469], Reconst Loss: 10867.7861, KL Div: 3107.9331\n",
      "Epoch[4/15], Step [250/469], Reconst Loss: 11106.6113, KL Div: 3288.3379\n",
      "Epoch[4/15], Step [260/469], Reconst Loss: 11358.9150, KL Div: 3218.2407\n",
      "Epoch[4/15], Step [270/469], Reconst Loss: 11365.5693, KL Div: 3112.9653\n",
      "Epoch[4/15], Step [280/469], Reconst Loss: 11237.9658, KL Div: 3313.2317\n",
      "Epoch[4/15], Step [290/469], Reconst Loss: 10720.0732, KL Div: 3085.6106\n",
      "Epoch[4/15], Step [300/469], Reconst Loss: 10977.2373, KL Div: 3112.4834\n",
      "Epoch[4/15], Step [310/469], Reconst Loss: 11461.9443, KL Div: 3135.6565\n",
      "Epoch[4/15], Step [320/469], Reconst Loss: 10724.2529, KL Div: 3104.8906\n",
      "Epoch[4/15], Step [330/469], Reconst Loss: 11304.1016, KL Div: 3186.7500\n",
      "Epoch[4/15], Step [340/469], Reconst Loss: 11045.1064, KL Div: 3121.7664\n",
      "Epoch[4/15], Step [350/469], Reconst Loss: 11003.2812, KL Div: 3203.5427\n",
      "Epoch[4/15], Step [360/469], Reconst Loss: 11440.9795, KL Div: 3250.4631\n",
      "Epoch[4/15], Step [370/469], Reconst Loss: 11445.7520, KL Div: 3190.8438\n",
      "Epoch[4/15], Step [380/469], Reconst Loss: 10933.8184, KL Div: 3037.6255\n",
      "Epoch[4/15], Step [390/469], Reconst Loss: 11007.0381, KL Div: 3116.2229\n",
      "Epoch[4/15], Step [400/469], Reconst Loss: 11229.4600, KL Div: 3214.4167\n",
      "Epoch[4/15], Step [410/469], Reconst Loss: 11293.0361, KL Div: 3197.6047\n",
      "Epoch[4/15], Step [420/469], Reconst Loss: 11301.1885, KL Div: 3165.3079\n",
      "Epoch[4/15], Step [430/469], Reconst Loss: 11211.5312, KL Div: 3137.3188\n",
      "Epoch[4/15], Step [440/469], Reconst Loss: 11122.6641, KL Div: 3179.3521\n",
      "Epoch[4/15], Step [450/469], Reconst Loss: 10781.3105, KL Div: 3134.6519\n",
      "Epoch[4/15], Step [460/469], Reconst Loss: 10301.2930, KL Div: 3090.0671\n",
      "Epoch[5/15], Step [10/469], Reconst Loss: 10445.1797, KL Div: 3084.8591\n",
      "Epoch[5/15], Step [20/469], Reconst Loss: 10703.0322, KL Div: 3106.8865\n",
      "Epoch[5/15], Step [30/469], Reconst Loss: 10566.6787, KL Div: 3151.8167\n",
      "Epoch[5/15], Step [40/469], Reconst Loss: 10773.2002, KL Div: 3202.2278\n",
      "Epoch[5/15], Step [50/469], Reconst Loss: 11116.2744, KL Div: 3167.7671\n",
      "Epoch[5/15], Step [60/469], Reconst Loss: 11293.2236, KL Div: 3358.9570\n",
      "Epoch[5/15], Step [70/469], Reconst Loss: 11127.5068, KL Div: 3085.1196\n",
      "Epoch[5/15], Step [80/469], Reconst Loss: 10994.4277, KL Div: 3247.5684\n",
      "Epoch[5/15], Step [90/469], Reconst Loss: 10997.0898, KL Div: 3235.8972\n",
      "Epoch[5/15], Step [100/469], Reconst Loss: 10850.8057, KL Div: 3231.0654\n",
      "Epoch[5/15], Step [110/469], Reconst Loss: 11295.2295, KL Div: 3161.5386\n",
      "Epoch[5/15], Step [120/469], Reconst Loss: 10711.1797, KL Div: 3233.7207\n",
      "Epoch[5/15], Step [130/469], Reconst Loss: 11134.4756, KL Div: 3242.9009\n",
      "Epoch[5/15], Step [140/469], Reconst Loss: 11182.8066, KL Div: 3173.1372\n",
      "Epoch[5/15], Step [150/469], Reconst Loss: 10967.4111, KL Div: 3156.9089\n",
      "Epoch[5/15], Step [160/469], Reconst Loss: 10772.6699, KL Div: 3145.9875\n",
      "Epoch[5/15], Step [170/469], Reconst Loss: 10819.8945, KL Div: 3100.9688\n",
      "Epoch[5/15], Step [180/469], Reconst Loss: 11300.2246, KL Div: 3158.2673\n",
      "Epoch[5/15], Step [190/469], Reconst Loss: 11331.8496, KL Div: 3263.4351\n",
      "Epoch[5/15], Step [200/469], Reconst Loss: 10668.7217, KL Div: 3157.8782\n",
      "Epoch[5/15], Step [210/469], Reconst Loss: 10932.0059, KL Div: 3226.9971\n",
      "Epoch[5/15], Step [220/469], Reconst Loss: 11080.6787, KL Div: 3154.5867\n",
      "Epoch[5/15], Step [230/469], Reconst Loss: 11562.2959, KL Div: 3252.8936\n",
      "Epoch[5/15], Step [240/469], Reconst Loss: 10580.8750, KL Div: 3117.7493\n",
      "Epoch[5/15], Step [250/469], Reconst Loss: 11056.3672, KL Div: 3123.6941\n",
      "Epoch[5/15], Step [260/469], Reconst Loss: 10862.1045, KL Div: 3149.7827\n",
      "Epoch[5/15], Step [270/469], Reconst Loss: 11130.9121, KL Div: 3127.9741\n",
      "Epoch[5/15], Step [280/469], Reconst Loss: 10560.0576, KL Div: 3258.9810\n",
      "Epoch[5/15], Step [290/469], Reconst Loss: 11088.5156, KL Div: 3211.4917\n",
      "Epoch[5/15], Step [300/469], Reconst Loss: 10285.3975, KL Div: 3132.3926\n",
      "Epoch[5/15], Step [310/469], Reconst Loss: 11123.3164, KL Div: 3172.2959\n",
      "Epoch[5/15], Step [320/469], Reconst Loss: 10853.9902, KL Div: 3083.6448\n",
      "Epoch[5/15], Step [330/469], Reconst Loss: 10911.4990, KL Div: 3227.0847\n",
      "Epoch[5/15], Step [340/469], Reconst Loss: 10748.0859, KL Div: 3091.9048\n",
      "Epoch[5/15], Step [350/469], Reconst Loss: 11049.6572, KL Div: 3186.1226\n",
      "Epoch[5/15], Step [360/469], Reconst Loss: 10169.4697, KL Div: 3193.5518\n",
      "Epoch[5/15], Step [370/469], Reconst Loss: 10421.1992, KL Div: 3099.1943\n",
      "Epoch[5/15], Step [380/469], Reconst Loss: 10336.9561, KL Div: 3065.1799\n",
      "Epoch[5/15], Step [390/469], Reconst Loss: 10950.6113, KL Div: 3208.8918\n",
      "Epoch[5/15], Step [400/469], Reconst Loss: 11176.4600, KL Div: 3153.2874\n",
      "Epoch[5/15], Step [410/469], Reconst Loss: 10785.2793, KL Div: 3182.6326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/15], Step [420/469], Reconst Loss: 10776.7852, KL Div: 3163.8567\n",
      "Epoch[5/15], Step [430/469], Reconst Loss: 10950.8770, KL Div: 3286.9265\n",
      "Epoch[5/15], Step [440/469], Reconst Loss: 10385.1270, KL Div: 3066.8127\n",
      "Epoch[5/15], Step [450/469], Reconst Loss: 11020.1416, KL Div: 3126.3542\n",
      "Epoch[5/15], Step [460/469], Reconst Loss: 10962.5557, KL Div: 3231.2507\n",
      "Epoch[6/15], Step [10/469], Reconst Loss: 10895.3213, KL Div: 3172.8396\n",
      "Epoch[6/15], Step [20/469], Reconst Loss: 10927.1631, KL Div: 3287.9685\n",
      "Epoch[6/15], Step [30/469], Reconst Loss: 10296.2803, KL Div: 3256.9756\n",
      "Epoch[6/15], Step [40/469], Reconst Loss: 10378.1797, KL Div: 3157.3025\n",
      "Epoch[6/15], Step [50/469], Reconst Loss: 10792.6729, KL Div: 3095.2041\n",
      "Epoch[6/15], Step [60/469], Reconst Loss: 10180.3223, KL Div: 3139.9712\n",
      "Epoch[6/15], Step [70/469], Reconst Loss: 11175.7939, KL Div: 3292.1858\n",
      "Epoch[6/15], Step [80/469], Reconst Loss: 10681.0254, KL Div: 3243.1489\n",
      "Epoch[6/15], Step [90/469], Reconst Loss: 10498.7510, KL Div: 3115.7705\n",
      "Epoch[6/15], Step [100/469], Reconst Loss: 10405.3623, KL Div: 3194.6121\n",
      "Epoch[6/15], Step [110/469], Reconst Loss: 10472.7266, KL Div: 3161.6602\n",
      "Epoch[6/15], Step [120/469], Reconst Loss: 10664.7939, KL Div: 3181.4143\n",
      "Epoch[6/15], Step [130/469], Reconst Loss: 10679.0762, KL Div: 3190.9214\n",
      "Epoch[6/15], Step [140/469], Reconst Loss: 10610.6592, KL Div: 3296.7830\n",
      "Epoch[6/15], Step [150/469], Reconst Loss: 11218.6709, KL Div: 3217.4478\n",
      "Epoch[6/15], Step [160/469], Reconst Loss: 10475.0381, KL Div: 3139.7446\n",
      "Epoch[6/15], Step [170/469], Reconst Loss: 10522.4805, KL Div: 3284.6892\n",
      "Epoch[6/15], Step [180/469], Reconst Loss: 10442.2412, KL Div: 3066.0874\n",
      "Epoch[6/15], Step [190/469], Reconst Loss: 10829.9395, KL Div: 3284.1194\n",
      "Epoch[6/15], Step [200/469], Reconst Loss: 10783.2529, KL Div: 3219.5962\n",
      "Epoch[6/15], Step [210/469], Reconst Loss: 10368.4609, KL Div: 3174.5371\n",
      "Epoch[6/15], Step [220/469], Reconst Loss: 10967.8691, KL Div: 3181.8030\n",
      "Epoch[6/15], Step [230/469], Reconst Loss: 10998.3643, KL Div: 3239.2473\n",
      "Epoch[6/15], Step [240/469], Reconst Loss: 10968.5273, KL Div: 3201.3853\n",
      "Epoch[6/15], Step [250/469], Reconst Loss: 10850.2715, KL Div: 3202.2263\n",
      "Epoch[6/15], Step [260/469], Reconst Loss: 10404.6338, KL Div: 3221.4863\n",
      "Epoch[6/15], Step [270/469], Reconst Loss: 10902.7061, KL Div: 3343.4968\n",
      "Epoch[6/15], Step [280/469], Reconst Loss: 10647.4873, KL Div: 3179.0334\n",
      "Epoch[6/15], Step [290/469], Reconst Loss: 10472.3047, KL Div: 3160.1670\n",
      "Epoch[6/15], Step [300/469], Reconst Loss: 10221.4531, KL Div: 3156.9788\n",
      "Epoch[6/15], Step [310/469], Reconst Loss: 10729.6191, KL Div: 3154.4006\n",
      "Epoch[6/15], Step [320/469], Reconst Loss: 10377.8232, KL Div: 3159.2124\n",
      "Epoch[6/15], Step [330/469], Reconst Loss: 10689.5537, KL Div: 3205.8821\n",
      "Epoch[6/15], Step [340/469], Reconst Loss: 10471.1152, KL Div: 3255.9199\n",
      "Epoch[6/15], Step [350/469], Reconst Loss: 10706.4941, KL Div: 3132.5864\n",
      "Epoch[6/15], Step [360/469], Reconst Loss: 10413.5684, KL Div: 3198.6230\n",
      "Epoch[6/15], Step [370/469], Reconst Loss: 10465.4434, KL Div: 3230.9536\n",
      "Epoch[6/15], Step [380/469], Reconst Loss: 10388.0381, KL Div: 3100.5730\n",
      "Epoch[6/15], Step [390/469], Reconst Loss: 10976.8682, KL Div: 3310.1489\n",
      "Epoch[6/15], Step [400/469], Reconst Loss: 10553.4326, KL Div: 3163.0864\n",
      "Epoch[6/15], Step [410/469], Reconst Loss: 11085.7383, KL Div: 3243.1382\n",
      "Epoch[6/15], Step [420/469], Reconst Loss: 10540.8945, KL Div: 3181.9973\n",
      "Epoch[6/15], Step [430/469], Reconst Loss: 10465.8213, KL Div: 3270.9370\n",
      "Epoch[6/15], Step [440/469], Reconst Loss: 10738.6836, KL Div: 3167.9927\n",
      "Epoch[6/15], Step [450/469], Reconst Loss: 10640.5420, KL Div: 3260.8108\n",
      "Epoch[6/15], Step [460/469], Reconst Loss: 10004.1562, KL Div: 3217.7578\n",
      "Epoch[7/15], Step [10/469], Reconst Loss: 10588.7246, KL Div: 3262.8318\n",
      "Epoch[7/15], Step [20/469], Reconst Loss: 10114.5117, KL Div: 3205.3586\n",
      "Epoch[7/15], Step [30/469], Reconst Loss: 10717.9678, KL Div: 3309.8289\n",
      "Epoch[7/15], Step [40/469], Reconst Loss: 10387.5703, KL Div: 3177.3396\n",
      "Epoch[7/15], Step [50/469], Reconst Loss: 10343.1885, KL Div: 3253.5706\n",
      "Epoch[7/15], Step [60/469], Reconst Loss: 10766.2109, KL Div: 3192.6348\n",
      "Epoch[7/15], Step [70/469], Reconst Loss: 10937.4395, KL Div: 3240.6675\n",
      "Epoch[7/15], Step [80/469], Reconst Loss: 10696.3682, KL Div: 3163.2568\n",
      "Epoch[7/15], Step [90/469], Reconst Loss: 10730.4795, KL Div: 3278.1516\n",
      "Epoch[7/15], Step [100/469], Reconst Loss: 10481.5430, KL Div: 3150.0071\n",
      "Epoch[7/15], Step [110/469], Reconst Loss: 10234.5664, KL Div: 3248.6692\n",
      "Epoch[7/15], Step [120/469], Reconst Loss: 10483.0068, KL Div: 3177.9478\n",
      "Epoch[7/15], Step [130/469], Reconst Loss: 10990.7256, KL Div: 3226.5078\n",
      "Epoch[7/15], Step [140/469], Reconst Loss: 10208.2266, KL Div: 3084.1111\n",
      "Epoch[7/15], Step [150/469], Reconst Loss: 10328.3057, KL Div: 3131.1567\n",
      "Epoch[7/15], Step [160/469], Reconst Loss: 10795.3691, KL Div: 3199.6113\n",
      "Epoch[7/15], Step [170/469], Reconst Loss: 10564.7871, KL Div: 3193.0847\n",
      "Epoch[7/15], Step [180/469], Reconst Loss: 10624.3789, KL Div: 3104.2603\n",
      "Epoch[7/15], Step [190/469], Reconst Loss: 10439.5811, KL Div: 3235.0913\n",
      "Epoch[7/15], Step [200/469], Reconst Loss: 11278.6758, KL Div: 3230.6599\n",
      "Epoch[7/15], Step [210/469], Reconst Loss: 10913.1426, KL Div: 3303.8501\n",
      "Epoch[7/15], Step [220/469], Reconst Loss: 10676.1768, KL Div: 3216.3926\n",
      "Epoch[7/15], Step [230/469], Reconst Loss: 10873.4209, KL Div: 3079.0874\n",
      "Epoch[7/15], Step [240/469], Reconst Loss: 10294.2314, KL Div: 3226.5691\n",
      "Epoch[7/15], Step [250/469], Reconst Loss: 11140.4170, KL Div: 3372.9683\n",
      "Epoch[7/15], Step [260/469], Reconst Loss: 10903.3525, KL Div: 3220.9084\n",
      "Epoch[7/15], Step [270/469], Reconst Loss: 10315.6514, KL Div: 3177.6016\n",
      "Epoch[7/15], Step [280/469], Reconst Loss: 10227.9404, KL Div: 3172.9961\n",
      "Epoch[7/15], Step [290/469], Reconst Loss: 10566.2480, KL Div: 3321.5681\n",
      "Epoch[7/15], Step [300/469], Reconst Loss: 10420.8838, KL Div: 3191.4902\n",
      "Epoch[7/15], Step [310/469], Reconst Loss: 10627.1992, KL Div: 3152.7341\n",
      "Epoch[7/15], Step [320/469], Reconst Loss: 10489.2676, KL Div: 3316.7905\n",
      "Epoch[7/15], Step [330/469], Reconst Loss: 10217.0029, KL Div: 3122.9773\n",
      "Epoch[7/15], Step [340/469], Reconst Loss: 10594.9004, KL Div: 3175.7351\n",
      "Epoch[7/15], Step [350/469], Reconst Loss: 10640.2178, KL Div: 3263.1868\n",
      "Epoch[7/15], Step [360/469], Reconst Loss: 11154.7676, KL Div: 3208.3901\n",
      "Epoch[7/15], Step [370/469], Reconst Loss: 10657.1611, KL Div: 3348.7485\n",
      "Epoch[7/15], Step [380/469], Reconst Loss: 10511.9775, KL Div: 3194.1687\n",
      "Epoch[7/15], Step [390/469], Reconst Loss: 10824.2051, KL Div: 3200.3013\n",
      "Epoch[7/15], Step [400/469], Reconst Loss: 10381.0684, KL Div: 3150.6174\n",
      "Epoch[7/15], Step [410/469], Reconst Loss: 11029.5664, KL Div: 3339.9651\n",
      "Epoch[7/15], Step [420/469], Reconst Loss: 10077.1973, KL Div: 3187.5198\n",
      "Epoch[7/15], Step [430/469], Reconst Loss: 10728.2441, KL Div: 3309.9390\n",
      "Epoch[7/15], Step [440/469], Reconst Loss: 10406.6230, KL Div: 3183.9600\n",
      "Epoch[7/15], Step [450/469], Reconst Loss: 10714.9580, KL Div: 3242.9595\n",
      "Epoch[7/15], Step [460/469], Reconst Loss: 10683.2598, KL Div: 3320.8486\n",
      "Epoch[8/15], Step [10/469], Reconst Loss: 10598.8281, KL Div: 3227.4292\n",
      "Epoch[8/15], Step [20/469], Reconst Loss: 10590.9766, KL Div: 3169.6216\n",
      "Epoch[8/15], Step [30/469], Reconst Loss: 10558.7881, KL Div: 3308.2041\n",
      "Epoch[8/15], Step [40/469], Reconst Loss: 10276.5557, KL Div: 3151.6218\n",
      "Epoch[8/15], Step [50/469], Reconst Loss: 10472.5508, KL Div: 3199.3865\n",
      "Epoch[8/15], Step [60/469], Reconst Loss: 10626.3945, KL Div: 3200.4204\n",
      "Epoch[8/15], Step [70/469], Reconst Loss: 10423.7607, KL Div: 3178.2205\n",
      "Epoch[8/15], Step [80/469], Reconst Loss: 10466.9902, KL Div: 3232.4712\n",
      "Epoch[8/15], Step [90/469], Reconst Loss: 10452.8848, KL Div: 3193.5679\n",
      "Epoch[8/15], Step [100/469], Reconst Loss: 10513.2490, KL Div: 3190.6704\n",
      "Epoch[8/15], Step [110/469], Reconst Loss: 10376.1992, KL Div: 3179.3435\n",
      "Epoch[8/15], Step [120/469], Reconst Loss: 10385.8994, KL Div: 3174.0256\n",
      "Epoch[8/15], Step [130/469], Reconst Loss: 10698.7363, KL Div: 3167.1921\n",
      "Epoch[8/15], Step [140/469], Reconst Loss: 10727.0791, KL Div: 3184.1089\n",
      "Epoch[8/15], Step [150/469], Reconst Loss: 10194.5371, KL Div: 3177.0161\n",
      "Epoch[8/15], Step [160/469], Reconst Loss: 10123.7744, KL Div: 3173.5745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/15], Step [170/469], Reconst Loss: 10385.6758, KL Div: 3283.0322\n",
      "Epoch[8/15], Step [180/469], Reconst Loss: 10606.3740, KL Div: 3188.4648\n",
      "Epoch[8/15], Step [190/469], Reconst Loss: 10237.3799, KL Div: 3314.6216\n",
      "Epoch[8/15], Step [200/469], Reconst Loss: 10470.8691, KL Div: 2997.6401\n",
      "Epoch[8/15], Step [210/469], Reconst Loss: 10357.5771, KL Div: 3343.6084\n",
      "Epoch[8/15], Step [220/469], Reconst Loss: 10124.4609, KL Div: 3174.0579\n",
      "Epoch[8/15], Step [230/469], Reconst Loss: 10637.8008, KL Div: 3159.7700\n",
      "Epoch[8/15], Step [240/469], Reconst Loss: 10922.1846, KL Div: 3227.6096\n",
      "Epoch[8/15], Step [250/469], Reconst Loss: 10509.6768, KL Div: 3170.9534\n",
      "Epoch[8/15], Step [260/469], Reconst Loss: 10759.7139, KL Div: 3322.8594\n",
      "Epoch[8/15], Step [270/469], Reconst Loss: 10450.9434, KL Div: 3214.1401\n",
      "Epoch[8/15], Step [280/469], Reconst Loss: 10713.8047, KL Div: 3272.4685\n",
      "Epoch[8/15], Step [290/469], Reconst Loss: 10453.1494, KL Div: 3220.3774\n",
      "Epoch[8/15], Step [300/469], Reconst Loss: 10825.6523, KL Div: 3267.2537\n",
      "Epoch[8/15], Step [310/469], Reconst Loss: 10439.6250, KL Div: 3264.5527\n",
      "Epoch[8/15], Step [320/469], Reconst Loss: 10891.0000, KL Div: 3213.7776\n",
      "Epoch[8/15], Step [330/469], Reconst Loss: 10535.0352, KL Div: 3241.6560\n",
      "Epoch[8/15], Step [340/469], Reconst Loss: 10810.3984, KL Div: 3210.0745\n",
      "Epoch[8/15], Step [350/469], Reconst Loss: 10495.0898, KL Div: 3247.9448\n",
      "Epoch[8/15], Step [360/469], Reconst Loss: 10458.8555, KL Div: 3174.4014\n",
      "Epoch[8/15], Step [370/469], Reconst Loss: 10730.1055, KL Div: 3226.4912\n",
      "Epoch[8/15], Step [380/469], Reconst Loss: 9896.6113, KL Div: 3209.4204\n",
      "Epoch[8/15], Step [390/469], Reconst Loss: 10699.7100, KL Div: 3211.3882\n",
      "Epoch[8/15], Step [400/469], Reconst Loss: 9880.4697, KL Div: 3150.1611\n",
      "Epoch[8/15], Step [410/469], Reconst Loss: 10192.8584, KL Div: 3192.3562\n",
      "Epoch[8/15], Step [420/469], Reconst Loss: 10427.8340, KL Div: 3246.2888\n",
      "Epoch[8/15], Step [430/469], Reconst Loss: 10540.5137, KL Div: 3273.5596\n",
      "Epoch[8/15], Step [440/469], Reconst Loss: 10323.0938, KL Div: 3303.9431\n",
      "Epoch[8/15], Step [450/469], Reconst Loss: 10532.6943, KL Div: 3260.6819\n",
      "Epoch[8/15], Step [460/469], Reconst Loss: 10531.7275, KL Div: 3324.9600\n",
      "Epoch[9/15], Step [10/469], Reconst Loss: 10462.0957, KL Div: 3184.0688\n",
      "Epoch[9/15], Step [20/469], Reconst Loss: 10492.5244, KL Div: 3229.2495\n",
      "Epoch[9/15], Step [30/469], Reconst Loss: 10374.9121, KL Div: 3143.1875\n",
      "Epoch[9/15], Step [40/469], Reconst Loss: 10299.2402, KL Div: 3286.1147\n",
      "Epoch[9/15], Step [50/469], Reconst Loss: 10083.0195, KL Div: 3175.4475\n",
      "Epoch[9/15], Step [60/469], Reconst Loss: 10512.0488, KL Div: 3250.4299\n",
      "Epoch[9/15], Step [70/469], Reconst Loss: 10459.7021, KL Div: 3263.8047\n",
      "Epoch[9/15], Step [80/469], Reconst Loss: 10594.4619, KL Div: 3144.7009\n",
      "Epoch[9/15], Step [90/469], Reconst Loss: 10580.2559, KL Div: 3170.1848\n",
      "Epoch[9/15], Step [100/469], Reconst Loss: 10391.9600, KL Div: 3231.1960\n",
      "Epoch[9/15], Step [110/469], Reconst Loss: 10711.0146, KL Div: 3262.5334\n",
      "Epoch[9/15], Step [120/469], Reconst Loss: 10473.8555, KL Div: 3178.6077\n",
      "Epoch[9/15], Step [130/469], Reconst Loss: 10130.3145, KL Div: 3170.1165\n",
      "Epoch[9/15], Step [140/469], Reconst Loss: 10572.4678, KL Div: 3259.1545\n",
      "Epoch[9/15], Step [150/469], Reconst Loss: 10271.0596, KL Div: 3300.6050\n",
      "Epoch[9/15], Step [160/469], Reconst Loss: 10624.8574, KL Div: 3277.2354\n",
      "Epoch[9/15], Step [170/469], Reconst Loss: 10726.8037, KL Div: 3266.7751\n",
      "Epoch[9/15], Step [180/469], Reconst Loss: 10534.4502, KL Div: 3202.5176\n",
      "Epoch[9/15], Step [190/469], Reconst Loss: 10616.2822, KL Div: 3201.2942\n",
      "Epoch[9/15], Step [200/469], Reconst Loss: 10332.5176, KL Div: 3212.4956\n",
      "Epoch[9/15], Step [210/469], Reconst Loss: 10728.9229, KL Div: 3327.0210\n",
      "Epoch[9/15], Step [220/469], Reconst Loss: 10728.1152, KL Div: 3221.5476\n",
      "Epoch[9/15], Step [230/469], Reconst Loss: 10898.1172, KL Div: 3104.7595\n",
      "Epoch[9/15], Step [240/469], Reconst Loss: 10287.7002, KL Div: 3172.7615\n",
      "Epoch[9/15], Step [250/469], Reconst Loss: 10801.0664, KL Div: 3217.1545\n",
      "Epoch[9/15], Step [260/469], Reconst Loss: 10588.1191, KL Div: 3198.3892\n",
      "Epoch[9/15], Step [270/469], Reconst Loss: 10144.3926, KL Div: 3213.4185\n",
      "Epoch[9/15], Step [280/469], Reconst Loss: 10558.7373, KL Div: 3167.9614\n",
      "Epoch[9/15], Step [290/469], Reconst Loss: 10387.5342, KL Div: 3252.8853\n",
      "Epoch[9/15], Step [300/469], Reconst Loss: 10602.5762, KL Div: 3183.6736\n",
      "Epoch[9/15], Step [310/469], Reconst Loss: 9800.5869, KL Div: 3207.4216\n",
      "Epoch[9/15], Step [320/469], Reconst Loss: 10236.0039, KL Div: 3113.7996\n",
      "Epoch[9/15], Step [330/469], Reconst Loss: 10152.0068, KL Div: 3339.7478\n",
      "Epoch[9/15], Step [340/469], Reconst Loss: 10230.2773, KL Div: 3138.0269\n",
      "Epoch[9/15], Step [350/469], Reconst Loss: 10106.9297, KL Div: 3145.5757\n",
      "Epoch[9/15], Step [360/469], Reconst Loss: 10790.3398, KL Div: 3199.9751\n",
      "Epoch[9/15], Step [370/469], Reconst Loss: 10175.1797, KL Div: 3267.4268\n",
      "Epoch[9/15], Step [380/469], Reconst Loss: 10826.2852, KL Div: 3159.4744\n",
      "Epoch[9/15], Step [390/469], Reconst Loss: 10983.2910, KL Div: 3292.8511\n",
      "Epoch[9/15], Step [400/469], Reconst Loss: 10282.6162, KL Div: 3195.2937\n",
      "Epoch[9/15], Step [410/469], Reconst Loss: 10599.4561, KL Div: 3215.1211\n",
      "Epoch[9/15], Step [420/469], Reconst Loss: 10305.9316, KL Div: 3182.5913\n",
      "Epoch[9/15], Step [430/469], Reconst Loss: 10545.4727, KL Div: 3306.2551\n",
      "Epoch[9/15], Step [440/469], Reconst Loss: 10519.0850, KL Div: 3144.2422\n",
      "Epoch[9/15], Step [450/469], Reconst Loss: 10461.6504, KL Div: 3271.6021\n",
      "Epoch[9/15], Step [460/469], Reconst Loss: 10643.3369, KL Div: 3197.3235\n",
      "Epoch[10/15], Step [10/469], Reconst Loss: 10409.4580, KL Div: 3242.2734\n",
      "Epoch[10/15], Step [20/469], Reconst Loss: 10128.2031, KL Div: 3239.8906\n",
      "Epoch[10/15], Step [30/469], Reconst Loss: 10152.6357, KL Div: 3131.8022\n",
      "Epoch[10/15], Step [40/469], Reconst Loss: 10229.8955, KL Div: 3197.1792\n",
      "Epoch[10/15], Step [50/469], Reconst Loss: 10531.0527, KL Div: 3236.1201\n",
      "Epoch[10/15], Step [60/469], Reconst Loss: 10656.4082, KL Div: 3309.0850\n",
      "Epoch[10/15], Step [70/469], Reconst Loss: 10343.5557, KL Div: 3297.6238\n",
      "Epoch[10/15], Step [80/469], Reconst Loss: 10472.7891, KL Div: 3317.7056\n",
      "Epoch[10/15], Step [90/469], Reconst Loss: 10206.4082, KL Div: 3275.0940\n",
      "Epoch[10/15], Step [100/469], Reconst Loss: 10325.3115, KL Div: 3251.7065\n",
      "Epoch[10/15], Step [110/469], Reconst Loss: 10401.1582, KL Div: 3270.2283\n",
      "Epoch[10/15], Step [120/469], Reconst Loss: 10665.3926, KL Div: 3115.1902\n",
      "Epoch[10/15], Step [130/469], Reconst Loss: 10030.4307, KL Div: 3174.8511\n",
      "Epoch[10/15], Step [140/469], Reconst Loss: 10321.1885, KL Div: 3256.4192\n",
      "Epoch[10/15], Step [150/469], Reconst Loss: 10744.3408, KL Div: 3227.3008\n",
      "Epoch[10/15], Step [160/469], Reconst Loss: 10306.2764, KL Div: 3279.6743\n",
      "Epoch[10/15], Step [170/469], Reconst Loss: 10563.1934, KL Div: 3250.2859\n",
      "Epoch[10/15], Step [180/469], Reconst Loss: 10670.5088, KL Div: 3267.3457\n",
      "Epoch[10/15], Step [190/469], Reconst Loss: 10384.9629, KL Div: 3322.8521\n",
      "Epoch[10/15], Step [200/469], Reconst Loss: 10402.8906, KL Div: 3171.2529\n",
      "Epoch[10/15], Step [210/469], Reconst Loss: 10018.7041, KL Div: 3216.5854\n",
      "Epoch[10/15], Step [220/469], Reconst Loss: 10529.7402, KL Div: 3251.2737\n",
      "Epoch[10/15], Step [230/469], Reconst Loss: 10218.0703, KL Div: 3259.0840\n",
      "Epoch[10/15], Step [240/469], Reconst Loss: 10519.3965, KL Div: 3168.4678\n",
      "Epoch[10/15], Step [250/469], Reconst Loss: 10425.7412, KL Div: 3192.7563\n",
      "Epoch[10/15], Step [260/469], Reconst Loss: 10696.3252, KL Div: 3198.0518\n",
      "Epoch[10/15], Step [270/469], Reconst Loss: 10293.7080, KL Div: 3209.5835\n",
      "Epoch[10/15], Step [280/469], Reconst Loss: 10633.2275, KL Div: 3358.5718\n",
      "Epoch[10/15], Step [290/469], Reconst Loss: 10482.2764, KL Div: 3245.5181\n",
      "Epoch[10/15], Step [300/469], Reconst Loss: 10559.1973, KL Div: 3267.0854\n",
      "Epoch[10/15], Step [310/469], Reconst Loss: 10383.0137, KL Div: 3250.1567\n",
      "Epoch[10/15], Step [320/469], Reconst Loss: 10023.7373, KL Div: 3112.2173\n",
      "Epoch[10/15], Step [330/469], Reconst Loss: 10466.9082, KL Div: 3206.7998\n",
      "Epoch[10/15], Step [340/469], Reconst Loss: 10655.7344, KL Div: 3231.5278\n",
      "Epoch[10/15], Step [350/469], Reconst Loss: 10148.1787, KL Div: 3231.8879\n",
      "Epoch[10/15], Step [360/469], Reconst Loss: 10358.5566, KL Div: 3172.6970\n",
      "Epoch[10/15], Step [370/469], Reconst Loss: 10501.6133, KL Div: 3220.2122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/15], Step [380/469], Reconst Loss: 10744.4756, KL Div: 3303.2786\n",
      "Epoch[10/15], Step [390/469], Reconst Loss: 10468.0859, KL Div: 3328.0149\n",
      "Epoch[10/15], Step [400/469], Reconst Loss: 10767.8936, KL Div: 3247.0728\n",
      "Epoch[10/15], Step [410/469], Reconst Loss: 10477.1436, KL Div: 3241.9668\n",
      "Epoch[10/15], Step [420/469], Reconst Loss: 10174.9580, KL Div: 3249.3704\n",
      "Epoch[10/15], Step [430/469], Reconst Loss: 10257.8779, KL Div: 3239.3967\n",
      "Epoch[10/15], Step [440/469], Reconst Loss: 10237.5127, KL Div: 3261.8223\n",
      "Epoch[10/15], Step [450/469], Reconst Loss: 10135.8848, KL Div: 3231.4771\n",
      "Epoch[10/15], Step [460/469], Reconst Loss: 10284.6719, KL Div: 3190.0610\n",
      "Epoch[11/15], Step [10/469], Reconst Loss: 10654.4561, KL Div: 3249.2561\n",
      "Epoch[11/15], Step [20/469], Reconst Loss: 10707.9600, KL Div: 3328.6704\n",
      "Epoch[11/15], Step [30/469], Reconst Loss: 10711.5723, KL Div: 3340.4468\n",
      "Epoch[11/15], Step [40/469], Reconst Loss: 10160.0801, KL Div: 3383.0254\n",
      "Epoch[11/15], Step [50/469], Reconst Loss: 10607.1289, KL Div: 3229.6868\n",
      "Epoch[11/15], Step [60/469], Reconst Loss: 9679.0352, KL Div: 3142.0896\n",
      "Epoch[11/15], Step [70/469], Reconst Loss: 10747.5410, KL Div: 3301.9370\n",
      "Epoch[11/15], Step [80/469], Reconst Loss: 10340.8584, KL Div: 3246.4194\n",
      "Epoch[11/15], Step [90/469], Reconst Loss: 10010.2891, KL Div: 3165.6299\n",
      "Epoch[11/15], Step [100/469], Reconst Loss: 10443.7705, KL Div: 3282.9180\n",
      "Epoch[11/15], Step [110/469], Reconst Loss: 10463.6367, KL Div: 3286.8330\n",
      "Epoch[11/15], Step [120/469], Reconst Loss: 10484.4619, KL Div: 3362.7256\n",
      "Epoch[11/15], Step [130/469], Reconst Loss: 10132.6494, KL Div: 3240.4573\n",
      "Epoch[11/15], Step [140/469], Reconst Loss: 10449.5137, KL Div: 3253.3181\n",
      "Epoch[11/15], Step [150/469], Reconst Loss: 10369.6074, KL Div: 3238.6975\n",
      "Epoch[11/15], Step [160/469], Reconst Loss: 10634.0791, KL Div: 3221.4675\n",
      "Epoch[11/15], Step [170/469], Reconst Loss: 10105.1104, KL Div: 3212.0791\n",
      "Epoch[11/15], Step [180/469], Reconst Loss: 9975.5703, KL Div: 3178.9905\n",
      "Epoch[11/15], Step [190/469], Reconst Loss: 10466.3916, KL Div: 3293.3533\n",
      "Epoch[11/15], Step [200/469], Reconst Loss: 9719.6816, KL Div: 3214.5325\n",
      "Epoch[11/15], Step [210/469], Reconst Loss: 10175.5391, KL Div: 3186.7778\n",
      "Epoch[11/15], Step [220/469], Reconst Loss: 10074.8730, KL Div: 3194.4661\n",
      "Epoch[11/15], Step [230/469], Reconst Loss: 10570.6533, KL Div: 3183.9417\n",
      "Epoch[11/15], Step [240/469], Reconst Loss: 10774.9678, KL Div: 3209.1890\n",
      "Epoch[11/15], Step [250/469], Reconst Loss: 10647.6328, KL Div: 3264.5254\n",
      "Epoch[11/15], Step [260/469], Reconst Loss: 10754.5859, KL Div: 3270.4790\n",
      "Epoch[11/15], Step [270/469], Reconst Loss: 10422.5488, KL Div: 3272.2075\n",
      "Epoch[11/15], Step [280/469], Reconst Loss: 10516.6582, KL Div: 3096.6062\n",
      "Epoch[11/15], Step [290/469], Reconst Loss: 10328.0654, KL Div: 3244.3215\n",
      "Epoch[11/15], Step [300/469], Reconst Loss: 10508.3496, KL Div: 3266.5261\n",
      "Epoch[11/15], Step [310/469], Reconst Loss: 10262.2451, KL Div: 3269.6753\n",
      "Epoch[11/15], Step [320/469], Reconst Loss: 10083.4082, KL Div: 3215.5796\n",
      "Epoch[11/15], Step [330/469], Reconst Loss: 10507.8965, KL Div: 3231.6223\n",
      "Epoch[11/15], Step [340/469], Reconst Loss: 9919.1123, KL Div: 3234.6367\n",
      "Epoch[11/15], Step [350/469], Reconst Loss: 9891.4424, KL Div: 3186.5156\n",
      "Epoch[11/15], Step [360/469], Reconst Loss: 10631.5752, KL Div: 3318.0603\n",
      "Epoch[11/15], Step [370/469], Reconst Loss: 9814.1973, KL Div: 3275.0217\n",
      "Epoch[11/15], Step [380/469], Reconst Loss: 10299.1523, KL Div: 3253.8672\n",
      "Epoch[11/15], Step [390/469], Reconst Loss: 9975.5918, KL Div: 3207.5100\n",
      "Epoch[11/15], Step [400/469], Reconst Loss: 9799.9053, KL Div: 3258.8359\n",
      "Epoch[11/15], Step [410/469], Reconst Loss: 10267.7754, KL Div: 3185.2432\n",
      "Epoch[11/15], Step [420/469], Reconst Loss: 10570.3066, KL Div: 3348.3843\n",
      "Epoch[11/15], Step [430/469], Reconst Loss: 10512.2451, KL Div: 3296.3389\n",
      "Epoch[11/15], Step [440/469], Reconst Loss: 10034.2451, KL Div: 3121.2412\n",
      "Epoch[11/15], Step [450/469], Reconst Loss: 10315.0967, KL Div: 3192.7634\n",
      "Epoch[11/15], Step [460/469], Reconst Loss: 10355.2529, KL Div: 3357.0754\n",
      "Epoch[12/15], Step [10/469], Reconst Loss: 10247.2881, KL Div: 3139.6438\n",
      "Epoch[12/15], Step [20/469], Reconst Loss: 10310.5449, KL Div: 3177.2156\n",
      "Epoch[12/15], Step [30/469], Reconst Loss: 10710.0986, KL Div: 3237.5120\n",
      "Epoch[12/15], Step [40/469], Reconst Loss: 10491.3125, KL Div: 3308.5688\n",
      "Epoch[12/15], Step [50/469], Reconst Loss: 10289.7461, KL Div: 3231.8381\n",
      "Epoch[12/15], Step [60/469], Reconst Loss: 10577.8574, KL Div: 3335.4844\n",
      "Epoch[12/15], Step [70/469], Reconst Loss: 10049.3105, KL Div: 3263.6377\n",
      "Epoch[12/15], Step [80/469], Reconst Loss: 9982.9844, KL Div: 3203.7131\n",
      "Epoch[12/15], Step [90/469], Reconst Loss: 10778.0703, KL Div: 3338.6587\n",
      "Epoch[12/15], Step [100/469], Reconst Loss: 10516.3809, KL Div: 3268.7676\n",
      "Epoch[12/15], Step [110/469], Reconst Loss: 10294.4434, KL Div: 3201.4272\n",
      "Epoch[12/15], Step [120/469], Reconst Loss: 10587.8008, KL Div: 3415.8123\n",
      "Epoch[12/15], Step [130/469], Reconst Loss: 10274.4502, KL Div: 3136.2629\n",
      "Epoch[12/15], Step [140/469], Reconst Loss: 11019.8926, KL Div: 3263.2109\n",
      "Epoch[12/15], Step [150/469], Reconst Loss: 10120.5342, KL Div: 3254.4878\n",
      "Epoch[12/15], Step [160/469], Reconst Loss: 10693.4551, KL Div: 3209.2271\n",
      "Epoch[12/15], Step [170/469], Reconst Loss: 10577.2920, KL Div: 3350.8928\n",
      "Epoch[12/15], Step [180/469], Reconst Loss: 10265.1592, KL Div: 3219.7827\n",
      "Epoch[12/15], Step [190/469], Reconst Loss: 9979.9102, KL Div: 3301.8984\n",
      "Epoch[12/15], Step [200/469], Reconst Loss: 10589.6318, KL Div: 3196.1143\n",
      "Epoch[12/15], Step [210/469], Reconst Loss: 9937.6631, KL Div: 3202.8545\n",
      "Epoch[12/15], Step [220/469], Reconst Loss: 9929.8027, KL Div: 3154.4739\n",
      "Epoch[12/15], Step [230/469], Reconst Loss: 10143.9727, KL Div: 3315.9749\n",
      "Epoch[12/15], Step [240/469], Reconst Loss: 10366.0449, KL Div: 3218.4072\n",
      "Epoch[12/15], Step [250/469], Reconst Loss: 9987.0605, KL Div: 3202.9893\n",
      "Epoch[12/15], Step [260/469], Reconst Loss: 10628.1709, KL Div: 3237.1899\n",
      "Epoch[12/15], Step [270/469], Reconst Loss: 10423.8242, KL Div: 3221.0894\n",
      "Epoch[12/15], Step [280/469], Reconst Loss: 10269.3115, KL Div: 3262.9004\n",
      "Epoch[12/15], Step [290/469], Reconst Loss: 9861.5078, KL Div: 3180.7900\n",
      "Epoch[12/15], Step [300/469], Reconst Loss: 9885.8232, KL Div: 3255.4836\n",
      "Epoch[12/15], Step [310/469], Reconst Loss: 10076.7900, KL Div: 3292.5486\n",
      "Epoch[12/15], Step [320/469], Reconst Loss: 10790.7852, KL Div: 3239.8984\n",
      "Epoch[12/15], Step [330/469], Reconst Loss: 9910.9980, KL Div: 3187.6006\n",
      "Epoch[12/15], Step [340/469], Reconst Loss: 10386.3096, KL Div: 3286.3313\n",
      "Epoch[12/15], Step [350/469], Reconst Loss: 10104.6162, KL Div: 3318.0908\n",
      "Epoch[12/15], Step [360/469], Reconst Loss: 10295.1484, KL Div: 3291.1633\n",
      "Epoch[12/15], Step [370/469], Reconst Loss: 10654.2559, KL Div: 3173.2827\n",
      "Epoch[12/15], Step [380/469], Reconst Loss: 10141.9854, KL Div: 3220.6709\n",
      "Epoch[12/15], Step [390/469], Reconst Loss: 10821.9082, KL Div: 3384.4243\n",
      "Epoch[12/15], Step [400/469], Reconst Loss: 10372.8818, KL Div: 3111.2275\n",
      "Epoch[12/15], Step [410/469], Reconst Loss: 10590.8740, KL Div: 3230.1838\n",
      "Epoch[12/15], Step [420/469], Reconst Loss: 10263.6494, KL Div: 3260.3984\n",
      "Epoch[12/15], Step [430/469], Reconst Loss: 10769.9014, KL Div: 3239.4136\n",
      "Epoch[12/15], Step [440/469], Reconst Loss: 10199.5322, KL Div: 3272.7446\n",
      "Epoch[12/15], Step [450/469], Reconst Loss: 10632.2266, KL Div: 3241.0208\n",
      "Epoch[12/15], Step [460/469], Reconst Loss: 10257.9941, KL Div: 3285.0554\n",
      "Epoch[13/15], Step [10/469], Reconst Loss: 10118.9883, KL Div: 3226.9429\n",
      "Epoch[13/15], Step [20/469], Reconst Loss: 9958.4980, KL Div: 3173.6230\n",
      "Epoch[13/15], Step [30/469], Reconst Loss: 10225.4561, KL Div: 3148.5859\n",
      "Epoch[13/15], Step [40/469], Reconst Loss: 10715.5059, KL Div: 3264.6450\n",
      "Epoch[13/15], Step [50/469], Reconst Loss: 10575.9746, KL Div: 3312.2131\n",
      "Epoch[13/15], Step [60/469], Reconst Loss: 10531.9883, KL Div: 3218.0093\n",
      "Epoch[13/15], Step [70/469], Reconst Loss: 10018.8311, KL Div: 3233.3975\n",
      "Epoch[13/15], Step [80/469], Reconst Loss: 10993.2949, KL Div: 3196.0952\n",
      "Epoch[13/15], Step [90/469], Reconst Loss: 10624.1699, KL Div: 3367.5115\n",
      "Epoch[13/15], Step [100/469], Reconst Loss: 10410.5293, KL Div: 3341.3796\n",
      "Epoch[13/15], Step [110/469], Reconst Loss: 9910.7373, KL Div: 3187.8604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/15], Step [120/469], Reconst Loss: 10556.6611, KL Div: 3233.7148\n",
      "Epoch[13/15], Step [130/469], Reconst Loss: 10408.9248, KL Div: 3235.0742\n",
      "Epoch[13/15], Step [140/469], Reconst Loss: 10075.7695, KL Div: 3299.1680\n",
      "Epoch[13/15], Step [150/469], Reconst Loss: 9913.7002, KL Div: 3230.2495\n",
      "Epoch[13/15], Step [160/469], Reconst Loss: 9940.9863, KL Div: 3219.9619\n",
      "Epoch[13/15], Step [170/469], Reconst Loss: 10607.9502, KL Div: 3299.2991\n",
      "Epoch[13/15], Step [180/469], Reconst Loss: 10189.4980, KL Div: 3187.9370\n",
      "Epoch[13/15], Step [190/469], Reconst Loss: 10267.3662, KL Div: 3245.7290\n",
      "Epoch[13/15], Step [200/469], Reconst Loss: 10150.7041, KL Div: 3249.1333\n",
      "Epoch[13/15], Step [210/469], Reconst Loss: 10131.6240, KL Div: 3219.9338\n",
      "Epoch[13/15], Step [220/469], Reconst Loss: 10338.5781, KL Div: 3309.5422\n",
      "Epoch[13/15], Step [230/469], Reconst Loss: 9976.4453, KL Div: 3207.8540\n",
      "Epoch[13/15], Step [240/469], Reconst Loss: 9879.0342, KL Div: 3226.7844\n",
      "Epoch[13/15], Step [250/469], Reconst Loss: 10338.5752, KL Div: 3232.4421\n",
      "Epoch[13/15], Step [260/469], Reconst Loss: 9861.4814, KL Div: 3269.9351\n",
      "Epoch[13/15], Step [270/469], Reconst Loss: 10376.7061, KL Div: 3234.7371\n",
      "Epoch[13/15], Step [280/469], Reconst Loss: 10847.1494, KL Div: 3301.9285\n",
      "Epoch[13/15], Step [290/469], Reconst Loss: 10178.7324, KL Div: 3229.6941\n",
      "Epoch[13/15], Step [300/469], Reconst Loss: 10092.7246, KL Div: 3159.6494\n",
      "Epoch[13/15], Step [310/469], Reconst Loss: 10190.2744, KL Div: 3264.7034\n",
      "Epoch[13/15], Step [320/469], Reconst Loss: 10164.6553, KL Div: 3198.6318\n",
      "Epoch[13/15], Step [330/469], Reconst Loss: 10716.2891, KL Div: 3215.8740\n",
      "Epoch[13/15], Step [340/469], Reconst Loss: 10333.5088, KL Div: 3394.6025\n",
      "Epoch[13/15], Step [350/469], Reconst Loss: 10706.7744, KL Div: 3259.2896\n",
      "Epoch[13/15], Step [360/469], Reconst Loss: 10519.3828, KL Div: 3293.3213\n",
      "Epoch[13/15], Step [370/469], Reconst Loss: 10364.7080, KL Div: 3204.7412\n",
      "Epoch[13/15], Step [380/469], Reconst Loss: 10052.7559, KL Div: 3191.7983\n",
      "Epoch[13/15], Step [390/469], Reconst Loss: 10185.9326, KL Div: 3274.2568\n",
      "Epoch[13/15], Step [400/469], Reconst Loss: 10400.3623, KL Div: 3329.1553\n",
      "Epoch[13/15], Step [410/469], Reconst Loss: 10182.3936, KL Div: 3294.4087\n",
      "Epoch[13/15], Step [420/469], Reconst Loss: 10335.0352, KL Div: 3129.6128\n",
      "Epoch[13/15], Step [430/469], Reconst Loss: 10241.0869, KL Div: 3329.1467\n",
      "Epoch[13/15], Step [440/469], Reconst Loss: 9783.9688, KL Div: 3182.5840\n",
      "Epoch[13/15], Step [450/469], Reconst Loss: 9805.3721, KL Div: 3201.2693\n",
      "Epoch[13/15], Step [460/469], Reconst Loss: 9904.9023, KL Div: 3117.7119\n",
      "Epoch[14/15], Step [10/469], Reconst Loss: 10289.1865, KL Div: 3289.6721\n",
      "Epoch[14/15], Step [20/469], Reconst Loss: 9952.6826, KL Div: 3125.3894\n",
      "Epoch[14/15], Step [30/469], Reconst Loss: 10087.6172, KL Div: 3193.6987\n",
      "Epoch[14/15], Step [40/469], Reconst Loss: 10211.9912, KL Div: 3296.7354\n",
      "Epoch[14/15], Step [50/469], Reconst Loss: 10022.3301, KL Div: 3168.3665\n",
      "Epoch[14/15], Step [60/469], Reconst Loss: 10121.5410, KL Div: 3268.8315\n",
      "Epoch[14/15], Step [70/469], Reconst Loss: 10380.4092, KL Div: 3288.1899\n",
      "Epoch[14/15], Step [80/469], Reconst Loss: 10218.2227, KL Div: 3340.3452\n",
      "Epoch[14/15], Step [90/469], Reconst Loss: 10723.7285, KL Div: 3242.9487\n",
      "Epoch[14/15], Step [100/469], Reconst Loss: 10105.3604, KL Div: 3288.2690\n",
      "Epoch[14/15], Step [110/469], Reconst Loss: 9940.8828, KL Div: 3240.7788\n",
      "Epoch[14/15], Step [120/469], Reconst Loss: 10170.5664, KL Div: 3229.3469\n",
      "Epoch[14/15], Step [130/469], Reconst Loss: 10196.0439, KL Div: 3276.8535\n",
      "Epoch[14/15], Step [140/469], Reconst Loss: 10339.5000, KL Div: 3167.5884\n",
      "Epoch[14/15], Step [150/469], Reconst Loss: 10437.4121, KL Div: 3256.4226\n",
      "Epoch[14/15], Step [160/469], Reconst Loss: 9930.7021, KL Div: 3302.3440\n",
      "Epoch[14/15], Step [170/469], Reconst Loss: 10175.9424, KL Div: 3229.6321\n",
      "Epoch[14/15], Step [180/469], Reconst Loss: 10417.0342, KL Div: 3289.9790\n",
      "Epoch[14/15], Step [190/469], Reconst Loss: 10058.9512, KL Div: 3264.4907\n",
      "Epoch[14/15], Step [200/469], Reconst Loss: 10321.2715, KL Div: 3194.0107\n",
      "Epoch[14/15], Step [210/469], Reconst Loss: 9551.6895, KL Div: 3224.2637\n",
      "Epoch[14/15], Step [220/469], Reconst Loss: 10382.0713, KL Div: 3391.6069\n",
      "Epoch[14/15], Step [230/469], Reconst Loss: 10183.9014, KL Div: 3148.5503\n",
      "Epoch[14/15], Step [240/469], Reconst Loss: 9747.8838, KL Div: 3170.4478\n",
      "Epoch[14/15], Step [250/469], Reconst Loss: 10430.8154, KL Div: 3231.8013\n",
      "Epoch[14/15], Step [260/469], Reconst Loss: 9976.3477, KL Div: 3222.4407\n",
      "Epoch[14/15], Step [270/469], Reconst Loss: 10229.5898, KL Div: 3338.3320\n",
      "Epoch[14/15], Step [280/469], Reconst Loss: 9683.8848, KL Div: 3117.7805\n",
      "Epoch[14/15], Step [290/469], Reconst Loss: 10134.4619, KL Div: 3343.8013\n",
      "Epoch[14/15], Step [300/469], Reconst Loss: 10146.1221, KL Div: 3274.2275\n",
      "Epoch[14/15], Step [310/469], Reconst Loss: 10314.3467, KL Div: 3231.8071\n",
      "Epoch[14/15], Step [320/469], Reconst Loss: 10336.4102, KL Div: 3294.8196\n",
      "Epoch[14/15], Step [330/469], Reconst Loss: 10213.8662, KL Div: 3294.0662\n",
      "Epoch[14/15], Step [340/469], Reconst Loss: 9828.1445, KL Div: 3131.0510\n",
      "Epoch[14/15], Step [350/469], Reconst Loss: 10179.2051, KL Div: 3170.1113\n",
      "Epoch[14/15], Step [360/469], Reconst Loss: 10408.5000, KL Div: 3330.2336\n",
      "Epoch[14/15], Step [370/469], Reconst Loss: 10414.8711, KL Div: 3231.1008\n",
      "Epoch[14/15], Step [380/469], Reconst Loss: 9851.8584, KL Div: 3234.5554\n",
      "Epoch[14/15], Step [390/469], Reconst Loss: 10073.1504, KL Div: 3304.9019\n",
      "Epoch[14/15], Step [400/469], Reconst Loss: 10288.2002, KL Div: 3241.9868\n",
      "Epoch[14/15], Step [410/469], Reconst Loss: 10434.0586, KL Div: 3168.1428\n",
      "Epoch[14/15], Step [420/469], Reconst Loss: 10283.1475, KL Div: 3282.5317\n",
      "Epoch[14/15], Step [430/469], Reconst Loss: 9442.3906, KL Div: 3140.6040\n",
      "Epoch[14/15], Step [440/469], Reconst Loss: 9680.9277, KL Div: 3179.1707\n",
      "Epoch[14/15], Step [450/469], Reconst Loss: 10118.7021, KL Div: 3151.4529\n",
      "Epoch[14/15], Step [460/469], Reconst Loss: 10237.5254, KL Div: 3238.4761\n",
      "Epoch[15/15], Step [10/469], Reconst Loss: 10221.9180, KL Div: 3194.8665\n",
      "Epoch[15/15], Step [20/469], Reconst Loss: 10229.2100, KL Div: 3274.6140\n",
      "Epoch[15/15], Step [30/469], Reconst Loss: 10599.0801, KL Div: 3176.6597\n",
      "Epoch[15/15], Step [40/469], Reconst Loss: 9797.9346, KL Div: 3202.1089\n",
      "Epoch[15/15], Step [50/469], Reconst Loss: 10398.3057, KL Div: 3282.9072\n",
      "Epoch[15/15], Step [60/469], Reconst Loss: 10257.9111, KL Div: 3237.7400\n",
      "Epoch[15/15], Step [70/469], Reconst Loss: 9756.7402, KL Div: 3177.5840\n",
      "Epoch[15/15], Step [80/469], Reconst Loss: 9619.2139, KL Div: 3210.4248\n",
      "Epoch[15/15], Step [90/469], Reconst Loss: 10504.3125, KL Div: 3184.3208\n",
      "Epoch[15/15], Step [100/469], Reconst Loss: 10437.4736, KL Div: 3317.8564\n",
      "Epoch[15/15], Step [110/469], Reconst Loss: 9979.6318, KL Div: 3237.1528\n",
      "Epoch[15/15], Step [120/469], Reconst Loss: 10105.9990, KL Div: 3250.6462\n",
      "Epoch[15/15], Step [130/469], Reconst Loss: 10552.4688, KL Div: 3368.8767\n",
      "Epoch[15/15], Step [140/469], Reconst Loss: 10047.1816, KL Div: 3121.4036\n",
      "Epoch[15/15], Step [150/469], Reconst Loss: 10017.9297, KL Div: 3223.2383\n",
      "Epoch[15/15], Step [160/469], Reconst Loss: 10032.3682, KL Div: 3177.6284\n",
      "Epoch[15/15], Step [170/469], Reconst Loss: 10385.2139, KL Div: 3344.1143\n",
      "Epoch[15/15], Step [180/469], Reconst Loss: 10405.1758, KL Div: 3301.1052\n",
      "Epoch[15/15], Step [190/469], Reconst Loss: 10057.6484, KL Div: 3281.6506\n",
      "Epoch[15/15], Step [200/469], Reconst Loss: 10364.2061, KL Div: 3301.2461\n",
      "Epoch[15/15], Step [210/469], Reconst Loss: 10480.0195, KL Div: 3331.7446\n",
      "Epoch[15/15], Step [220/469], Reconst Loss: 10182.6270, KL Div: 3292.9053\n",
      "Epoch[15/15], Step [230/469], Reconst Loss: 10608.0762, KL Div: 3226.8774\n",
      "Epoch[15/15], Step [240/469], Reconst Loss: 10286.3779, KL Div: 3235.0200\n",
      "Epoch[15/15], Step [250/469], Reconst Loss: 10502.6904, KL Div: 3323.0903\n",
      "Epoch[15/15], Step [260/469], Reconst Loss: 9719.3994, KL Div: 3127.9846\n",
      "Epoch[15/15], Step [270/469], Reconst Loss: 10059.9990, KL Div: 3207.9229\n",
      "Epoch[15/15], Step [280/469], Reconst Loss: 10374.5225, KL Div: 3382.9368\n",
      "Epoch[15/15], Step [290/469], Reconst Loss: 10311.4414, KL Div: 3198.8999\n",
      "Epoch[15/15], Step [300/469], Reconst Loss: 10201.6230, KL Div: 3292.0234\n",
      "Epoch[15/15], Step [310/469], Reconst Loss: 10040.1299, KL Div: 3302.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/15], Step [320/469], Reconst Loss: 10417.8223, KL Div: 3177.1838\n",
      "Epoch[15/15], Step [330/469], Reconst Loss: 9803.8467, KL Div: 3215.8281\n",
      "Epoch[15/15], Step [340/469], Reconst Loss: 9983.5078, KL Div: 3223.2058\n",
      "Epoch[15/15], Step [350/469], Reconst Loss: 10500.2100, KL Div: 3342.8684\n",
      "Epoch[15/15], Step [360/469], Reconst Loss: 9925.4199, KL Div: 3187.9319\n",
      "Epoch[15/15], Step [370/469], Reconst Loss: 9745.0762, KL Div: 3246.3953\n",
      "Epoch[15/15], Step [380/469], Reconst Loss: 10011.7715, KL Div: 3136.9636\n",
      "Epoch[15/15], Step [390/469], Reconst Loss: 10092.8359, KL Div: 3155.0017\n",
      "Epoch[15/15], Step [400/469], Reconst Loss: 10070.7119, KL Div: 3341.8015\n",
      "Epoch[15/15], Step [410/469], Reconst Loss: 10214.5088, KL Div: 3241.5994\n",
      "Epoch[15/15], Step [420/469], Reconst Loss: 10122.7783, KL Div: 3281.0703\n",
      "Epoch[15/15], Step [430/469], Reconst Loss: 9878.8193, KL Div: 3196.2974\n",
      "Epoch[15/15], Step [440/469], Reconst Loss: 10156.1475, KL Div: 3249.3672\n",
      "Epoch[15/15], Step [450/469], Reconst Loss: 10021.4502, KL Div: 3340.6423\n",
      "Epoch[15/15], Step [460/469], Reconst Loss: 9906.8047, KL Div: 3069.2944\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='../../data',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Save the sampled images\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "        # Save the reconstructed images\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
