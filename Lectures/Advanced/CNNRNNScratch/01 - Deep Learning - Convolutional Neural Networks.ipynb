{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.4 Deep Learning - Convolutional Neural Networks\n",
    "\n",
    "### Readings\n",
    "\n",
    "- [WEIDMAN] Ch5\n",
    "- [CHARU] Ch8\n",
    "- A great guide to calculating padding, strides, etc. https://arxiv.org/pdf/1603.07285v1.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from last time work so we can extend further\n",
    "from neuralnet.second_version import *\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on **Dense** layers (or also known as fully-connected layer) which are nice in understanding relationships.  Adding **activation function** like Sigmoid or Tanh allows us to understand the non-linear relationship between features and output, with Tanh having a steeper gradient, allowing the network to learn faster.  Adding **SoftMaxCrossEntropy** also enhance the gradient produced but remember that it only works with classification problems.  Adding **Dropout** helps in overfitting; **glorot initialization** to make sure the weight is normally distributed, **learning decay** to make sure we eventually reach the minimum instead of hopping all over the places, and last, the **momentum** to make sure we do not stuck in local minimum.  Such architecture is usally quite okay for **normal classification** problem.   \n",
    "\n",
    "However, when we talk about specific classification problem such as image or text or signal, they all have specific nature that would benefit from different architectures.   \n",
    "\n",
    "Today, we gonna work on image (this field is called computer vision) and discuss why Dense layer may not be the best, and propose CNN (Convolutional Neural Network) as a better way for dealing with image classification.\n",
    "\n",
    "There are mainly three layers that can help dealing with images:\n",
    "\n",
    "1. Convolutional layer\n",
    "2. Max/Average pooling layer\n",
    "3. Flatten layer\n",
    "\n",
    "### 1. Convolutional Layer\n",
    "Let's say given a image of 14 x 14 pixels = 196 features like this.  Each data point is an array of numbers describing how dark each pixel is, where value range from 0 to 255.  These values can be normalized ranging from 0 to 1. For example, for the following digit (the digit 1), we could have:\n",
    "\n",
    "<img src =\"figures/one.png\" width=\"400\">\n",
    "\n",
    "It is first important to define the input shape of an image, which will be <code>(input channels, image height, image width)</code>.  If we have lots of images, the input shall be <code>(batch size, input channels, image height, image width)</code>.  For our case, if it is a grayscale image, the shape is <code>(1, 14, 14)</code>.  If it is a RGB image, it shall be <code>(3, 14, 14)</code>.  If it is a CMYK, it shall be <code>(4, 14, 14)</code>.  If I define batch size as 500 (out of many more images I have), my input is <code>(500, 4, 14, 14)</code>.  (Commonly, batch size is around few hundreds).\n",
    "\n",
    "We might input these features into Dense layers and try to ask the Dense layers to understand the relationships.  How do we input it? Well, we can actually try converting <code>(500, 4, 14, 14)</code> to <code>(500, 784)</code> and then simply feed to Dense Layer.  What's wrong?\n",
    "\n",
    "Obviously, this is not so optimal since we do not **actually understand the nature of image**.  The key is that each single pixel actually holds very little information, right?  However, pattern of image can be better recognized by patches of pixels, rather than single pixel.  Imagine I give you a picture of cat, and I give you only a one-fourth of the picture, can you recognize that it's a cat?  Probably yes.  But what if I give you a single pixel.....you will have zero idea. \n",
    "\n",
    "**Why pattern of images are better recognized by patches?**...because humans recognize some visual patterns like corners, edges, sharpness.  Combining all these visual patterns form the image.  This is how humans visualize, and in fact, we should also apply these principles to neural networks\n",
    "\n",
    "**So how do we generate each patch of feature?**...actually, it is very easy.  We simply perform a convolution operation like this:\n",
    "\n",
    "<img src =\"figures/no_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "Mathematically, it looks like this:\n",
    "\n",
    "Let's say we have a 5 x 5 input image $I$ of channel 0 of batch 0:\n",
    "\n",
    "$$ I = \\begin{bmatrix}\n",
    "i_{11} & i_{12} & i_{13} & i_{14} & i_{15}\n",
    "\\\\\n",
    "i_{21} & i_{22} & i_{23} & i_{24} & i_{25}\n",
    "\\\\\n",
    "i_{31} & i_{32} & i_{33} & i_{34} & i_{35}\n",
    "\\\\\n",
    "i_{41} & i_{42} & i_{43} & i_{44} & i_{45}\n",
    "\\\\\n",
    "i_{51} & i_{52} & i_{53} & i_{54} & i_{55}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each of this pixel may represent the brightness ranging from 0 to 255.  Or if normalized, shall be 0 to 1.\n",
    "\n",
    "If we define a 3 x 3 patch which we commonly called **weights (W)** or in computer vision, we called **filters/kernels** like this (*we shall called filters in this lecture note for simplicity*) :\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13}\n",
    "\\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\\\\n",
    "w_{31} & w_{32} & w_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's say we are scanning the middle of the image, then the output feature would be (we'll denote this as $o_{33}$):\n",
    "\n",
    "$$o_{33} = w_{11} * i_{22} + w_{12} * i_{23} + w_{13} * i_{24} + \\\n",
    "           w_{21} * i_{32} + w_{22} * i_{32} + w_{23} * i_{34} + \\\n",
    "           w_{32} * i_{43} + w_{33} * i_{44}$$\n",
    "           \n",
    "This will result in one output feature called **feature map**.  Of course, we may add bias to it and then will be fed through an activation function.\n",
    "\n",
    "When we do this operation across the whole image, that is, *sliding $W$ over the input image*, taking the dot product of $W$ with the pixels at each location of the image, and ending up with a new image $O$ of almost identical size to the original image (Note that it may be slightly different, depending on how we convolve the edge of the image), this is called **convolution** which will result in the output features called **feature maps** or **output depth** or **output channels of the layer** (*we shall called output channels in this lecture for simplicity*).  What the convolution does is basically detecting certain patterns defined by $W$ at certain location of the input image.\n",
    "\n",
    "Actual feature maps look like this.  Each feature map is a output of a single training example and convolve each kernel over the sample.    In simple words, if we have $k$ filters, then we have $k$ feature maps.  They represent the activation part corresponding to the kernels.\n",
    "\n",
    "<img src =\"figures/feature-map2.png\" width=\"450\">\n",
    "\n",
    "In a CNN, there are 3 main hyperparameters to fine tune - (1) filter size, (2) padding, and (3) stride.  To answer the principle in tuning them, it is more beneficial to first answer these questions (which will then naturally answer the former questions):\n",
    "\n",
    "1. How the filters look like? What is the shape of filters?  What is the width and height of filters? Why filter is typically odd-square size?\n",
    "2. How should we convolve the edges?\n",
    "3. How many step we should take slide our filter? Skip 2?\n",
    "4. What would be the shape of the output matrix? Also in summary, what is the shape of the input, output, and filter?\n",
    "\n",
    "#### A. Filters\n",
    "\n",
    "1. **How the filters look like?**.  It turns out that each filter actually detect the presence of certain visual pattern.  For example, this filter below detects whether there is an edge at that location of the image.  There are also other similar filters detecting corners, lines, etc.  Check out https://setosa.io/ev/image-kernels/  and try changing the values\n",
    "\n",
    "$$ w = \\begin{bmatrix}\n",
    "0 & 1 & 0\n",
    "\\\\\n",
    "1 & -4 & 1\n",
    "\\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Real filters can look like this.  They may look somewhat random at first glance, but we can see that clear structure being learned in most kernels. For example, filters 3 and 4 seem to be learning diagonal edges in opposite directions, and other capture round edges or enclosed spaces:\n",
    "\n",
    "<img src =\"figures/kernels.png\" width=\"450\">\n",
    "\n",
    "However, **it is important to note that we DON'T need to decide the filters** to use.  We can simply feed a random generated filter, and it is the job of CNN to learn these filters.   These learned filters will learn what features are most efficient for the classification process.\n",
    "\n",
    "**What is the shape of filters?**.  For each image, we can apply multiple filters, depending on how many output channels we want.  Let's say the input channel is 3, and we want the output channel to 64, then we apply a filter of size <code>(3, 64, filter width, filter height)</code>, just like in Dense layer, where we input <code>(input_neurons, output_neurons)</code>.  What on earth is *output channel*, they are simply the number of patterns detected by your filters.  How do we know how many output channel to use? The answer is we don't know...we just try and see what works.  **The rule of thumb** is to first try a few filters, and gradually increase.\n",
    "\n",
    "We shall talk more about the size of output-width and output-height after convolution.  However, it is important to note that it is NOT necessary that output-width = image-width, and same for height.  That will be determined by how we convolve, which we shall discuss now.\n",
    "\n",
    "**What is the width and height of filters?**  If we use a filter width = filter height = 3, then our filter shape will be <code>(input channel, output channel, 3, 3)</code>.  The question is how to decide the filter height and width.  The idea is that if we use a 3 x 3 filter, each pixel got 8 neighboring information.  On the other hand, if we we use big filter like 9 x 9, then we got 80 neighboring information.  To choose this depends on your image classification task, if you think separating cat and dog images requires fine details like the nose, use a smaller filter.  But if you think it requires high-level details like the shape of the body, then maybe bigger filter size.  **How to choose is NOT science, but is an art, so simply try out.  Another way is to read papers and copy them.  Third is following the typical filter size which is 3x3, 5x5, and 7x7**.  \n",
    "\n",
    "**Why filter is typically odd-square size?** The sole reason of being an odd-square size filter because it is easy to do paddings due to its symmetry.  For example, when a 3x3 filter is used, there would be a padding of 1 in all sides (if you want the output to be of the same size as input). In case of 2x2 filters, there are four possible padding scenarios:\n",
    "\n",
    "- Left = 1, Right = 0, Top = 0, Bottom = 1\n",
    "- Left = 0, Right = 1, Top = 0, Bottom = 1\n",
    "- Left = 1, Right = 0, Top = 1, Bottom = 0\n",
    "- Left = 0, Right = 1, Top = 1, Bottom = 0\n",
    "\n",
    "This is unnecessary computation without any benefit.\n",
    "\n",
    "In addition, why the filter is square is not of importance.  It is also possible to use non-square size of filter (see Inception Network), but it is just more difficult to handle...that's it.  Afterall, there is no real evidence that square-filters do not work, thus most of networks prefer square-sized filters.\n",
    "\n",
    "#### B. Padding\n",
    "\n",
    "2. **How should we convolve the edges?**. Should we do the entire image?  Should we maintain the output features to be the same size as input features?  Recall this image:\n",
    "\n",
    "<img src =\"figures/no_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "It has 4 x 4 pixels = 16 features.  But after convolution, we only got 2 x 2 pixels = 4 features left.  Is that good?  There is no correct answers here but we are quite sure that we lose some information.  In fact, it is always nice to **maintain the output features to be the same size as input features**, but how?  There is no space to convolve since the filter is 2 x 2 and it can only shift right one time.\n",
    "\n",
    "The answer is **padding**, where we can enlarge the input image by padding the surroundings with zeros.  How much?  Padding until we get the original size or larger size, for example, like this.  The below put zero padding around which result the output features to be the same size as input features.\n",
    "\n",
    "<img src =\"figures/same_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "The below put even more padding which pad to make sure each single pixel is convoluted (full padding), which result the output features to be even large\n",
    "\n",
    "<img src =\"figures/full_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "Mathematically, it is easiest to understand padding from the 1D input like this:\n",
    "\n",
    "$$ input = [1, 2, 3, 4, 5] $$\n",
    "\n",
    "to\n",
    "\n",
    "$$ input_{padded} = [0, 1, 2, 3, 4, 5, 0] $$\n",
    "\n",
    "Normally, large size may benefit from more features, but also suffer from lengthy training time.  It is probably best to only perform enough padding to get the same size as input features.\n",
    "\n",
    "#### C. Strides\n",
    "\n",
    "3. **How many step we should take to slide our filter? Skip 2?** Should we shift 1 step per convolution, or 2 steps, or how many steps.  **In fact, it really depends on how detail you want it to be.  But defining bigger steps reduce the feature size and thus reduce the computation time.**  Bigger step is like human scanning picture more roughly but can reduce the computation time....whether to use it is something to be experimented though. \n",
    "\n",
    "In computer vision, we called this step as **stride**.  Example is like this:\n",
    "\n",
    "**No padding with stride of 2**\n",
    "\n",
    "<img src =\"figures/no_padding_strides.gif\" width=\"150\">\n",
    "\n",
    "**Padding with stride of 2**\n",
    "\n",
    "<img src =\"figures/padding_strides.gif\" width=\"150\">\n",
    "\n",
    "Actual image convolution can look like this (with stride 1 and no padding):\n",
    "\n",
    "<img src =\"figures/conv.gif\" width=\"500\">\n",
    "\n",
    "The convoluted image may look like this (nothing relate with the above matrix though):\n",
    "\n",
    "<img src =\"figures/convimages.png\" width=\"500\">\n",
    "\n",
    "**The formula to be used to measure the padding value to get the spatial size of the input and output volume to be the same with stride 1** is\n",
    "\n",
    "$$ \\frac{K-1}{2} $$\n",
    "\n",
    "where $K$ is the filter size.\n",
    "\n",
    "This means that if our image is size $24 * 24$, and the filter size is $3 x 3$, then our $K$ has size 3 so the padding should be $(3-1)/2 = 1$, then we need to add **a border of one pixel valued 0 around the outside of the image**, which would result in the input image of size $26 * 26$\n",
    "\n",
    "#### D. Shape\n",
    "\n",
    "4. **What would be the shape of the output matrix? Also in summary, what is the shape of the input, output, and filter?**.  Recall that in Dense layer, the shape of weight matrix is defined as \n",
    "\n",
    "<code>(neuron_{in}, neuron_{out})</code>\n",
    "\n",
    "For example, given a image of 24 x 24 pixels = 576 features.  Let's say we got around 1000 images, thus our input has a shape of (1000, 576).  Thus the input layer should have 576 neurons.  Let's say our next hidden layer has 10 neurons, what should be the shape of the weight matrix?  The answer is easy, we need to simply find the ? here:\n",
    "\n",
    "$$ (1000, 576) @ ? = (1000, 10) $$\n",
    "\n",
    "Obviously, the weight matrix would be\n",
    "\n",
    "$$ (576, 10) $$\n",
    "\n",
    "where you can clearly see 576 is the number of input neurons and 10 is the number of output neurons.\n",
    "\n",
    "Now our question is **how about convolutional layers**.  In convolutional layer, the shape of input has shape of <code>(batch size, input channels, image height, image width)</code>.  For example, let's say after we have 1000 of batch size, 3 input channels, and image height and width to be 24, thus the shape of input is <code>(1000, 3, 24, 24)</code>.  Now let's say we would like output channel of 4?  What should be the shape of the weight matrix?  Also, what would be the shape of the output?  The answer is a little tough, but we know one thing is that the number of samples will remain the same, thus we get:\n",
    "\n",
    "$$ (1000, 3, 24, 24) \\circledast W = (1000, 4, O, O) $$\n",
    "\n",
    "O actually depend on W, i.e., on the stride (denote as $S$), padding (denote as $P$), filter size (denote as $F$) as well as the input width and height (denote as $I$). $O$ can be calculated with the formula as follows:\n",
    "\n",
    "$$O = \\frac{I-F+2P}{S} + 1$$\n",
    "\n",
    "Suppose we have an input image of size $3*24*24$, we apply filters of size $3*3$, with single stride and no zero padding.\n",
    "\n",
    "Here I=24, F=3, P=0 and S=1.\n",
    "\n",
    "The size of the output volume will be $([24-3+0]/1)+1 = 22$.  Thus\n",
    "\n",
    "$$ (1000, 3, 24, 24) \\circledast (3, 4, 3, 3)_{p=0, s=1} = (1000, 4, 22, 22) $$\n",
    "\n",
    "\n",
    "In conclusion, \n",
    "\n",
    "- The input will have a 4D shape of <code>(batch size, input channels, input height, input width)</code>\n",
    "\n",
    "- The output will have a 4D shape of <code>(batch size, output channels, output height, output width)</code>\n",
    "\n",
    "- The convolutional filters will have 4D shape of <code>(input channels, output channels, filter height, filter width)</code>\n",
    "\n",
    "**Note: The order does not matter and it depends on the python library you use but these four dimensions always exist in CNN.**\n",
    "\n",
    "**The general rule of selecting padding, stride and filter size are of course of trial-and-error.  But it's important to remember that they should result in output image size of integers not decimals\"\n",
    "\n",
    "#### Demo\n",
    "\n",
    "https://www.cs.ryerson.ca/~aharley/vis/conv/\n",
    "\n",
    "### 2. Max/Average Pooling Layer\n",
    "\n",
    "Talking about **reducing computation time**, a common way is to perform a **pooling layer** which simply downsample the image by average a set of pixels, or by taking the maximum value.  If we define a pooling size of 2, this involves mapping each 2 x 2 pixels to one output, like this:\n",
    "\n",
    "<img src =\"figures/pooling.png\" width=\"300\">\n",
    "\n",
    "Nevertheless, pooling has a really big downsides, i.e., it basically lose a lot of information.  Compared to strides, strides simply scan less but maintain the same resolution but pooling simply reduce the resolution of the images....As Geoffrey Hinton said on Reddit AMA in 2014 - **The pooling operation used in CNN is a big mistake and the fact that it works so well is a disaster**.  In fact, in most recent CNN architectures like ResNets, it uses pooling very minimially or not at all.  In this lecture, we are not going to implement pooling, but we just talk about it for the sake of completeness since very early architectures like AlexNet uses pooling.\n",
    "\n",
    "### 3. Flatten Layer\n",
    "\n",
    "It must be said that in CNN, probably there are many convolutional layers.  However, in the last layer, typically, if we want to predict a certain class, it make sense to use Dense layer as the output layer.  However, the question is how do we send input of shape $(\\text{size}, \\text{image height}, \\text{image width}, \\text{input channels})$ into Dense layer?\n",
    "\n",
    "This is actually quite easy.  What we can do is simply squash all these 4D vectors into 2D vectors.  For example, given (1000, 2, 22, 22), through a *flatten* operation, the vector becomes (1000, 968), which we can then multiply with weight just like in Dense layer, make predictions, and calculate loss just like we did in previous class.\n",
    "\n",
    "Why we can perform *flatten* operation?  Does it not lost any information?  This is because through flattening, it is just another representations, thus flattening does not result in any loss of information.  It also allow the Dense layer to understand the relationships of visual patterns from prior convolutional layers to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start coding!!\n",
    "\n",
    "### 1D input\n",
    "\n",
    "First off, to make us easily understand CNN coding, let's start simple, working with 1D input.  Also let's write some helpers to make our life easier, namely <code>assert_same_shape</code>, and <code>assert_dim</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(A: ndarray, B: ndarray):\n",
    "    assert A.shape == B.shape\n",
    "    \n",
    "def assert_dim(X: ndarray, dim: ndarray):\n",
    "    assert len(X.shape) == dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "Padding can be easily coded.  Let's start simple with 1D input like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d = np.array([1,2,3,4,5])\n",
    "param_1d = np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_1d(input_: ndarray,\n",
    "            padding: int) -> ndarray:\n",
    "    zero = np.array([0])\n",
    "    zero = np.repeat(zero, padding)  #number of zeros * padding\n",
    "    return np.concatenate([zero, input_, zero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pad_1d(input_1d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass - convolution\n",
    "\n",
    "Convolution in 1D is simple.\n",
    "\n",
    "We are actually doing something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[0, 1, 2, 3, 4, 5, 0]\n",
    "[1, 1, 1]  = 0*1 + 1*1 + 2*1 = 3\n",
    "   [1, 1, 1]  = 1*1 + 2*1 + 3*1 = 6\n",
    "       [1, 1, 1] = 9\n",
    "           [1, 1, 1] = 12\n",
    "              [1, 1, 1] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d(input_: ndarray, \n",
    "            param: ndarray) -> ndarray:\n",
    "    \n",
    "    # assert 1D data\n",
    "    assert_dim(input_, 1)\n",
    "    assert_dim(param, 1)\n",
    "    \n",
    "    # 1. pad the input\n",
    "    # (k - 1) / 2 can be implemented as k // 2 where // is floor division\n",
    "    param_len = param.shape[0]  #3\n",
    "    param_mid = param_len // 2  #3 // 2 = 1\n",
    "    input_pad = _pad_1d(input_, param_mid) # [0, 1, 2, 3, 4, 5, 0]\n",
    "    \n",
    "    # initialize the output\n",
    "    # we let output has the same shape of input\n",
    "    output = np.zeros(input_.shape) # [0, 0, 0, 0, 0]\n",
    "\n",
    "    # perform the 1d convolution\n",
    "    # 2. Use the padded input and params to compute output\n",
    "    for o in range(output.shape[0]): #0 to 4\n",
    "        for w in range(param_len):  #0 to 2\n",
    "            output[o] += param[w] * input_pad[o+w] #o move along with w thus o+w\n",
    "        \n",
    "    # ensure input has same shape as output\n",
    "    # this is actually optional\n",
    "    assert_same_shape(input_, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we code the sum, which is basically sum everything return by the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d_sum(input_: ndarray, \n",
    "                param: ndarray) -> ndarray:\n",
    "    output = conv_1d(input_, param)\n",
    "    return np.sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_sum(input_1d, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "How to compute the gradients of convolution?\n",
    "\n",
    "Let's first try some set of numbers and manually get the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly choose to increase 5th element by 1 \n",
    "#so we can know the gradient of 5th element in respect to the convolution sum\n",
    "input_1d_2 = np.array([1,2,3,4,6])\n",
    "param_1d = np.array([1,1,1])\n",
    "\n",
    "conv_1d_sum(input_1d_2, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean?  Since we change the 5th element by 1, which increase the convolution sum by 2, thus the gradient of the 5th element is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we actually get the 2.\n",
    "\n",
    "Given \n",
    "\n",
    "$$ t = [0, 1, 2, 3, 4, 5, 0]  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ w = [1, 1, 1] $$\n",
    "\n",
    "and let $o$ be the output of convolution: $o_0....o_4$\n",
    "\n",
    "First, let's look at the convolution equation like this:\n",
    "\n",
    "$$ o_0 = t_0*w_0 + t_1*w_1 + t_2*w_2 $$\n",
    "$$ o_1 = t_1*w_0 + t_2*w_1 + t_3*w_2 $$\n",
    "$$ o_2 = t_2*w_0 + t_3*w_1 + t_4*w_2 $$\n",
    "$$ o_3 = t_3*w_0 + t_4*w_1 + t_5*w_2 $$\n",
    "$$ o_4 = t_4*w_0 + t_5*w_1 + t_6*w_2 $$\n",
    "\n",
    "Look at $t_5$ which is our 5th element, where $t_5$ changes $O_3$ based on $w_2$, $O_4$ based on $w_1$, and $O_5$ based on $w_0$ (which we don't have; the reason we wrote it so to detect the underlying pattern)\n",
    "\n",
    "This gradient can be written as:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_3} * \\frac{\\partial o_3}{\\partial t_5} +  \\frac{\\partial L}{\\partial o_4} * \\frac{\\partial o_4}{\\partial t_5} + \\frac{\\partial L}{\\partial o_5} * \\frac{\\partial o_5}{\\partial t_5}$$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_3} * w_2 +  \\frac{\\partial L}{\\partial o_4} * w_1 + \\frac{\\partial L}{\\partial o_5} * w_0$$\n",
    "\n",
    "Of course, in this simple example, when the loss is just the sum, and since $o_i$ is contributing to the sum of the convolution sum, i.e., \n",
    "\n",
    "$$L = o_0 + o_1 + o_2 + o_3 + o_4$$\n",
    "\n",
    "its derivative is simply $$\\frac{\\partial L}{\\partial o_i} = 1$$\n",
    "\n",
    "Thus, $$ \\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_3} * w_2 +  \\frac{\\partial L}{\\partial o_4} * w_1 + \\frac{\\partial L}{\\partial o_5} * w_0 = 1 * w_2 + 1 * w_1 + 0 * w_0 = 2$$\n",
    "\n",
    "since $o_5$ does not exist\n",
    "\n",
    "Since we need to code this, we need to see whether there is any general pattern.  Let's look at other elements as well:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_3} * w_2 +  \\frac{\\partial L}{\\partial o_4} * w_1 + \\frac{\\partial L}{\\partial o_5} * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_4} = \\frac{\\partial L}{\\partial o_2} * w_2 +  \\frac{\\partial L}{\\partial o_3} * w_1 +  \\frac{\\partial L}{\\partial o_4} * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_3} = \\frac{\\partial L}{\\partial o_1} * w_2 +  \\frac{\\partial L}{\\partial o_2} * w_1 +  \\frac{\\partial L}{\\partial o_3} * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_2} = \\frac{\\partial L}{\\partial o_0} * w_2 +  \\frac{\\partial L}{\\partial o_1} * w_1 +  \\frac{\\partial L}{\\partial o_2} * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_1} =  \\frac{\\partial L}{\\partial o_{-1}} * w_2 + \\frac{\\partial L}{\\partial o_0} * w_1 +  \\frac{\\partial L}{\\partial o_1} * w_0 $$\n",
    "\n",
    "**How should we code this?** In terms of code, it is easy to represent \n",
    "\n",
    "$$w_0, w_1, w_2$$ \n",
    "\n",
    "simply by iterating.\n",
    "\n",
    "But we need to find a way to represent the output gradients of:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o_{-1}} \\text{  to  } \\frac{\\partial L}{\\partial o_5}$$ \n",
    "\n",
    "In fact, we know \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o_{-1}} = 0 $$ \n",
    "\n",
    "as well as \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o_{5}} = 0 $$\n",
    "\n",
    "while other gradients are simply one.  Thus, we can represent as a list of\n",
    "\n",
    "$$grad = [0, 1, 1, 1, 1, 1, 0] = [\\frac{\\partial L}{\\partial o_{-1}}, \\frac{\\partial L}{\\partial o_{0}}, \\frac{\\partial L}{\\partial o_{2}}, \\frac{\\partial L}{\\partial o_{3}}, \\frac{\\partial L}{\\partial o_{4}}, \\frac{\\partial L}{\\partial o_{5}}]$$\n",
    "\n",
    "Let's called this <code>grad</code>.  It can be a bit confusing now that we are coding, since the indices start from 0.  Let's rewrite the equation using these indices:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial t_5} = inputgrad_4 = grad_4 * w_2 +  grad_5 * w_1 + grad_6 * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_4} = inputgrad_3 = grad_3 * w_2 +  grad_4 * w_1 +  grad_5 * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_3} = inputgrad_2 = grad_2 * w_2 +  grad_3 * w_1 +  grad_4 * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_2} = inputgrad_1 = grad_1 * w_2 +  grad_2 * w_1 +  grad_3 * w_0$$\n",
    "$$ \\frac{\\partial L}{\\partial t_1} = inputgrad_0 =  grad_0 * w_2 + grad_1 * w_1 +  grad_2 * w_0 $$\n",
    "\n",
    "Now we have to map the indices in coding.  This is simple.\n",
    "\n",
    "For each inputgrad, we need to repeatingly run, $w_0$ to $w_2$, we simply iterate using something like \n",
    "\n",
    "<code>\n",
    "    for each inputgrad\n",
    "        for each p in param  #we call our w as param, just like pyTorch\n",
    "</code>\n",
    "\n",
    "Then for the first input $inputgrad_0$, we need to make sure to run the <code>grad</code> indicies to be 2, 1, 0 in this order. <code>grad</code> indices depend on two things: a) As $w$ up by 1, <code>grad</code> index lower by 1; this can be easily coded simply by subtracting $w$, thus when $w$ increases, the <code>grad</code> decreases; b) <code>grad</code> index starts at index of <code>inputgrad + 2</code>, where 2 is actually length of <code>w - 1</code>.\n",
    "\n",
    "Thus, this can be summarized as the following python code:\n",
    "\n",
    "<code>\n",
    "    for i in range(input_grad.shape[0])  #this is ok since inputgrad shape == input.shape\n",
    "        for p in range(param.shape[0])  # this will represent index of w\n",
    "            inputgrad_i += grad[i +     (len(p) - 1)    - p] * param[p]\n",
    "</code>\n",
    "\n",
    "This can be written as function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_grad_1d(input_: ndarray, \n",
    "                   param: ndarray, \n",
    "                   grad: ndarray = None) -> ndarray:\n",
    "    \n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    \n",
    "    if grad is None:\n",
    "        grad = np.ones_like(input_) #choose one so grad can be multiplied and not become zero\n",
    "    else:\n",
    "        assert_same_shape(input_, grad)\n",
    "    \n",
    "    #1. pad the output gradients\n",
    "    grad = _pad_1d(grad, param_mid)  #[0, 1, 1, 1, 1, 1, 0]\n",
    "    \n",
    "    #prepare input_grad which has grad of the five elements\n",
    "    #thus the initial look can be [0, 0, 0, 0, 0]\n",
    "    input_grad = np.zeros_like(input_)\n",
    "    \n",
    "    #2. Use the padded output gradients, along with param, to compute the input gradient\n",
    "    for i in range(input_grad.shape[0]):  #for each input grad which follows the same shape as input\n",
    "        for p in range(param.shape[0]):  #for each param\n",
    "            input_grad[i] += grad[i +  param_len - 1 - p] * param[p] \n",
    "        \n",
    "    assert_same_shape(input_grad, input_)\n",
    "    \n",
    "    return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_grad_1d(input_1d, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, it means if I change the first input by 1, it shall increase the output by 2, if I change the second input by 1, it shall increase the output by 3, etc.\n",
    "\n",
    "Now, we have learned how to find gradients of the input (i.e., <code>input_grad</code>).  How about the gradients of the filters (i.e., <code>param_grad</code>?)\n",
    "\n",
    "Let's try change element 1 of the param by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "input_1d = np.array([1,2,3,4,5])\n",
    "param_1d_2 = np.array([2,1,1]) #increase first element by 1\n",
    "\n",
    "print(conv_1d_sum(input_1d, param_1d_2) - conv_1d_sum(input_1d, param_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we find that $$\\frac{\\partial L}{\\partial w_0} = 10 $$\n",
    "\n",
    "Recall this:\n",
    "\n",
    "$$ t = [0, 1, 2, 3, 4, 5, 0]  $$\n",
    "\n",
    "$$ w = [1, 1, 1] $$\n",
    "\n",
    "$$ o_0 = t_0*w_0 + t_1*w_1 + t_2*w_2 $$\n",
    "$$ o_1 = t_1*w_0 + t_2*w_1 + t_3*w_2 $$\n",
    "$$ o_2 = t_2*w_0 + t_3*w_1 + t_4*w_2 $$\n",
    "$$ o_3 = t_3*w_0 + t_4*w_1 + t_5*w_2 $$\n",
    "$$ o_4 = t_4*w_0 + t_5*w_1 + t_6*w_2 $$\n",
    "\n",
    "We can clearly see that $w_o$ is changing the convolution sum in respect of $t_0$ to $t_4$.  Using the same logic as above which we can get\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial o_0} * \\frac{\\partial o_0}{\\partial w_0} +  \\frac{\\partial L}{\\partial o_1} * \\frac{\\partial o_1}{\\partial w_0} + \\frac{\\partial L}{\\partial o_2} * \\frac{\\partial o_2}{\\partial w_0} + \\frac{\\partial L}{\\partial o_3} * \\frac{\\partial o_3}{\\partial w_0} + \\frac{\\partial L}{\\partial o_4} * \\frac{\\partial o_4}{\\partial w_0}$$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial o_0} * t_0 +  \\frac{\\partial L}{\\partial o_1} * t_1 + \\frac{\\partial L}{\\partial o_2} * t_2 + \\frac{\\partial L}{\\partial o_3} * t_3 + \\frac{\\partial L}{\\partial o_4} * t_4$$\n",
    "\n",
    "since $t_0$ is a 0, and $\\frac{\\partial L}{\\partial o_i} = 1$, thus the gradient of the first element is indeed 10:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_0} = w_0^{grad} = t_1 + t_2 + t_3 + t_4 = 1 + 2 + 3 + 4 = 10 $$\n",
    "\n",
    "The general pattern is:\n",
    "\n",
    "$$ w_0^{grad} = t_0 + t_1 + t_2 + t_3 + t_4 $$\n",
    "$$ w_1^{grad} = t_1 + t_2 + t_3 + t_4 + t_5  $$\n",
    "$$ w_2^{grad} = t_2 + t_3 + t_4 + t_5 + t_6 $$\n",
    "\n",
    "**How to code this?**\n",
    "\n",
    "Luckily, you can clearly see that the indices are moving the same direction, thus it is easy to code like this.  We simply define <code>grad</code> to be \n",
    "\n",
    "$$grad = [1, 1, 1, 1, 1] = [\\frac{\\partial L}{\\partial o_{0}}, \\frac{\\partial L}{\\partial o_{1}}, \\frac{\\partial L}{\\partial o_{2}}, \\frac{\\partial L}{\\partial o_{3}}, \\frac{\\partial L}{\\partial o_{4}}]$$\n",
    "\n",
    "and <code>input_pad</code> as follows:\n",
    "\n",
    "$$input_{pad} = [0, 1, 2, 3, 4, 5, 0]$$\n",
    "\n",
    "simply let it multiply with the <code>input_pad</code> of <code>[0, 1, 2, 3, 4, 5, 0]</code> like this:\n",
    "\n",
    "<code>\n",
    "[0, 1, 2, 3, 4, 5, 0]\n",
    "[1, 1, 1, 1, 1]  = gradient of w0\n",
    "   [1, 1, 1, 1, 1]  = gradient of w1\n",
    "       [1, 1, 1, 1, 1]  = gradient of w2\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_grad_1d(input_: ndarray, \n",
    "                   param: ndarray, \n",
    "                   grad: ndarray = None) -> ndarray:\n",
    "    \n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    input_pad = _pad_1d(input_, param_mid)  #[0, 1, 2, 3, 4, 5, 0]\n",
    "    \n",
    "    #1. pepare the output gradients\n",
    "    if grad is None:\n",
    "        grad = np.ones_like(input_) #[1, 1, 1, 1, 1]\n",
    "    else:\n",
    "        assert_same_shape(input_, grad)\n",
    "\n",
    "    #prepare param_grad which has grad of the three w\n",
    "    #thus the initial look can be [0, 0, 0]\n",
    "    param_grad = np.zeros_like(param) #[0, 0, 0]\n",
    "\n",
    "    #2. Use the padded output gradients, along with padded input, to compute the param gradient\n",
    "    for i in range(input_.shape[0]):\n",
    "        for p in range(param.shape[0]):\n",
    "            #as w increase, shift input_pad right by w amount\n",
    "            param_grad[p] += input_pad[i+p] * grad[i]\n",
    "        \n",
    "    assert_same_shape(param_grad, param)\n",
    "    \n",
    "    return param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_param_grad_1d(input_1d, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D input with batch (sample > 1)\n",
    "\n",
    "How about if we have more samples of the 1D input like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d_batch = np.array([[0,1,2,3,4,5,6], \n",
    "                           [1,2,3,4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**\n",
    "\n",
    "In fact, this is simple, we simply run <code>cov_1d</code> on the first sample, and iterate and stack the results on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_1d_batch(input_: ndarray, \n",
    "                  padding: int) -> ndarray:\n",
    "    outs = [_pad_1d(sample, padding) for sample in input_]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3, 4, 5, 6, 0],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pad_1d_batch(input_1d_batch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass**\n",
    "\n",
    "Same concept.  For forward pass, we simply iterate our previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d_batch(input_: ndarray, \n",
    "                  param: ndarray) -> ndarray:\n",
    "\n",
    "    outs = [conv_1d(sample, param) for sample in input_]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  3.,  6.,  9., 12., 15., 11.],\n",
       "       [ 3.,  6.,  9., 12., 15., 18., 13.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_batch(input_1d_batch, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass**\n",
    "\n",
    "For <code>input_grad</code>, it's the same concept.  We simply do a for loop on the previous function we have already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_grad_1d_batch(input_: ndarray, \n",
    "                        param: ndarray) -> ndarray:\n",
    "    \n",
    "    #first perform a forward pass\n",
    "    out = conv_1d_batch(input_, param)\n",
    "    \n",
    "    #generate grad for input to the _input_grad_1d function\n",
    "    grad = np.ones_like(out)\n",
    "    \n",
    "    batch_size = grad.shape[0]\n",
    "        \n",
    "    grads = [_input_grad_1d(input_[i], param, grad[i]) for i in range(batch_size)]    \n",
    "\n",
    "    return np.stack(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 3, 3, 3, 3, 2],\n",
       "       [2, 3, 3, 3, 3, 3, 2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_grad_1d_batch(input_1d_batch, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for <code>param_grad</code>, since <code>param_grad</code> is a filter that dependent across samples, thus we need to change our previous code.  So, to compute the parameter gradient, we have to loop through all of the observations and increment the appropriate values of the parameter gradient as we do so. Still, this just involves adding an outer for loop to the code to compute the parameter gradient that we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grad_1d_batch(input_: ndarray, \n",
    "                        param: ndarray) -> ndarray:\n",
    "\n",
    "    grad = np.ones_like(input_)\n",
    "    \n",
    "    input_pad = _pad_1d_batch(input_, 1)\n",
    "\n",
    "    param_grad = np.zeros_like(param)    \n",
    "    \n",
    "    for s in range(input_.shape[0]):\n",
    "        for i in range(input_.shape[1]):\n",
    "            for w in range(param.shape[0]):\n",
    "                param_grad[w] += input_pad[s][i+w] * grad[s][i]    \n",
    "\n",
    "    return param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 49, 48])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grad_1d_batch(input_1d_batch, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D convolutions\n",
    "\n",
    "The 2D convolution is a straightforward extension of the 1D case because, fundamentally, the way the input is connected to the output via the filters in each dimension of the 2D case is identical to the 1D case.  Consider this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_2d_batch = np.random.randn(3, 5, 5)\n",
    "param_2d = np.random.randn(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "Recall that for 1D convolutions the code for computing the output given the input and the parameters on the forard pass looked as follows:\n",
    "\n",
    "<code>def conv_1d(input_: ndarray, param: ndarray) -> ndarray:\n",
    "    param_len = param.shape[0]  \n",
    "    param_mid = param_len // 2  \n",
    "    input_pad = \\_pad_1d(input_, param_mid)  \n",
    "    output = np.zeros(input_.shape) \n",
    "    for o in range(output.shape[0]):\n",
    "        for w in range(param_len):\n",
    "            output[o] += param[w] * input_pad[o+w]\n",
    "    return output\n",
    "</code>\n",
    "\n",
    "For 2D convolutions, instead of 1D output, we simply make it loop twice for height and width.  In addition, instead of 1D param (filter), we break it into two loops, one for filter width, and another for filter height.  The code will be simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each sample\n",
    "def _conv_sample_2d(sample: ndarray, \n",
    "                           param: ndarray):\n",
    "\n",
    "    param_mid = param.shape[0] // 2\n",
    "    \n",
    "    sample_pad = _pad_2d_sample(sample, param_mid)\n",
    "    \n",
    "    out = np.zeros_like(sample)\n",
    "    \n",
    "    for i_w in range(sample.shape[0]):  #loop through the image height\n",
    "        for i_h in range(sample.shape[1]):  #loop through the image width\n",
    "            for p_w in range(param.shape[0]): #loop through the filter width\n",
    "                for p_h in range(param.shape[1]): #loop through the filter height\n",
    "                    out[i_w][i_h] += param[p_w][p_h] * sample_pad[i_w+p_w][i_h+p_h]\n",
    "    return out\n",
    "\n",
    "#for many samples...simply a for loop\n",
    "def _conv_2d(img_batch: ndarray,\n",
    "                       param: ndarray):\n",
    "    \n",
    "    assert_dim(img_batch, 3)\n",
    "    \n",
    "    outs = [_conv_sample_2d(sample, param) for sample in img_batch]\n",
    "    \n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "Of course, this function would requires us to implement <code>_pad_2d_samples</code> which is actually very straightforward.  Recall that the code is like this:\n",
    "\n",
    "<code>def _pad_1d(input_: ndarray,\n",
    "            padding: int) -> ndarray:\n",
    "    zero = np.array([0])\n",
    "    zero = np.repeat(zero, padding)  #number of zeros * num\n",
    "    return np.concatenate([zero, input_, zero])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2d_sample(sample: ndarray, \n",
    "                padding: int):\n",
    "\n",
    "    input_pad = _pad_1d_batch(sample, padding)  #this will add left and right\n",
    "    \n",
    "    #these are zeros that will be added on top and bottom.  Padding*2 so account for both left and right\n",
    "    zero = np.zeros((padding, sample.shape[0] + padding * 2))\n",
    " \n",
    "    return np.concatenate([zero, input_pad, zero])\n",
    "\n",
    "def _pad_2d(img_batch: ndarray, \n",
    "            padding: int):\n",
    "\n",
    "    outs = [_pad_2d_sample(sample, padding) for sample in img_batch]\n",
    "    \n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check whether the padding works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (3, 5, 5)\n",
      "Padded shape (must plus 2):  (3, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original shape: \", imgs_2d_batch.shape)\n",
    "print(\"Padded shape (must plus 2): \", _pad_2d(imgs_2d_batch, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our forward pass for 2D data with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (3, 5, 5)\n",
      "Convoluted shape(must be same):  (3, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original shape: \", imgs_2d_batch.shape)\n",
    "print(\"Convoluted shape(must be same): \", _conv_2d(imgs_2d_batch, param_2d).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "\n",
    "*Input gradients*\n",
    "\n",
    "Recall that for input_grad, our code is like this:\n",
    "    \n",
    "<code>def \\_input_grad_1d(input_: ndarray, \n",
    "                   param: ndarray, \n",
    "                   grad: ndarray = None) -> ndarray:\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    if grad is None:\n",
    "        grad = np.ones_like(input_) \n",
    "    else:\n",
    "        assert_same_shape(input_, grad)\n",
    "    grad = \\_pad_1d(grad, param_mid)  #[0, 1, 1, 1, 1, 1, 0]\n",
    "    input_grad = np.zeros_like(input_)\n",
    "    for i in range(input_.shape[0]):\n",
    "        for w in range(param.shape[0]):\n",
    "            input_grad[i] += grad[i +  param_len - 1 - w] * param[w] \n",
    "    return input_grad\n",
    "</code>\n",
    "\n",
    "In the 2D case, we simply break our input to 2 loops since we have image width and height, and we also break the param into 2 loops since we have width and height of the filters. Yes, that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_grad_sample_2d(sample: ndarray,\n",
    "                          param: ndarray,\n",
    "                          grad: ndarray) -> ndarray:\n",
    "   \n",
    "    param_size = param.shape[0]\n",
    "    grad = _pad_2d_sample(grad, param_size // 2)  #sample refers to each sample\n",
    "    input_grad = np.zeros_like(sample)\n",
    "\n",
    "    for i_w in range(sample.shape[0]):  #img width\n",
    "        for i_h in range(sample.shape[1]):  #img height\n",
    "            for p_w in range(param_size):      #filter width\n",
    "                for p_h in range(param_size):    #filter height\n",
    "                    input_grad[i_w][i_h] += grad[i_w + param_size - 1 - p_w][i_h + param_size - 1 - p_h] \\\n",
    "                    * param[p_w][p_h]\n",
    "                    \n",
    "    return input_grad\n",
    "\n",
    "def _input_grad_2d(samples: ndarray,\n",
    "                   param: ndarray,\n",
    "                   grad: ndarray) -> ndarray:\n",
    "    grads = [_input_grad_sample_2d(samples[i], param, grad[i]) for i in range(grad.shape[0])]    \n",
    "    return np.stack(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_grads = _input_grad_2d(imgs_2d_batch, \n",
    "                           param_2d,\n",
    "                           np.ones_like(imgs_2d_batch))\n",
    "img_grads.shape  ##img grad should equal to img size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Param gradients*\n",
    "\n",
    "For the parameter gradient, we have to loop through all the images in the batch and add components from each one to the appropriate places in the parameter gradient.  The reason is because the filter gradients overlap over multiple samples.\n",
    "\n",
    "This is the code we have used earlier for 1D batch\n",
    "\n",
    "<code>def param_grad_1d_batch(input_: ndarray, \n",
    "                        param: ndarray) -> ndarray:\n",
    "    grad = np.ones_like(input_)\n",
    "    input_pad = \\_pad_1d_batch(input_, 1)\n",
    "    param_grad = np.zeros_like(param)    \n",
    "    for s in range(input_.shape[0]):\n",
    "        for i in range(input_.shape[1]):\n",
    "            for w in range(param.shape[0]):\n",
    "                param_grad[w] += input_pad[s][i+w] * grad[s][i]    \n",
    "    return param_grad \n",
    "</code>\n",
    "\n",
    "What we have to do is that we have to replace i with two loops representing img width and height, and replace w with two loops, representing filter width and height.  Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_grad_2d(input_: ndarray,\n",
    "                   grad: ndarray, \n",
    "                   param: ndarray) -> ndarray:\n",
    "\n",
    "    param_size = param.shape[0]  #for filter width, height\n",
    "    input_pad = _pad_2d(input_, param_size // 2)  #input_pad\n",
    "\n",
    "    param_grad = np.zeros_like(param)\n",
    "    img_shape = input_.shape[1:]  #get only 5, 5\n",
    "    \n",
    "    for s in range(input_.shape[0]):  #loop samples\n",
    "        for i_w in range(img_shape[0]):  #loop img width\n",
    "            for i_h in range(img_shape[1]):  #loop img height\n",
    "                for p_w in range(param_size):   #loop param width\n",
    "                    for p_h in range(param_size):   #loop param height\n",
    "                        param_grad[p_w][p_h] += input_pad[s][i_w+p_w][i_h+p_h] \\\n",
    "                        * grad[s][i_w][i_h]\n",
    "    return param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param shape:  (3, 3)\n",
      "Param_grad shape (must equal param2d):  (3, 3)\n"
     ]
    }
   ],
   "source": [
    "param_grad = _param_grad_2d(imgs_2d_batch, \n",
    "                            np.ones_like(imgs_2d_batch),\n",
    "                            param_2d)\n",
    "print(\"Param shape: \", param_2d.shape)\n",
    "print(\"Param_grad shape (must equal param2d): \", param_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing gradients\n",
    "\n",
    "Let's test whether our code really works.  Let's make a code to perform convolution sum for 2d batch, and then we can manually subtract to find the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d_sum(img_batch: ndarray,\n",
    "                param: ndarray):\n",
    "    \n",
    "    out = _conv_2d(img_batch, param)\n",
    "    \n",
    "    return out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual:  -0.5138257424616004\n",
      "Our code:  -0.5138257424615983\n"
     ]
    }
   ],
   "source": [
    "#Testing input gradient\n",
    "\n",
    "#let's randomly change one pixel of a sample by 1\n",
    "imgs_2d_batch_2 = imgs_2d_batch.copy()\n",
    "imgs_2d_batch_2[0][2][2] += 1\n",
    "\n",
    "manual = conv_2d_sum(imgs_2d_batch_2, param_2d) - conv_2d_sum(imgs_2d_batch, param_2d)\n",
    "print(\"Manual: \", manual)\n",
    "print(\"Our code: \", img_grads[0][2][2]) #this is already computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual:  -2.658933224682264\n",
      "Our code:  -2.6589332246822615\n"
     ]
    }
   ],
   "source": [
    "#Testing param gradient\n",
    "\n",
    "param_2d_2 = param_2d.copy()\n",
    "param_2d_2[0][2] += 1\n",
    "\n",
    "manual = conv_2d_sum(imgs_2d_batch, param_2d_2) - conv_2d_sum(imgs_2d_batch, param_2d)\n",
    "print(\"Manual: \", manual)\n",
    "print(\"Our code: \", param_grad[0][2]) #this is already computed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channels\n",
    "\n",
    "Ok....so we have 2D input with batch size.  But we still have one more dimension, and that is number of channels.  Currently, we assume only one filter so far, what if we have more filters?\n",
    "\n",
    "The answer, as it was when we added batches earlier, is simple: we add two outer for loops to the code we’ve already seen—one loop for the input channels and another for the output channels. By looping through all combinations of the input channel and the output channel, we make each output feature map a combination of all of the input feature maps, as desired.\n",
    "\n",
    "#### Forward pass\n",
    "\n",
    "Let's code the forward pass for each sample which have channels.  This is actually easy, we simply copy our previous code <code>_conv_sample_2d</code> and add two more outer loops, make sure we loop through.\n",
    "\n",
    "For your reference, this is the previous code we use:\n",
    "\n",
    "<code>def _conv_sample_2d(sample: ndarray, \n",
    "                           param: ndarray):\n",
    "    param_mid = param.shape[0] // 2\n",
    "    sample_pad = _pad_2d_sample(sample, param_mid)\n",
    "    out = np.zeros_like(sample)\n",
    "    for o_w in range(out.shape[0]):  #loop through the image height\n",
    "        for o_h in range(out.shape[1]):  #loop through the image width\n",
    "            for p_w in range(param.shape[0]): #loop through the filter width\n",
    "                for p_h in range(param.shape[1]): #loop through the filter height\n",
    "                    out[o_w][o_h] += param[p_w][p_h] * sample_pad[o_w+p_w][o_h+p_h]\n",
    "    return out\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_sample_channels_2d(sample: ndarray, \n",
    "                             param: ndarray):\n",
    "    '''\n",
    "    sample: [channels, img_width, img_height]\n",
    "    param: [in_channels, out_channels, fil_width, fil_height]    \n",
    "    '''\n",
    "    assert_dim(sample, 3)\n",
    "    assert_dim(param, 4)\n",
    "    \n",
    "    param_size = param.shape[2]\n",
    "    param_mid = param_size // 2\n",
    "    sample_pad = _pad_2d_sample_channel(sample, param_mid)  #pad the input\n",
    "    \n",
    "    #define for loops\n",
    "    in_channels = param.shape[0]\n",
    "    out_channels = param.shape[1]\n",
    "    img_size = sample.shape[1]\n",
    "    \n",
    "    out = np.zeros((out_channels,) + sample.shape[1:])\n",
    "    \n",
    "    for c_in in range(in_channels):\n",
    "        for c_out in range(out_channels):\n",
    "            for i_w in range(img_size):\n",
    "                for i_h in range(img_size):\n",
    "                    for p_w in range(param_size):\n",
    "                        for p_h in range(param_size):\n",
    "                            out[c_out][i_w][i_h] += \\\n",
    "                            param[c_in][c_out][p_w][p_h] * sample_pad[c_in][i_w+p_w][i_h+p_h]\n",
    "    return out  \n",
    "\n",
    "def _conv_channels_2d(input_: ndarray,\n",
    "                    param: ndarray) -> ndarray:\n",
    "    '''\n",
    "    input_: [batch_size, channels, img_width, img_height]\n",
    "    param: [in_channels, out_channels, fil_width, fil_height]    \n",
    "    '''\n",
    "    outs = [_conv_sample_channels_2d(sample, param) for sample in input_]    \n",
    "\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can test this code, let's make sure we implement the padding for channels as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2d_sample_channel(input_: ndarray, \n",
    "                    padding: int):\n",
    "    '''\n",
    "    input_ has dimension [num_channels, image_width, image_height] \n",
    "    '''\n",
    "    return np.stack([_pad_2d_sample(channel, padding) for channel in input_])\n",
    "\n",
    "def _pad_2d_channel(input_: ndarray,\n",
    "                    padding: int):   \n",
    "    '''\n",
    "    input_ has dimension [batch_size, num_channels, image_width, image_height]\n",
    "    '''    \n",
    "    return np.stack([_pad_2d_sample_channel(sample, padding) for sample in input_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code whether it works.  I will leave the gradient test to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 16, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "img = np.random.randn(10, 3, 32, 32)\n",
    "param = np.random.randn(3, 16, 5, 5)\n",
    "\n",
    "print(_conv_channels_2d(img, param).shape)  #must equal 10, 16, 32, 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "\n",
    "*Input gradients*\n",
    "\n",
    "We simply add two outer loops for in and out channels.\n",
    "\n",
    "Here is the previous code for input_grad for your comparison references\n",
    "\n",
    "<code>def _input_grad_sample_2d(sample: ndarray,\n",
    "                          param: ndarray,\n",
    "                          grad: ndarray) -> ndarray:\n",
    "    param_size = param.shape[0]\n",
    "    grad = _pad_2d_sample(grad, param_size // 2)  #sample refers to each sample\n",
    "    input_grad = np.zeros_like(sample)\n",
    "    for i_w in range(sample.shape[0]):  #img width\n",
    "        for i_h in range(sample.shape[1]):  #img height\n",
    "            for p_w in range(param_size):      #filter width\n",
    "                for p_h in range(param_size):    #filter height\n",
    "                    input_grad[i_w][i_h] += grad[i_w + param_size - 1 - p_w][i_h + param_size - 1 - p_h] * param[p_w][p_h]\n",
    "    return input_grad\n",
    "<code>\n",
    "<code>def _input_grad_2d(samples: ndarray,\n",
    "                   param: ndarray,\n",
    "                   grad: ndarray) -> ndarray:\n",
    "    grads = [_input_grad_sample_2d(samples[i], param, grad[i]) for i in range(grad.shape[0])]    \n",
    "    return np.stack(grads)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_grad_sample_channel_2d(sample: ndarray,\n",
    "                       grad: ndarray,\n",
    "                       param: ndarray) -> ndarray:\n",
    "    '''\n",
    "    sample: [in_channels, img_width, img_height]\n",
    "    grad: [out_channels, img_width, img_height]\n",
    "    param: [in_channels, out_channels, img_width, img_height]    \n",
    "    '''\n",
    "    #for looping\n",
    "    param_size = param.shape[2]\n",
    "    img_size = sample.shape[1]\n",
    "    in_channels = sample.shape[0]\n",
    "    out_channels = param.shape[1]\n",
    "    \n",
    "    #for holding the result\n",
    "    input_grad = np.zeros_like(sample)\n",
    "    \n",
    "    #for the output grad\n",
    "    grad = _pad_2d_sample_channel(grad, param_size // 2)\n",
    "    \n",
    "    for c_in in range(in_channels):\n",
    "        for c_out in range(out_channels):\n",
    "            for i_w in range(img_size):\n",
    "                for i_h in range(img_size):\n",
    "                    for p_w in range(param_size):\n",
    "                        for p_h in range(param_size):\n",
    "                            input_grad[c_in][i_w][i_h] += \\\n",
    "                            grad[c_out][i_w+param_size-p_w-1][i_h+param_size-p_h-1] \\\n",
    "                            * param[c_in][c_out][p_w][p_h]\n",
    "    return input_grad\n",
    "\n",
    "def _input_grad_channel_2d(samples: ndarray,\n",
    "                grad: ndarray, \n",
    "                param: ndarray) -> ndarray:\n",
    "\n",
    "    grads = [_input_grad_sample_channel_2d(samples[i], grad[i], param) for i in range(grad.shape[0])]    \n",
    "\n",
    "    return np.stack(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Param gradients*\n",
    "\n",
    "Recall that the previous code without channel looks like this:\n",
    "\n",
    "<code>def \\_param_grad_2d(input_: ndarray,\n",
    "                   grad: ndarray, \n",
    "                   param: ndarray) -> ndarray:=\n",
    "    param_size = param.shape[0]  #for filter width, height\n",
    "    input_pad = \\_pad_2d(input_, param_size // 2)  #input_pad\n",
    "    param_grad = np.zeros_like(param)\n",
    "    img_shape = input_.shape[1:]  #get only 5, 5\n",
    "    for s in range(input_.shape[0]):  #loop samples\n",
    "        for i_w in range(img_shape[0]):  #loop img width\n",
    "            for i_h in range(img_shape[1]):  #loop img height\n",
    "                for p_w in range(param_size):   #loop param width\n",
    "                    for p_h in range(param_size):   #loop param height\n",
    "                        param_grad[p_w][p_h] += input_pad[s][i_w+p_w][i_h+p_h] \n",
    "                            \\* grad[s][i_w][i_h]\n",
    "    return param_grad\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_grad_2d_channel(input_: ndarray,\n",
    "                grad: ndarray, \n",
    "                param: ndarray) -> ndarray:\n",
    "    '''\n",
    "    input_: [in_channels, img_width, img_height]\n",
    "    grad: [out_channels, img_width, img_height]\n",
    "    param: [in_channels, out_channels, img_width, img_height]    \n",
    "    '''\n",
    "    \n",
    "    #for looping\n",
    "    sample_size = input_.shape[0]\n",
    "    param_size = param.shape[2]\n",
    "    img_size = input_.shape[2]\n",
    "    in_channels = input_.shape[1]\n",
    "    out_channels = grad.shape[1]\n",
    "    img_shape = grad.shape[2:]\n",
    "\n",
    "    #for holding the results\n",
    "    param_grad = np.zeros_like(param)    \n",
    "    \n",
    "    #use for calculating param grads\n",
    "    input_pad = _pad_2d_channel(input_, param_size // 2)\n",
    "    \n",
    "    for i in range(sample_size):  \n",
    "        for c_in in range(in_channels):\n",
    "            for c_out in range(out_channels):\n",
    "                for i_w in range(img_shape[0]):\n",
    "                    for i_h in range(img_shape[1]):\n",
    "                        for p_w in range(param_size):\n",
    "                            for p_h in range(param_size):\n",
    "                                param_grad[c_in][c_out][p_w][p_h] += \\\n",
    "                                input_pad[i][c_in][i_w+p_w][i_h+p_h] \\\n",
    "                                * grad[i][c_out][i_w][i_h]\n",
    "    return param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing gradients\n",
    "\n",
    "To test, let's simply create a convolution sum based on channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d_channel_sum(imgs: ndarray,\n",
    "                        param: ndarray):\n",
    "    return _conv_channels_2d(imgs, param).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual:  -5.345935650419051\n",
      "Our code:  -5.345935650418792\n"
     ]
    }
   ],
   "source": [
    "#let's create a random image\n",
    "imgs = np.random.randn(10, 3, 32, 32)  #(samples, channels, width, height)\n",
    "param = np.random.randn(3, 16, 5, 5)   #(in_channels, out_channels, width, height)\n",
    "\n",
    "imgs_2 = imgs.copy()\n",
    "imgs_2[3][1][2][19] += 1\n",
    "\n",
    "#let's test our input_grad function\n",
    "\n",
    "print(\"Manual: \", conv_2d_channel_sum(imgs_2, param) - conv_2d_channel_sum(imgs, param))\n",
    "\n",
    "input_grad = _input_grad_channel_2d(imgs,\n",
    "                         np.ones((10, 16, 32, 32)),\n",
    "                         param)\n",
    "\n",
    "print(\"Our code: \", input_grad[3][1][2][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual:  55.34918873134302\n",
      "Our code:  55.34918873134217\n"
     ]
    }
   ],
   "source": [
    "#let's test our param_grad function\n",
    "param_2 = param.copy()\n",
    "param_2[0][8][0][2] += 1\n",
    "\n",
    "print(\"Manual: \", conv_2d_channel_sum(imgs, param_2) - conv_2d_channel_sum(imgs, param))\n",
    "\n",
    "param_grad = _param_grad_2d_channel(imgs,\n",
    "                         np.ones((10, 16, 32, 32)),\n",
    "                         param)\n",
    "\n",
    "print(\"Our code: \", param_grad[0][8][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together!!\n",
    "\n",
    "We need to implement a few more pieces before we can have a working CNN model:\n",
    "\n",
    "1. We have to implement the <code>Flatten</code> operation which is just simple reshape method\n",
    "\n",
    "2. We have to incorporate this <code>Operation</code> as well as the <code>Conv2DOpOperation</code> into a <code>Conv2D</code> Layer\n",
    "\n",
    "3.  Finally, for it to be runnable, we have to write an optimized version.  I have included this code in the \n",
    "\n",
    "#### Flattening\n",
    "\n",
    "Flatten is fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool = False) -> ndarray:\n",
    "        #squeeze everything to the second dimension\n",
    "        #(10, 3, 32, 32) --> (10, 3072) \n",
    "        return self.input_.reshape(self.input_.shape[0], -1)  \n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        #simply transform back\n",
    "        return output_grad.reshape(self.input_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer\n",
    "\n",
    "Setting up the Layer is also fairly easy.  We just have to make sure it is possible to set the flag flatten depending on whether we want the output of this layer to be passed forward into another convolutional layer or passed into another fully connected layer for predictions.  Flatten should be set True when the next layer is a Dense layer where it expects a flatten input of shape <code>(num_samples, num_features)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    def __init__(self,\n",
    "                 out_channels: int,\n",
    "                 param_size: int,\n",
    "                 dropout: int = 1.0,\n",
    "                 weight_init: str = \"glorot\",\n",
    "                 activation: Operation = Linear(),\n",
    "                 flatten: bool = False) -> None:\n",
    "        super().__init__(out_channels)\n",
    "        self.param_size = param_size\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        self.dropout = dropout\n",
    "        self.weight_init = weight_init\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> ndarray:\n",
    "\n",
    "        self.params = []\n",
    "        in_channels = input_.shape[1]\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2/(in_channels + self.out_channels)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        conv_param = np.random.normal(loc=0,\n",
    "                                      scale=scale,\n",
    "                                      size=(input_.shape[1],  # input channels\n",
    "                                     self.out_channels,\n",
    "                                     self.param_size,\n",
    "                                     self.param_size))\n",
    "\n",
    "        self.params.append(conv_param)\n",
    "\n",
    "        self.operations = []\n",
    "        self.operations.append(Conv2D_Op(conv_param))\n",
    "        self.operations.append(self.activation)\n",
    "\n",
    "        if self.flatten:\n",
    "            self.operations.append(Flatten())\n",
    "\n",
    "        if self.dropout < 1.0:\n",
    "            self.operations.append(Dropout(self.dropout))\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation and code optimization\n",
    "\n",
    "The Operation will need to be revised, to be a more optimized version. As those of you who are familiar with computational complexity will realize, this code is catastrophically slow: to calculate the parameter gradient, we needed to write seven nested for loops! There’s nothing wrong with doing this, since the purpose of writing the convolution operation from scratch was to solidify our understanding of how CNNs work. \n",
    "\n",
    "We’ll show how to express the batch, multichannel convoution operation in terms of a batch matrix multiplication to implement it efficiently in NumPy.   To understand how the convolution works, consider what happens in the forward pass of a fully connected neural network:\n",
    "\n",
    "We receive an input of size <code>[batch_size, in_features]</code>.\n",
    "We multiply it by a parameter of size <code>[in_features, out_features]</code>.\n",
    "We get a resulting output of size <code>[batch_size, out_features]</code>.\n",
    "\n",
    "In a convolutional layer, by contrast:\n",
    "\n",
    "We receive an input of size <code>[batch_size, in_channels, img_height, img_width]</code>.\n",
    "We convolve it with a parameter of size <code>[in_channels, out_channels, param_height, param_width]</code>.\n",
    "We get a resulting output of size <code>[batch_size, out_channels, img_height, img_width]</code>.\n",
    "\n",
    "**So the question is how to make the multiplication possible, since we need to prepare it to be something in the form of (i, j) @ (j, k) = (i, k)**\n",
    "\n",
    "The key to making the convolution operation look more like a regular feed-forward operation is to first extract img_height × img_width “image patches” from each channel of the input image. Once these patches are extracted, the input can be reshaped so that the convolution operation can be expressed as a batch matrix multiplication using NumPy’s <code>np.matmul</code> function.\n",
    "\n",
    "The steps are the followings:\n",
    "\n",
    "1. Get image patches of size <code>[img_height x img_width, batch_size, in_channels, filter_size, filter_size]</code>\n",
    "\n",
    "2. Reshape this to be <code>[batch_size, img_height × img_width, in_channels × filter_size× filter_size]</code>\n",
    "\n",
    "3. Reshape parameter to be <code>[in_channels × filter_size × filter_size, out_channels]</code>\n",
    "\n",
    "4. After we do a batch matrix multiplication, the result will be <code>[batch_size, img_height × img_width, out_channels]</code>\n",
    "\n",
    "5. Reshape this to be <code>[batch_size, out_channels, img_height, img_width]</code>\n",
    "\n",
    "That's it.  That's the forward pass!\n",
    "\n",
    "For backward pass, it's the same concept.  We are just finding the right squeeze-reorder-reshape method so that things can be multiplied into desired shape.  Fall in love with vectors?  Fantastic right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D_Op(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        super().__init__(W)\n",
    "        self.param_size = W.shape[2]\n",
    "        self.param_pad = self.param_size // 2\n",
    "\n",
    "    def _pad_1d(self, inp: ndarray) -> ndarray:\n",
    "        z = np.array([0])\n",
    "        z = np.repeat(z, self.param_pad)\n",
    "        return np.concatenate([z, inp, z])\n",
    "\n",
    "    def _pad_1d_batch(self,\n",
    "                      inp: ndarray) -> ndarray:\n",
    "        outs = [self._pad_1d(obs) for obs in inp]\n",
    "        return np.stack(outs)\n",
    "\n",
    "    def _pad_2d_obs(self,   #obs stands for observation\n",
    "                    inp: ndarray):\n",
    "        inp_pad = self._pad_1d_batch(inp)\n",
    "\n",
    "        other = np.zeros((self.param_pad, inp.shape[0] + self.param_pad * 2))\n",
    "\n",
    "        return np.concatenate([other, inp_pad, other])\n",
    "\n",
    "    def _pad_2d_channel(self,\n",
    "                        inp: ndarray):\n",
    "        '''\n",
    "        inp has dimension [num_channels, image_width, image_height]\n",
    "        '''\n",
    "        return np.stack([self._pad_2d_obs(channel) for channel in inp])\n",
    "\n",
    "    def _get_image_patches(self,\n",
    "                           input_: ndarray):\n",
    "        '''\n",
    "        imgs_batch: [batch_size, channels, img_width, img_height]\n",
    "        '''\n",
    "        \n",
    "        #pad the images\n",
    "        imgs_batch_pad = np.stack([self._pad_2d_channel(obs) for obs in input_])\n",
    "        patches = []\n",
    "        img_height = imgs_batch_pad.shape[2]\n",
    "        \n",
    "        #for each location in the images, cut the filter width x filter height image\n",
    "        #and stack them\n",
    "        for h in range(img_height-self.param_size+1):\n",
    "            for w in range(img_height-self.param_size+1):\n",
    "                patch = imgs_batch_pad[:, :, h:h+self.param_size, w:w+self.param_size]\n",
    "                patches.append(patch)\n",
    "        #[img_height * img_width, batch_size, in_channels, param_width, param_height]\n",
    "        return np.stack(patches)\n",
    "\n",
    "    def _output(self,\n",
    "                inference: bool = False):\n",
    "        '''\n",
    "        conv_in: [batch_size, channels, img_width, img_height]\n",
    "        param: [in_channels, out_channels, fil_width, fil_height]\n",
    "        '''\n",
    "    #     assert_dim(obs, 4)\n",
    "    #     assert_dim(param, 4)\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_height = self.input_.shape[2]\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        patch_size = self.param.shape[0] * self.param.shape[2] * self.param.shape[3]\n",
    "\n",
    "        #shape: [img_height * img_width, batch_size, in_channels, param_width, param_height]\n",
    "        patches = self._get_image_patches(self.input_)\n",
    "\n",
    "        #reshape to: [batch_size, img_height * img_width, in_channels, param_width, param_height]\n",
    "        #then squeeze into [batch_size, img_height * img_width, in_channels * param_width * param_height]\n",
    "        patches_reshaped = (patches\n",
    "                            .transpose(1, 0, 2, 3, 4)\n",
    "                            .reshape(batch_size, img_size, -1))\n",
    "        \n",
    "        #shape of param: [in_channels, out_channels, param_width, param_height]\n",
    "        #make it into: [in_channels, param_width, param_height, out_channels]\n",
    "        #then squeeze into: [in_channel * param_width * param_height, out_channels]\n",
    "        param_reshaped = (self.param\n",
    "                          .transpose(0, 2, 3, 1)\n",
    "                          .reshape(patch_size, -1))\n",
    "\n",
    "        #now patch @ param = \n",
    "        #[batch_size, img_height * img_width, in_channels * param_width * param_height] @\n",
    "        #[in_channel * param_width * param_height, out_channels] = \n",
    "        #[batch_size, img_height * img_width, out_channels]\n",
    "        #then reshape into [batch_size, img_height, img_width, out_channels]\n",
    "        #then reorder into [batch_size, out_channels, img_height, img_width]\n",
    "        #yay!\n",
    "        output_reshaped = (\n",
    "            np.matmul(patches_reshaped, param_reshaped)\n",
    "            .reshape(batch_size, img_height, img_height, -1)\n",
    "            .transpose(0, 3, 1, 2))\n",
    "\n",
    "        return output_reshaped\n",
    "\n",
    "    \n",
    "    def _input_grad(self, output_grad: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        #simple parameter\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        img_height = self.input_.shape[2]\n",
    "\n",
    "        #first the output_grad has shape of [batch_sizes, out_channel, param_width, param_height]\n",
    "        #then, _get_image_patches will give out a shape of\n",
    "        #[img_height * img_width, batch_size, out_channels, param_width, param_height]\n",
    "        #then we reorder to [batch_size, img_height * img_width, out_channels, param_width, param_height]\n",
    "        #then squeeze into [batch_size * img_height * img_width, out_channels * param_width * param_height]\n",
    "        output_patches = (self._get_image_patches(output_grad)\n",
    "                          .transpose(1, 0, 2, 3, 4)\n",
    "                          .reshape(batch_size * img_size, -1))\n",
    "        #param shape is\n",
    "        #[in_channels, out_channels, param_width, param_height]\n",
    "        #reshape into [in_channels, out_channels * param_width * param_height]\n",
    "        #transpose to get [out_channels * param_width * param_height, in_channels]\n",
    "        param_reshaped = (self.param\n",
    "                          .reshape(self.param.shape[0], -1)\n",
    "                          .transpose(1, 0))\n",
    "        \n",
    "        #gradient of input is simply gradients @ W.T\n",
    "        #output_patches have shape of \n",
    "        #[batch_size * img_height * img_width, out_channels * param_width * param_height]\n",
    "        #param shape of [out_channels * param_width * param_height, in_channels]\n",
    "        #the dot product is [batch_size * img_height * img_width, in_channels]\n",
    "        #then we reshape to [batch_size, img_height, img_width, in_channels]\n",
    "        #then we reorder to [batch_size, in_channels, img_height, img_width]\n",
    "        #input_grad should be the same shape as input, which is correct\n",
    "        return (\n",
    "            np.matmul(output_patches, param_reshaped)\n",
    "            .reshape(batch_size, img_height, img_height, self.param.shape[0])\n",
    "            .transpose(0, 3, 1, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        in_channels = self.param.shape[0]\n",
    "        out_channels = self.param.shape[1]\n",
    "\n",
    "        #input has a shape of\n",
    "        #[batch_size, in_channels, img_width, img_height]\n",
    "        #_get_image_patches will get [img_width * img_height, batch_size, in_channels, param_height, param_width]\n",
    "        #reshape to [batch_size * img_width * img_height, in_channels * param_height * param_width]\n",
    "        #then reorder to [in_channels * param_height * param_width, batch_size * img_width * img_height]\n",
    "        in_patches_reshape = (\n",
    "            self._get_image_patches(self.input_)\n",
    "            .reshape(batch_size * img_size, -1)\n",
    "            .transpose(1, 0)\n",
    "            )\n",
    "        \n",
    "        #output grad has a shape of\n",
    "        #[batch_sizes, out_channel, img_width, img_height]\n",
    "        #reorder to [batch_sizes, img_width, img_height, out_channel]\n",
    "        #reshape to [batch_sizes * img_width * img_height, out_channel]\n",
    "        out_grad_reshape = (output_grad\n",
    "                            .transpose(0, 2, 3, 1)\n",
    "                            .reshape(batch_size * img_size, -1))\n",
    "    \n",
    "        #gradient of param is simply X.T @ gradients\n",
    "        #in_patch @ out_grad\n",
    "        #[in_channels * param_height * param_width, batch_size * img_width * img_height]  @\n",
    "        #[batch_sizes * img_width * img_height, out_channel] = \n",
    "        #[in_channels * param_height * param_width, out_channel]\n",
    "        #then we reshape to [in_channels, param_width, param_height, out_channel]\n",
    "        #reorder to [in_channels, out_channels, param_width, param_height]\n",
    "        return (np.matmul(in_patches_reshape,\n",
    "                          out_grad_reshape)\n",
    "                .reshape(in_channels, self.param_size, self.param_size, out_channels)\n",
    "                .transpose(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can actually try out code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (60000, 784)\n",
      "X conv shape:  (60000, 1, 28, 28)\n",
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#1. Load data\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.target = mnist.target.astype(int)\n",
    "\n",
    "#2. Test train split\n",
    "X_train = mnist['data'][:60000]\n",
    "y_train = mnist['target'][:60000]\n",
    "\n",
    "X_test = mnist['data'][60000:]\n",
    "y_test = mnist['target'][60000:]\n",
    "\n",
    "#3. Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#4. Reshape X so that it has channel data\n",
    "print(\"X train shape: \", X_train.shape)\n",
    "X_train_conv, X_test_conv = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)\n",
    "print(\"X conv shape: \", X_train_conv.shape)\n",
    "\n",
    "#4. One hot encoding\n",
    "from sklearn import preprocessing\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "\n",
    "#sklearn expects a 2D array thus we have to reshape to (-1, 1)\n",
    "y_train_encode = onehot.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encode = onehot.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(y_train_encode.shape, y_test_encode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a CNN model now!  We gonna use only one CNN layer...since training CNN can take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 0.508\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Conv2D(out_channels=32,\n",
    "                   param_size=5,\n",
    "                   dropout=0.8,\n",
    "                   weight_init=\"glorot\",\n",
    "                   flatten=True,\n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr = 0.01, momentum=0.9))\n",
    "trainer.fit(X_train_conv, y_train_encode, X_test_conv, y_test_encode,\n",
    "            epochs = 1,\n",
    "            eval_every = 1,\n",
    "            seed=20200720,\n",
    "            batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9153\n"
     ]
    }
   ],
   "source": [
    "#5. define a simple accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calc_accuracy(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test, inference=True)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
    "    \n",
    "calc_accuracy(model, X_test_conv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, with only one layer of CNN, and one iteration, we can already achieved over 90\\% accuracy!\n",
    "\n",
    "Phew!  We are finally done.  It's a bit tough but it's satisfying, right?\n",
    "\n",
    "Next class, we shall explore other type of data, namely, sequential data (text, signal), in which the previous data affects how we interpret the current data.  In this case, we have a specialized network called RNN (Recurrent Neural Netork).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
