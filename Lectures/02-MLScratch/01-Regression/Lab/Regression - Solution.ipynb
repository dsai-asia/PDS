{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  25.68794672346631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "#actually you can do like this too\n",
    "#X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "class LinearRegression:\n",
    "    #if batch, set alpha to smaller values\n",
    "    def __init__(self, alpha=0.001, max_iter=10000, \n",
    "            loss_old=10000, tol=1e-5, method=\"batch\"):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_old = loss_old\n",
    "        self.tol = tol\n",
    "        self.method = method\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        iter_stop = 0\n",
    "        list_of_used_ix = [] #<===without replacement\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            \n",
    "            if self.method != \"batch\":\n",
    "                i = np.random.randint(X.shape[0])\n",
    "                while i in list_of_used_ix:\n",
    "                    i = np.random.randint(X.shape[0])\n",
    "                X_train = X[i, :].reshape(1, -1)\n",
    "                y_train = y[i]\n",
    "                list_of_used_ix.append(i)\n",
    "                if len(list_of_used_ix) == X.shape[0]:\n",
    "                    list_of_used_ix = []\n",
    "            else:\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "            \n",
    "            yhat = self.h_theta(X_train)\n",
    "            error = yhat - y_train\n",
    "            \n",
    "            #early stopping\n",
    "            loss_new = self.mse(yhat, y_train)\n",
    "            if self.delta_loss(loss_new, self.loss_old, self.tol):  #np.allclose\n",
    "                iter_stop = i\n",
    "                break\n",
    "            self.loss_old = loss_new\n",
    "\n",
    "            grad = self.gradient(X_train, error)\n",
    "            self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "    #can name it predict for easy understanding\n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "    def delta_loss(self, loss_new, loss_old, tol):\n",
    "        return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "model = LinearRegression(method=\"sto\") #<==try put method=\"sto\"\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.h_theta(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "\n",
    "#print the mse\n",
    "print(\"MSE: \", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
