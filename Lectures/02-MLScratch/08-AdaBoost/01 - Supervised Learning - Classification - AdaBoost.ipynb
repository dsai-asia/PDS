{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - AdaBoost\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is a general strategy for learning classifiers by combining simpler ones. The idea of boosting is to take a \"weak classifier\" — that is, any classifier that will do at least slightly better than chance — and use it to build a much better classifier, thereby boosting the performance of the weak classification algorithm. This boosting is done by averaging the outputs of a collection of weak classifiers.  The common form of hypothesis function for boosting is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) & =  \\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) ) \\\\\n",
    "& = \\Sigma_{s=1}^{S}\\alpha_sh_s(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $S =$ number of classifiers and $\\alpha$ is the weight associated with each classifier \n",
    "\n",
    "The among the first, and therefore popular boosting algorithm is **AdaBoost**, so-called because it is *adaptive.*\n",
    "\n",
    "AdaBoost is extremely simple to use and implement (far simpler than SVMs), and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Anyhow, Decision Tree with max_depth=1 and max_leaf_nodes=2 are often used (also known as **stump**)\n",
    "\n",
    "Suppose we are given training data ${(\\mathbf{x_i}, y_i)}$, where $\\mathbf{x_i} \\in \\mathbb{R}^n$ and $y_i \\in \\{-1, 1\\}$.  And suppose we are given a (potentially large) number (denoted $S$) of weak classifiers, denoted $h_s(x) \\in \\{-1, 1\\}$ where $s = 1, 2, \\cdots, S$, and for each classifier, we define $\\alpha_s$ as the *voting power* of the classifier $h_s(x)$. Then, the hypothesis function is based on a linear combination of the weak classifier and is written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) & = \\text{sign}\\big(\\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\Sigma_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more) in our hypothesis function since their accuracy is relatively better compared to other classifiers.  To get this alpha, we should define what is \"good\" classifier.  This is simple, since good classifier should simply has the maximum number of accurate classified samples as:\n",
    "\n",
    "$$ max \\big( \\Sigma_{i=1}^m I(h_s(x_i) = y_i) \\big)$$\n",
    "\n",
    "or can be written as minimization function as\n",
    "\n",
    "$$ min \\big( \\Sigma_{i=1}^m I(h_s(x_i) \\neq y_i) \\big)$$\n",
    "\n",
    "Aside from \"weighted\" wisdom of crowd, AdaBoost has one more capability, and that is that each subsequent classifier will try to correct the errors made by previous predictor.  In other words,  whatever samples the previous classifier misclassified, it should be prioritized in the subsequent classifier.  To realize this mechanism, the concept is to increase the penalty if those previously misclassified sample are wrong.  To do so, we first initialize the weight for each sample, which shall be applied to the first predictor $h_1(x)$ to be \n",
    "\n",
    "$$ w_i^{(s)} = \\frac{1}{m} ; s = 1;  i = 1, 2, \\cdots, m$$\n",
    "\n",
    "Then, after the first classifier was fitted, we readjust this weight by increasing weight for those misclassified sample, and decreasing weight for those correctly classified sample.  To make sure classifier will be chosen on the basis of these weighted errors, we shall revise our definition of \"good\" classifiers as follows:\n",
    "\n",
    "$$ min \\big( \\frac{\\Sigma_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i)}{\\Sigma_{i=1}^m w_i^{s}} \\big ) $$\n",
    "\n",
    "Note that the lower term is simply so that all weights sum to 1.\n",
    "\n",
    "Thus, the subsequent classifier will be chosen based on the one that can create the least weighted errors. \n",
    "\n",
    "Let's put everything into the AdaBoost algorithm as follows:\n",
    "\n",
    "define $S$\n",
    "\n",
    "**for** $i$ from 1 to m {\n",
    "\n",
    "$$w_i^{(1)} = \\frac{1}{m}$$ \n",
    "\n",
    "make sure $y \\in \\{-1, 1\\}$\n",
    "\n",
    "}\n",
    "\n",
    "**for** $s$ = 1 to $S$ {\n",
    "\n",
    "Looping through all features and threshold, identify the best stump, whose value has the minimum of this objective function:\n",
    "    \n",
    "$$\\epsilon_s = \\Sigma_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i) $$\n",
    "\n",
    "where $I$ is indicator function $I(h_s(x_i) \\neq y_i) = 1$ if $h_s(x_i) \\neq y_i$ and 0 otherwise\n",
    "\n",
    "Then calculate the voting power of the weak classifier, denoted $\\alpha_s$ and can be calculated as:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "Before fitting the next stump, we need to make sure to exaggerate the weights of *incorrectly* classified samples so our next stump will be chosen based on the new weighted objective function:\n",
    "\n",
    "Then **for** all $i$ { $$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\Sigma_{i=1}^m w_i^{s}}} $$}\n",
    "\n",
    "}\n",
    "\n",
    "This means that misclassified samples will get larger weights and correctly classified samples will get smaller weights.\n",
    "\n",
    "To predict, we simply take the weighted sum of all predictors and take the sign of them.  Recall that $S$ is number of stumps/predictors you have\n",
    "\n",
    "$$ \n",
    "  H(x) = \\text{sign}\\big(\\Sigma_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "$$\n",
    "\n",
    "Stopping criteria of AdaBoost is important to impose or else we can get some overfitting with AdaBoost by adding too many classifiers.  We can either specify the number of iterations, or when we reach a certain level of accuracy, or perform early stopping by using a validation set to detect the iteration when overfit starts to happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "W = np.full(m, 1/m)\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = W)\n",
    "    \n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train) \n",
    "    err = W[(yhat != y_train)].sum()\n",
    "        \n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    a_j = np.log ((1 - err) / err) / 2\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(-a_j * y_train * yhat)) \n",
    "    W = W / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    Hx += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implements AdaBoost using SAMME which stands for Stagewise Additive Modeling using a Multiclass Exponential Loss Function.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 Decision stumps.  A Decision stump is basically a Decision Tree with max_depth=1.  This is the default base estimator of AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index\n",
    "- Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
