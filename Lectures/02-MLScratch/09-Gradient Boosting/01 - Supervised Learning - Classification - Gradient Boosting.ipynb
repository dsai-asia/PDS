{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - Gradient Boosting\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.    The hypothesis function of gradient boosting is as follows:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "Although they look similar, notice that no alpha is applied to the first predictor.  In addition, each alpha is the same, as opposed to voting power in AdaBoost.  Typically, similar to AdaBoost, decision trees are used for each $h_i(x)$ but are not limited to stump.  In practice, min_leaves are set to around 8 to 32.\n",
    "\n",
    "Since gradient boosting actually originate from additive linear regression, we shall first talk about **gradient boosting for regression**.  Also assume that we are using **regression trees** for our regressors.\n",
    "\n",
    "### Gradient Boosting for Regression\n",
    "\n",
    "Firstly, let's look at the following equation where $h_0(x)$ is our first predictor and we would like to minimize the residual as follows:\n",
    "\n",
    "$$h_0(x) + residual_0 = y $$\n",
    "$$ residual_0 =  y - h_0(x) $$\n",
    "\n",
    "That is, we would $y$ to be as close as $h_0(x)$ such that residual is 0\n",
    "\n",
    "$$ y = h_0(x) $$\n",
    "\n",
    "The question is that is it possible to add the second predictor $h_1(x)$ such that the residual is further reduced\n",
    "\n",
    "$$ y = h_0(x) + h_1(x) $$\n",
    "\n",
    "This equation can be written as:\n",
    "\n",
    "$$h_1(x) = y - h_0(x) $$\n",
    "\n",
    "This equation informs us that if we can find a subsequent predictor that can best fit the \"residual\" (i.e. $y - h_0(x)$), then we can improve the accuracy.\n",
    "\n",
    "**How is this related to gradient descent?**\n",
    "\n",
    "Well, firstly, here is our loss function for regression:\n",
    "\n",
    "$$J = \\frac{1}{2}(y - h(x))^2$$\n",
    "\n",
    "And here, we want to minimize $J$ by gradient of the loss function in respect to by adjusting $h_x$.  We can thus treat $h_x$ as parameters and take derivatives:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = h(x) - y $$\n",
    "\n",
    "Thus, we can interpret residuals as negative gradients:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "y & = h_0(x) + h_1(x)\\\\\n",
    "& = h_0(x) + (y - h_0(x)) \\\\\n",
    "& = h_0(x) - (h_0(x) - y) \\\\\n",
    "& = h_0(x) - \\frac{\\partial J}{\\partial h_0(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So in fact, we are using \"gradient\" descent in \"gradient\" boosting to find the new model, written as:\n",
    "\n",
    "$$h_1(x) = - \\frac{\\partial J}{\\partial h_0(x)} = y - h_0(x)$$\n",
    "\n",
    "or more generally\n",
    "\n",
    "$$h_s(x) = - \\frac{\\partial J}{\\partial h_{s-1}(x)} = y - h_{s-1}(x)$$\n",
    "\n",
    "where $s$ is the index of predictor\n",
    "\n",
    "**So residuals or gradients?**\n",
    "\n",
    "Although they are equivalent in the mse loss function, it is more useful to use negative gradients as it is more general, and can apply to other loss functions as well, e.g., **cross-entropy** in the case of classification.\n",
    "\n",
    "In cross entropy, the loss function is\n",
    "\n",
    " $$J= y log h(x) + (1 - y) lg (1-h(x))$$\n",
    " \n",
    "If you look at our previous lecture on logistic regression, the derivative of this **in respect to h(x)** will be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = h(x) - y$$\n",
    "\n",
    "This may look the same as mse, but note that our $h(x)$ (i.e., regression tree) outputs continuous values.  In order to transform $h(x)$ into discrete class, we shall transform using sigmoid function $g$ as follows:\n",
    "\n",
    "$$g(h(x)) = g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "For multiclass classification, $g$ is defined as the softmax function:\n",
    "\n",
    "$$g(h(x)) = g(z) = \\frac{e^z_c}{\\Sigma_{i=1}^{k} e^z_k}$$\n",
    "\n",
    "Also remind that to use softmax function, we need to first one-hot encode our y.  And during prediction, we need to perform <code>np.argmax</code> along the axis=1\n",
    "\n",
    "\n",
    "**Adding learning rate**\n",
    "\n",
    "To make sure adding the subsequent predictor would not overfit our model, we shall add an learning rate $\\alpha$ in front of this, which shall be the same across all predictors (different from AdaBoost where alpha is different across all predictors)\n",
    "\n",
    "$$h_s(x) = - \\alpha \\frac{\\partial J}{\\partial h_{s-1}(x)}$$\n",
    "\n",
    "**What about next predictor**\n",
    "\n",
    "We can stop if we are happy, either using some predefined iterations, or whether the residual does not decrease further using some validation set.  \n",
    "\n",
    "In this case, it is obvious that 2 predictors are simply not enough.  Thus, we first need to calculate the residuals which are\n",
    "\n",
    "$$ residual_1 =  y - (h_0(x) + \\alpha h_1(x))$$\n",
    "\n",
    "then we define $h_2(x)$ as \n",
    "\n",
    "$$h_2(x) = \\alpha(y - (h_0(x) + \\alpha h_1(x)))$$\n",
    "\n",
    "And then repeat\n",
    "\n",
    "The final prediction shall use the following hypothesis function $H(x)$:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "**Summary of steps**\n",
    "\n",
    "1. Initialize the model as simply mean or some constant\n",
    "2. Predict and calculate the residual\n",
    "3. Let the next model fit the residual\n",
    "4. Predict using the combined models and calculate the residual\n",
    "5. Let the next model fit this residual\n",
    "6. Simply repeat 4-5 until stopping criteria is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = grad(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  12.945557601580582\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Our MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn \n",
    "\n",
    "sklearn has implemented GradientBoosting under the API of <code>GradientBoostingClassifier</code> for classification and <code>GradientBoostingRegressor</code> for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE:  12.945557601580584\n"
     ]
    }
   ],
   "source": [
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting, designed to be more efficient, flexible, and portable (Chen and Guestrin 2016).  In fact, XGBoost is often an important component of the winning entries in ML competitions (e.g., Kaggle).  XGBoost also offers several nice features, such as automatically taking care of early stopping: XGBoost’s API is quite similar to Scikit-Learn’s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:16.15458\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:11.84377\n",
      "[2]\tvalidation_0-rmse:8.79602\n",
      "[3]\tvalidation_0-rmse:6.72584\n",
      "[4]\tvalidation_0-rmse:5.46526\n",
      "[5]\tvalidation_0-rmse:4.65454\n",
      "[6]\tvalidation_0-rmse:4.08462\n",
      "[7]\tvalidation_0-rmse:3.76129\n",
      "[8]\tvalidation_0-rmse:3.54313\n",
      "[9]\tvalidation_0-rmse:3.37742\n",
      "[10]\tvalidation_0-rmse:3.24836\n",
      "[11]\tvalidation_0-rmse:3.18872\n",
      "[12]\tvalidation_0-rmse:3.10860\n",
      "[13]\tvalidation_0-rmse:3.09993\n",
      "[14]\tvalidation_0-rmse:3.08393\n",
      "[15]\tvalidation_0-rmse:3.08760\n",
      "[16]\tvalidation_0-rmse:3.06310\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "[18]\tvalidation_0-rmse:3.05715\n",
      "[19]\tvalidation_0-rmse:3.05827\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "\n",
      "MSE: 9.320308418219375\n"
     ]
    }
   ],
   "source": [
    "#make sure to pip install xgboost\n",
    "#for mac guys, do \"brew install libomp\" which installs openMP library\n",
    "#required for XGBoost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor() \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.1 ms ± 886 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "79.6 ms ± 329 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit xgboost.XGBRegressor().fit(X_train, y_train)\n",
    "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Modify the above scratch code such that:\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\n",
    "- Put everything into class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "Let's summarize some useful info about Gradient Boosting:\n",
    "\n",
    "Advantages:\n",
    "1. Extremely powerful - especially useful for heterogeneous data (e.g., house price, number of bedrooms). \n",
    "\n",
    "Disadvantages:\n",
    "1. They cannot be parallelized.  Obvious since they are sequential predictors.\n",
    "2. They can easily overfit, thus require careful choice of estimators or the use of regularization such as max_depth.\n",
    "3. When we talk about homogeneous data such as images, videos, audio, text, or huge amount of data, deep learning works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
