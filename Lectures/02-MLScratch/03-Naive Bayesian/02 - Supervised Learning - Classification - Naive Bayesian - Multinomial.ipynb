{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Supervised Learning - Classification - Naive Bayesian - Multinomial\n",
    "\n",
    "### Readings: \n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Classification\n",
    "\n",
    "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.  Another useful example is multinomial naive bayes, where the features are assumed to be generated from a simple multinomial distribution.  **The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive bayes is most appropriate for features that represent counts or count rates.**\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.\n",
    "\n",
    "One place where multinomial naive Bayes is often used is in **text classification**, where the features $w$ are related to word counts or frequencies within the documents to be classified and $y$ will be our class.  The formation is as follows:\n",
    "\n",
    "$$\n",
    "P(y|w) = \\frac{P(w|y)P(y)}{P(w)}\n",
    "$$\n",
    "\n",
    "Before we proceed, let's define some notations: $d$ stands for number of documents, $V$ stands for number of unique vocabulary\n",
    "\n",
    "**Implementation steps**: \n",
    "\n",
    "1. Prepare your data\n",
    "    - X and y in the right shape\n",
    "        - $X$ -> $(m, n)$\n",
    "        - $y$ -> $(m,  )$\n",
    "        - Note that theta is not needed.  Why?\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "    \n",
    "2. Using the train documents, calculate the **likelihoods** of each word.  Following multinomial distribution, for a given word $w_i$, we count how many of $w_i$ belong in class $k$, we then divide this by ALL the words that belong to $k$. This gives us the conditional probability for a word $w$ given $k$:\n",
    "\n",
    "    $$ P(w_i \\in train \\mid y=k) = \\frac{count(w_i \\in train, k)}{\\Sigma_{i=1}^{V} count(w_i \\in train, k)} $$\n",
    "    \n",
    "3. Since nothing in this world has zero probability, similarly, even we never see a particular word in some class should not gaurantee a zero probability, thus we can perform **Laplace smoothing** to account for any words with zero count\n",
    "    $$ P(w_i \\in train \\mid y=k) = \\frac{count(w_i \\in train, k) + 1}{\\Sigma_{i=1}^{V} count(w_i \\in train, k) + V} $$\n",
    "\n",
    "4. Once we get the **likelihoods** from the train data.  If given some test data, we simply use this likelihood to calculate the total likelihood of the test document.  Similarly, since we have more than one word in the test document, we need to make a product of all likelihood of each word in the test document.\n",
    "    $$P(w \\in test \\mid y=k) = \\prod_{i=1}^{pos} p(w_i \\in test \\mid y=k)$$\n",
    "    - Note that $pos$ indicates the index of each word in the test document\n",
    "    - Also note that if we have words occuring multiple times in the test document, we simply multiply that word's likelihood multiple times.  This can also be expressed as an equation where the word frequency is the power of the likelihood of the word $i$ like this:\n",
    "    $$ P(w \\in test \\mid y=k) = \\prod_{i=1}^{V} p(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$\n",
    "    \n",
    "5. Find **priors** $P(y)$ where is simply number of documents belonging to that class divided by all documents\n",
    "\n",
    "$$P(y = k) = \\frac{\\Sigma_{i=1}^{d}1(y=k)}{d} $$\n",
    "\n",
    "6. To use our (4) and (5) information for prediction, we can multiply $P(y)P(w \\in test \\mid y)$ for each class which will give us $p(y \\mid x)$ (**posteriors**)\n",
    "\n",
    "$$P(y=k)P(w \\in test \\mid y=k)$$ or\n",
    "\n",
    "$$P(y=k)\\prod_{i=1}^{V} p(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$\n",
    "\n",
    "7. Instead of probabilities, we gonna use log (base e) probabiities which have several benefits:\n",
    "    - **Speed** - Log probabilities become addition, which is faster than multiplication\n",
    "    - **Stability** - Probabilities can be too small where some significant digits can be lost during calculations. Log probabiities can prevent such underflow.  If you don't believe me, try perform $log_e(0.0000001)$ (BTW, $log_e$ is same as $ln$)\n",
    "    - **Simplicity** - Many distributions have exponential form.  Taking log cancels out the exp.  The reason we can apply $log$ is because $log$ is a monotically increasing function, thus will not alter the result \n",
    "    - **Dot product** - After log, addition can often expressed as dot product of matrix, simplifying the code implementation\n",
    "    \n",
    "   Now that you are convinced, \n",
    "   \n",
    "   $$P(y=k)\\prod_{i=1}^{V} p(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$  becomes\n",
    "   \n",
    "  $$log \\ P(y=k) + (\\text{freq of }w_i \\in test) * \\Sigma_{i=1}^{V} log \\ p(w_i \\in test \\mid y=k)$$\n",
    "  - Note 1: Log of multiplication becomes addition\n",
    "  - Note 2: Exponent of log becomes multiplicative scalar\n",
    "  \n",
    "8. Thus, in implementation we can expressed as\n",
    "\n",
    "<code>np.log(priors) + X_test @ np.log(likelihoods.T) </code>\n",
    "\n",
    "where <code>X_test</code> has been engineered/transformed in such a way that it represents the frequency of words\n",
    "\n",
    "9. Now, you will have $k$ number of posteriors for the test document.  Simply take the biggest one or <code>argmax</code>. Note that we can ignore $P(x)$ since they can be canceled on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch\n",
    "\n",
    "#### 1. Prepare some data\n",
    "\n",
    "Here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\n",
    "\n",
    "Let's download the data and take a look at the target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for simplicity here, we will select just a few of these categories, and download the training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "              'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we will print some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jono@mac-ak-24.rtsg.mot.com (Jon Ogden)\n",
      "Subject: Re: Losing your temper is not a Christian trait\n",
      "Organization: Motorola LPA Development\n",
      "Lines: 26\n",
      "\n",
      "In article <Apr.23.02.55.47.1993.3138@geneva.rutgers.edu>, jcj@tellabs.com\n",
      "(jcj) wrote:\n",
      "\n",
      "> I'd like to remind people of the withering of the fig tree and Jesus\n",
      "> driving the money changers et. al. out of the temple.  I think those\n",
      "> were two instances of Christ showing anger (as part of His human side).\n",
      "> \n",
      "Yes, and what about Paul saying:\n",
      "\n",
      "26 Be ye angry, and sin not: let not the sun go down upon your wrath:\n",
      "(Ephesians 4:26).\n",
      "\n",
      "Obviously then, we can be angry w/o sinning.\n",
      "\n",
      "Jon\n",
      "\n",
      "------------------------------------------------\n",
      "Jon Ogden         - jono@mac-ak-24.rtsg.mot.com\n",
      "Motorola Cellular - Advanced Products Division\n",
      "Voice: 708-632-2521      Data: 708-632-6086\n",
      "------------------------------------------------\n",
      "\n",
      "They drew a circle and shut him out.\n",
      "Heretic, Rebel, a thing to flout.\n",
      "But Love and I had the wit to win;\n",
      "We drew a circle and took him in.\n",
      "\n",
      "Target:  2\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0]) #first 300 words\n",
    "print(\"Target: \", train.target[0])  #start with 1, soc.religion.christian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidVectorizer\n",
    "\n",
    "Recall that in Naive Multinomial Classification, we want our features to be represented as frequency.  That is, we must feature engineer our input to be frequency, with each feature (column) representing the frequency of each word.  Of course, we will then have a lot of features like 1000+ depending the number of unique words in our train documents.  Thus, our X can be expressed in shape of $(m, V)$\n",
    "\n",
    "Here, we shall go beyond one more step, i.e., after counting the number of words, we shall perform a normalization process called TF-IDF which focuses on cutting less meaningful information like \"the\", \"a\", \"is\".\n",
    "\n",
    "Before you get more confused, let's see how we can easily engineer our documents into frequency features using <code>sklearn.feature_extraction.text.CountVectorizer</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first look at what is CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "Type:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Feature names: \", vectorizer.get_feature_names())\n",
    "print(\"Type: \", type(X))\n",
    "print(X.toarray())\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then let's look at what is TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#imagine that we already have a frequency features.  We can perform normalization\n",
    "#as a follow up\n",
    "#here we got V=3, and d=6\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explaination first:\n",
    "\n",
    "The formula is\n",
    "\n",
    "$$ \\text{TF-IDF}_{t} =  \\text{TF} * \\text{IDF} $$\n",
    "\n",
    "where $t$ is each term, $\\text{TF}$ is term frequency and $\\text{IDF}$ is inverse document frequency which can be calculated as\n",
    "\n",
    "$$ \\text{IDF}(t) = log (\\frac{d}{\\text{DF}(t)}) + 1 $$\n",
    "\n",
    "where $d$ is the total number of documents in the document set, $\\text{DF}(t)$ is the number of documents in the document set that contain term $t$.\n",
    "\n",
    "Then finally, it is normalized by Euclidean norm:\n",
    "\n",
    "$$ norm(t_i) = \\frac{t_i}{\\sqrt{t_1^2 + t_2^2 + ....+t_V^2}} $$ \n",
    "\n",
    "For example, for first term (first column),\n",
    "\n",
    "$$d = 6$$ \n",
    "$$\\text{TF}(t)_{term1doc1} = 3$$\n",
    "\n",
    "and for the $\\text{DF}$, it appears in all 6 documents, thus \n",
    "\n",
    "$$\\text{DF}(t)_{term1doc1} = 6$$ \n",
    "\n",
    "thus \n",
    "\n",
    "$$ \\text{IDF}(t)_{term1doc1} = log (\\frac{6}{6}) + 1 = 1$$\n",
    "\n",
    "thus \n",
    "\n",
    "$$ \\text{TF-IDF}_{term1doc1} = \\text{TF} * \\text{IDF} = 3 * 1 = 3 $$\n",
    "\n",
    "For term 3 (row1, col3), you will get $\\text{TF-IDF}(t)_{term3doc1} = 1 * (log (\\frac{6}{2}) + 1) = 2.0986$\n",
    "\n",
    "To normalize, simply do \n",
    "\n",
    "$$ t_{norm} = \\frac{[3, 0, 2.0986]}{\\sqrt{3^2 + 0^2 + 2.0986^2}} = [0.8515, 0, 0.5243]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand what is inverse term frequency which is commonly used on document analysis.  Let's build some pipeline and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Transform our data to features of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:    (0, 32091)\t0.06323663474836895\n",
      "  (0, 34604)\t0.08159803733249359\n",
      "  (0, 34678)\t0.09386677538713814\n",
      "  (0, 16025)\t0.03747094887347767\n",
      "  (0, 20341)\t0.053494245686987386\n",
      "  (0, 7576)\t0.023103622260498138\n",
      "  (0, 14421)\t0.11160127553180954\n",
      "  (0, 31753)\t0.04723937857426608\n",
      "  (0, 26646)\t0.10407682226672788\n",
      "  (0, 16493)\t0.1013912286522198\n",
      "  (0, 16610)\t0.09009773712773926\n",
      "  (0, 29122)\t0.08293805016995441\n",
      "  (0, 8666)\t0.16073965513378285\n",
      "  (0, 12166)\t0.19824118320941841\n",
      "  (0, 31737)\t0.02793242388131909\n",
      "  (0, 2645)\t0.11160127553180954\n",
      "  (0, 10677)\t0.054075767940987195\n",
      "  (0, 1631)\t0.10407682226672788\n",
      "  (0, 2691)\t0.2147274464147697\n",
      "  (0, 2879)\t0.19430735265559024\n",
      "  (0, 33990)\t0.07272800329036469\n",
      "  (0, 11891)\t0.07720124038682384\n",
      "  (0, 25675)\t0.07344668162795868\n",
      "  (0, 4416)\t0.07541359690487275\n",
      "  (0, 8236)\t0.11160127553180954\n",
      "  :\t:\n",
      "  (0, 1655)\t0.17106199151977294\n",
      "  (0, 20065)\t0.014736824020982909\n",
      "  (0, 11327)\t0.062343635038980855\n",
      "  (0, 20366)\t0.10407682226672788\n",
      "  (0, 22013)\t0.15958421417931642\n",
      "  (0, 23591)\t0.015581583944844032\n",
      "  (0, 32244)\t0.09912059160470921\n",
      "  (0, 8590)\t0.04214176725638149\n",
      "  (0, 22932)\t0.0675359313255703\n",
      "  (0, 18370)\t0.018104334029087538\n",
      "  (0, 31463)\t0.09246285796354252\n",
      "  (0, 35183)\t0.06657908279354553\n",
      "  (0, 20308)\t0.09000215396924795\n",
      "  (0, 26562)\t0.019778135877998683\n",
      "  (0, 30617)\t0.014729984000427944\n",
      "  (0, 23326)\t0.19430735265559024\n",
      "  (0, 18765)\t0.21416397135871157\n",
      "  (0, 9095)\t0.08596084824767848\n",
      "  (0, 21988)\t0.14999728067575058\n",
      "  (0, 27916)\t0.19430735265559024\n",
      "  (0, 1572)\t0.10586962394127693\n",
      "  (0, 4688)\t0.19083746819410952\n",
      "  (0, 20502)\t0.1308309917954776\n",
      "  (0, 18774)\t0.19430735265559024\n",
      "  (0, 14822)\t0.014729984000427944\n",
      "y_train:  2\n"
     ]
    }
   ],
   "source": [
    "#transform our X to vectorized data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "X_test = vectorizer.transform(test.data)\n",
    "X_test = X_test.toarray()  #vectorizer gives us a sparse matrix; convert back to dense matrix\n",
    "\n",
    "y_train = train.target\n",
    "y_test = test.target\n",
    "\n",
    "print(\"X_train: \", X_train[0])\n",
    "print(\"y_train: \", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 to 5:Calculating likelihood anrd prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(X_class, laplace=1):\n",
    "    #sklearn called alpha\n",
    "    return ((X_class.sum(axis=0)) + laplace) / (np.sum(X_class.sum(axis=0) + laplace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(X_class, m):\n",
    "    return X_class.shape[0] / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train):\n",
    "    m, n = X_train.shape\n",
    "    classes = np.unique(y_train)  #list of class\n",
    "    k = len(classes) #number of class\n",
    "    \n",
    "    priors = np.zeros(k) #prior for each classes\n",
    "    likelihoods = np.zeros((k, n)) #likehood for each class of each feature\n",
    "    \n",
    "    for idx, label in enumerate(classes):\n",
    "        X_train_c = X_train[y_train==label]\n",
    "        priors[idx] = prior(X_train_c, m)\n",
    "        likelihoods[idx, :] = likelihood(X_train_c)\n",
    "    return priors, likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Log probabilities (I just skip 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, priors, likelihoods, classes):\n",
    "    return np.log(priors) + X_test @ np.log(likelihoods.T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Let's use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors, likelihoods = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  [0 1 2 3]\n",
      "X_test shape:  (1432, 35329)\n",
      "priors shape:  (4,)\n",
      "likelihoods shape:  (4, 35329)\n",
      "X_test @ np.log(likelihoods.T) shape:  (1432, 4)\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_test)\n",
    "print(\"Classes: \", classes)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"priors shape: \", priors.shape)\n",
    "print(\"likelihoods shape: \", likelihoods.shape)\n",
    "print(\"X_test @ np.log(likelihoods.T) shape: \", (X_test @ np.log(likelihoods.T) ).shape)\n",
    "yhat = predict(X_test, priors, likelihoods, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two yhat samples:  [[-91.43527546 -90.73890674 -88.35563902 -89.56907333]\n",
      " [-78.97708704 -79.86434517 -79.68980348 -81.16942474]]\n",
      "Yhat shape after argmax:  (1432,)\n",
      "First two yhat samples:  [2 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"First two yhat samples: \", yhat[:2])\n",
    "yhat = np.argmax(yhat, axis=1)\n",
    "print(\"Yhat shape after argmax: \", yhat.shape)\n",
    "print(\"First two yhat samples: \", yhat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8016759776536313\n",
      "=========Average precision score=======\n",
      "Class 0 score:  0.888341920518241\n",
      "Class 1 score:  0.8744630809734135\n",
      "Class 2 score:  0.6122064043881043\n",
      "Class 3 score:  0.332994836297269\n",
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       389\n",
      "           1       0.92      0.92      0.92       394\n",
      "           2       0.62      0.98      0.76       398\n",
      "           3       1.00      0.19      0.32       251\n",
      "\n",
      "    accuracy                           0.80      1432\n",
      "   macro avg       0.88      0.75      0.73      1432\n",
      "weighted avg       0.86      0.80      0.77      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "print(\"Accuracy: \", np.sum(yhat == y_test)/len(y_test))\n",
    "\n",
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "yhat_binarized = label_binarize(yhat, classes=[0, 1, 2, 3])\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)\n",
    "    \n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)  ##later for checking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#fun thing you can do\n",
    "some_string = \"Programming is fun\"\n",
    "transformed = vectorizer.transform([some_string])\n",
    "transformed.shape\n",
    "\n",
    "prediction = model.predict(transformed)\n",
    "print(train.target_names[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(32.99999999999999, 0.5, 'predicted')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#use confusion matrix\n",
    "mat = confusion_matrix(y_test, yhat)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(mat.T, annot=True, fmt=\"d\",\n",
    "           xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, it seems the accuracy is quite ok, but religion talk was often confused with christianity talk which kinda make sense.\n",
    "\n",
    "Fun part is we can reuse the model.predict to predict any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Average precision score=======\n",
      "Class 0 score:  0.888341920518241\n",
      "Class 1 score:  0.8744630809734135\n",
      "Class 2 score:  0.6122064043881043\n",
      "Class 3 score:  0.332994836297269\n",
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       389\n",
      "           1       0.92      0.92      0.92       394\n",
      "           2       0.62      0.98      0.76       398\n",
      "           3       1.00      0.19      0.32       251\n",
      "\n",
      "    accuracy                           0.80      1432\n",
      "   macro avg       0.88      0.75      0.73      1432\n",
      "weighted avg       0.86      0.80      0.77      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "yhat_binarized = label_binarize(yhat, classes=[0, 1, 2, 3])\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)\n",
    "\n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Naive Bayes\n",
    "\n",
    "Usually only as baseline!  Because naive Bayesian classifiers make such stringent assumptions about data, they will **generally NOT perform as well as a more complicated model.**\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) tunable parameters\n",
    "\n",
    "Naive Bayes classifiers tend to perform well only when your data is clearly separable or has high dimension.\n",
    "\n",
    "The reason for high dimension is because new dimensions usually add more information, thus data become more separable.  Thus, if you have really large dataset, try Naive Bayes and it may surprise you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
