{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.4 Deep Learning -  PyTorch\n",
    "\n",
    "- [WEIDMAN] Ch7\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "Here we introduce PyTorch, an increasingly popular neural network framework based on **automatic differentiation**, which we introduced at the beginning of previous chapter.\n",
    "\n",
    "As in the rest of the book, weâ€™ll write our code in a way that maps to the mental models of how neural networks work, writing classes for Layers, Trainers, and so on. I recommend you to watch the 60-min blitz to understand different features of pytorch (https://pytorch.org/tutorials/).  Pytorch is really cool and simple!  For example, there is a function <code>tensor.to(cuda)</code> so you can use the GPU to run any model which is much faster.  I highly recommend checking it out.\n",
    "\n",
    "### Basics\n",
    "\n",
    "1. Basic autograd example               \n",
    "2. Loading data from numpy                \n",
    "3. Input pipline                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic autograd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x:  tensor(2.)\n",
      "Gradient of w:  tensor(1.)\n",
      "Gradient of b:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Create tensors.\n",
    "# only tensors of floating point dtype can get gradient\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients\n",
    "# Pytorch tensor can automatically compute the derivative\n",
    "# of the parameters in respect to loss\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(\"Gradient of x: \", x.grad)    # x.grad = 2 \n",
    "print(\"Gradient of w: \", w.grad)    # w.grad = 1 \n",
    "print(\"Gradient of b: \", b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#put this for puffers.cs.ait.ac.th\n",
    "#os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "#os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Retrieve batch of data\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print(images.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's have linear regression as a case study to study the different components of pyTorch.  You will fall in love with it (at least, compared to writing from scratch!).  These are the following components we will be covering:\n",
    "\n",
    "1. Specifying input and target\n",
    "2. Dataset and DataLoader\n",
    "3. nn.Linear (Dense)\n",
    "4. Define loss function\n",
    "5. Define optimizer function\n",
    "6. Train the model\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\">\n",
    "\n",
    "n a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$yield_{apple}  = w_{11} * temp + w_{12} * rainfall + w_{13} * humidity + b_{1}$$\n",
    "\n",
    "$$yield_{orange} = w_{21} * temp + w_{22} * rainfall + w_{23} * humidity + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> by looking at the training data, to make accurate predictions for new data (i.e. to predict the yields for apples and oranges in a new region using the average temperature, rainfall and humidity). This is done by adjusting the weights slightly many times to make better predictions, using an optimization technique called gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Specifiying input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "x_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(x_train)\n",
    "targets = torch.from_numpy(y_train)\n",
    "print(inputs.size())\n",
    "print(targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset and DataLoader\n",
    "\n",
    "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation (<code>[0:3]</code> in the above code). It returns a tuple (or pair), in which the first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a <code>DataLoader</code>, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 15\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader is typically used in a for-in loop. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 91.,  88.,  64.]])\n",
      "tensor([[103., 119.],\n",
      "        [119., 133.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 22.,  37.],\n",
      "        [119., 133.],\n",
      "        [ 56.,  70.],\n",
      "        [ 22.,  37.],\n",
      "        [ 81., 101.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data, with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, which can lead to faster reduction in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define some layer - nn.Linear (same as Dense)\n",
    "\n",
    "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4643, -0.0613,  0.0341],\n",
      "        [-0.5722, -0.0392, -0.0141]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([-0.5541, -0.2320], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define model\n",
    "model = nn.Linear(3, 2)  #nn.Linear assume this shape (in_features, out_features)\n",
    "print(model.weight)\n",
    "print(model.weight.size()) # (out_features, in_features)\n",
    "print(model.bias)\n",
    "print(model.bias.size()) #(out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our model is simply a function that performs a matrix multiplication of the <code>inputs</code> and the weights <code>w</code> and adds the bias <code>b</code> (for each observation)\n",
    "\n",
    "<img src = \"figures/dot.png\" width=\"400\">\n",
    "\n",
    "PyTorch models also have a helpful <code>.parameters</code> method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4643, -0.0613,  0.0341],\n",
       "         [-0.5722, -0.0392, -0.0141]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5541, -0.2320], requires_grad=True)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())  #model.param returns a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the <code>model(tensor)</code> API to perform a forward-pass that generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-37.0935, -45.2361],\n",
       "        [-46.0237, -56.6558],\n",
       "        [-47.1900, -56.0868],\n",
       "        [-49.2928, -60.8026],\n",
       "        [-36.0941, -44.4668],\n",
       "        [-37.0935, -45.2361],\n",
       "        [-46.0237, -56.6558],\n",
       "        [-47.1900, -56.0868],\n",
       "        [-49.2928, -60.8026],\n",
       "        [-36.0941, -44.4668],\n",
       "        [-37.0935, -45.2361],\n",
       "        [-46.0237, -56.6558],\n",
       "        [-47.1900, -56.0868],\n",
       "        [-49.2928, -60.8026],\n",
       "        [-36.0941, -44.4668]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define loss function\n",
    "\n",
    "The <code>nn</code> module contains a lot of useful loss function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_mse = nn.MSELoss()\n",
    "criterion_softmax_cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18702.5645, grad_fn=<MseLossBackward>)\n",
      "18702.564453125\n"
     ]
    }
   ],
   "source": [
    "mse = criterion_mse(preds, targets)\n",
    "print(mse)\n",
    "print(mse.item())  ##print out the loss number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define the optimizer\n",
    "\n",
    "*Learning rate and momentum*\n",
    "\n",
    "Instead of manually manipulating the model's weights & biases using gradients, we can use <code>torch.optim</code> API.  We can use <code>optim.SGD</code> to perform stochastic gradient descent where samples are selected in batches (often with random shuffling) instead of as a single group.\n",
    "\n",
    "Note that model.parameters() is passed as an argument to optim.SGD, so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a bit about other techniques we have used before and how they look like in PyTorch.  Since this is simple linear regression problem, we only use learning rate and momentum, but it's worth to talk it here and perhaps we can see the code in the neural network section\n",
    "\n",
    "*Weight initialization*\n",
    "\n",
    "We donâ€™t need to worry about weight initialization at all: the weights in most PyTorch operations involving parameters, including nn.Linear, are automatically scaled based on the size of the layer.\n",
    "\n",
    "*Dropout*\n",
    "\n",
    "Dropout is similarly easy. Just as PyTorch has a built-in Module <code>nn.Linear(n_in, n_out)</code> that computes the operations of a Dense layer from before, the Module nn.Dropout(dropout_prob) implements the Dropout operation, with the caveat that the probability passed in is by default the probability of dropping a given neuron.\n",
    "\n",
    "*Learning rate decay*\n",
    "\n",
    "PyTorch has an <code>lr_scheduler</code> class that can be used to decay the learning rate over the epochs. The key import you need to get started is <code>from torch.optim import lr_scheduler</code>   We shall use it in the latter section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Train the model\n",
    "\n",
    "We are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n",
    "\n",
    "1. Forward pass\n",
    "2. Calculate the loss\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "                    \n",
    "            # 1. Forward pass\n",
    "            pred = model(xb)\n",
    "                      \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Backward and optimize\n",
    "            opt.zero_grad()  #if not, the gradients will accumlate\n",
    "            loss.backward()\n",
    "            \n",
    "            # Print out the gradients.\n",
    "            #print ('dL/dw: ', model.weight.grad) \n",
    "            #print ('dL/db: ', model.bias.grad)\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    "- We use the data loader defined earlier to get batches of data for every iteration.\n",
    "- Instead of updating parameters (weights and biases) manually, we use <code>opt.step</code> to perform the update, and <code>opt.zero_grad</code> to reset the gradients to zero.\n",
    "- We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. loss.item returns the actual value stored in the loss tensor.\n",
    "\n",
    "Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 596.5170\n",
      "Epoch [20/100], Loss: 570.6320\n",
      "Epoch [30/100], Loss: 1023.7664\n",
      "Epoch [40/100], Loss: 547.9646\n",
      "Epoch [50/100], Loss: 145.3096\n",
      "Epoch [60/100], Loss: 14.6898\n",
      "Epoch [70/100], Loss: 0.7309\n",
      "Epoch [80/100], Loss: 3.6541\n",
      "Epoch [90/100], Loss: 3.0494\n",
      "Epoch [100/100], Loss: 1.4302\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, criterion_mse, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5906363129615784\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "loss = criterion_mse(preds, targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Neural Network\n",
    "\n",
    "Let's load the MNIST dataset.  Our architecture is simple:\n",
    "\n",
    "1. Input layer receiving 784 features\n",
    "2. Hidden layer with size of 89 neurons\n",
    "3. Output layer with size of 10 neurons\n",
    "\n",
    "We will be using Sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "# cuda refers to any NVIDIA GPU that you can use to run your code\n",
    "# it will be much faster\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 89\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a fully-connected neural network with one hidden layer.  Actually, you can use nn.Sequential to easily do this.  I will be showing you how to do this using a class way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()  #super(Model, self)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the model using the class.  Every <code>nn.Module</code> can also use the <code>.to(device)</code> to fully use the GPU capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the Loss and optimizer.\n",
    "\n",
    "Here we will be using Adam which is an adaptive learning rate optimization.  Comparing Adam and SGD, Adam is more adaptive in terms of how it uses momentum and learning rate.  Namely, Adam uses the **squared gradients to scale the learning rate** and it takes advantage of momentum by using **moving average of the gradient** instead of gradient itself like SGD with momentum\n",
    "\n",
    "Whether Adam vs. SGD is still very debatable. Adam is proposed in 2015 to great success and many recent papers found that SGD can be more generalized than Adam...so I really don't know.  It's best to try both, I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  #this is softmax indeed\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 1.1100\n",
      "Epoch [1/5], Step [200/600], Loss: 0.4848\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3929\n",
      "Epoch [1/5], Step [400/600], Loss: 0.3154\n",
      "Epoch [1/5], Step [500/600], Loss: 0.4807\n",
      "Epoch [1/5], Step [600/600], Loss: 0.3348\n",
      "Epoch [2/5], Step [100/600], Loss: 0.3484\n",
      "Epoch [2/5], Step [200/600], Loss: 0.2812\n",
      "Epoch [2/5], Step [300/600], Loss: 0.2330\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2405\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1494\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2341\n",
      "Epoch [3/5], Step [100/600], Loss: 0.2532\n",
      "Epoch [3/5], Step [200/600], Loss: 0.2039\n",
      "Epoch [3/5], Step [300/600], Loss: 0.2707\n",
      "Epoch [3/5], Step [400/600], Loss: 0.2939\n",
      "Epoch [3/5], Step [500/600], Loss: 0.1378\n",
      "Epoch [3/5], Step [600/600], Loss: 0.2070\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-7664081dc4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#for printing purpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#images shape is [100, 1, 28, 28]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)  #for printing purpose\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        \n",
    "        #images shape is [100, 1, 28, 28]\n",
    "        \n",
    "        # Move tensors to the configured device\n",
    "        # also reshape to [100, 784] so it can be inputted into the Dense layer\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "                \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  #returns max value, indices\n",
    "        total += labels.size(0)  #keep track of total\n",
    "        correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'models/dense-mnist.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Here we will be exploring how to use pyTorch for CNN.\n",
    "\n",
    "We will also be exploring more typical CNN architectures, as opposed to only simple one conv layer that we use earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical CNN architectures:\n",
    "\n",
    "1. **Conv2d**. Definitely all CNN will put some Conv layer.  **Number of channels** are often a power of 2, because this often results in more efficient processing. The more channels you have, the greater granularity you give to the network.   Also, for **filter size**, typically it will be 3 or 5, where in general, smaller filter size gives better results, while larger filer size speed up the training.  For image input, it is desirable to have **squared images** because they are easy to work with the squared filter.  In the case with non-squared images, we can use the squared-filter to extract square patches anyway.  As for **stride**, it is most common to use stride sizes of 1 in most settings.  At most, we use 2 but no more higher.  **Padding** is selected using the formula $(filtersize - 1)/2$ to maintain same spatial footprint.   To know the output size yield by the Conv2d layer, we can use the equation:\n",
    "\n",
    "$$ \\frac{W - F + 2P}{S} + 1 $$\n",
    "\n",
    "where W is the img width, F is the filter size, P is the padding, and S is the stride\n",
    "\n",
    "In this case (code below), if our W is 28, F is 5, P is 2, and S is 1 then the final img size is\n",
    "\n",
    "$$ \\frac{28 - 5 + 2*2}{1} + 1 = 28. $$\n",
    "\n",
    "which maintains the same image size, coincidentally!\n",
    "\n",
    "2. **BatchNorm**. As you add more layers, one key issue is **vanishing gradients and exploding gradients**.  Another one is **covariate shift**.  We have already talked about **vanishing gradients**, in which if the gradient is less than 1, as it backpropagates, these gradients drop off exponentially.  We know we can fix this by using non-saturating activation function like ReLu where its gradients are not small and does not disappear after some number. Likewise, if our gradient is greater than 1, as it backpropagtes, these gradients increase exponentially.  This is called **exploding gradients**. As for **covariate shift**, if you are training a neural network of \"red watermelon\", but then your model won't perform as well when training for \"green watermelon\".  Why?  Because the input distribution is different.  This is known as **covariate shift**.  All of these problems can be addressed effectively by batch normalization.  The idea is that even the values change from input layers to hidden layers, their mean and standard deviation remain the same, thus, reducing **covariate shift**.  Of course, since the values are kept small, this actually help solve **exploding gradients**.  In a nutshell, it reduce the influence of earlier layers to latter layers, hence, allow each layer to learn more independently.  This allows latter layers to be more stable and thus allow training to reach convergence much faster.  **Phew...in a super more simple sense, batch normalization keeps the neural network more stable and keep changes more regular**   Since batchnorm is simply a normalization procedure, the output size does not change from input size\n",
    "\n",
    "3. **ReLu**. Convolution operation works well with ReLu activation.  ReLu has many advantages.  It is faster to compute, and its derivative is also fast to compute.  It has non-saturation of gradient (which mean the gradient does not vanish) since its derivative is 1.  Since relu is simply an one-to-one mapping, the output size does not change from input size.\n",
    "\n",
    "4. **Max-pooling**. Back then, convolution operation typically use with max-pooling.  Aside from reduction of image size, it also help extract sharpest features of an image as for max pooling, and extract smooth features for average pooling.   Anyhow, as we said in earlier lesson, max pooling works somehow but there are also experiments showing worse results.  You may want to replace max-pooling with a convolution layer with stride of 2 and see whether it change anything.  To know the resulting output size, since pooling is actually a filter itself, we can use the same formula:\n",
    "\n",
    "$$ \\frac{W - F + 2P}{S} + 1 $$\n",
    "\n",
    "In this case, if our W is 28, our pooling filter size is 2, padding is 0, and stride is 2, thus the resulting image size is:\n",
    "\n",
    "$$ \\frac{28 - 2}{2} + 1 = 14 $$\n",
    "\n",
    "Thus the resulting image size is 14\n",
    "\n",
    "5. **Dropout**.  This is similar to what we have discussed in the previous class so I will not talk too much here. But I guess I should mention that the typical best practices is to choose the probability to be between 0.2 to 0.5 for percentage of values to be zeroed down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #in_channel = 1\n",
    "            #out_channel = 16\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #after layer 1 will be of shape [100, 16, 14, 14]\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #after layer 2 will be of shape [100, 32, 7, 7]\n",
    "        self.fc = nn.Linear(32*7*7, num_classes)\n",
    "        self.drop_out = nn.Dropout(p=0.2)  #zeroed 0.2% data\n",
    "        #after fc will be of shape [100, 10]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shape: [batch, in_channel, img_width, img_height]\n",
    "        #[100, 1, 28, 28]\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop_out(out)\n",
    "        #after layer 1: shape: [100, 16, 14, 14]\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out(out)\n",
    "        #after layer 2: shape: [100, 32, 7, 7]\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #after squeezing: shape: [100, 1568]\n",
    "        #we squeeze so that it can be inputted into the fc layer\n",
    "        out = self.fc(out)\n",
    "        #after fc layer: shape: [100, 10]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #con2d expects (batch, channel, width, height)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #print(images.size())\n",
    "        #print(labels.size())\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions  Same code.\n",
    "\n",
    "In previous class, we talk about inference mode in which real prediction happens. In such mode, usually, we do not apply dropout or we apply more generalized algorithm.  Anyhow, in pytorch, we do not need to manually code this, we simply do <code>model.eval()</code> to signal that the model will be run in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'models/cnn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "Here we will be exploring how to use pyTorch for RNN.\n",
    "\n",
    "Just like CNN, we will be exploring typical RNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next in your adventure?\n",
    "\n",
    "So this is our last lecture....sadly!\n",
    "\n",
    "We have come a long way?  Can you remember the \"you\" four months ago and now?  Did you learn a lot? I hope so!   I hope my scratch series help you better understand things from the first principle.\n",
    "\n",
    "In fact, I have only **covered the basic things**.  Seriously, I have barely scratched the surface.  Dense for structured data, CNN for images, RNN for sequence data.  They are all concepts that have been known for almost 10 years.   These are the following things that you probably want to keep studying.  If you are doing thesis, you are likely going to come across these techniques; CNN or RNN won't suffice for your thesis:\n",
    "\n",
    "1. **Generative Adversarial Network**: would it be cool if you give the AI a picture of Mona-Lisa and the AI can learn to draw similar picture?  This is the network containing a **generator** model and **discriminative** model.  It is trying to understand the distribution of the images, and try to generate a fake image with the **generator** where the **discriminative** model trying to distinguish whether its fake or real.  By going back and forth, you can create many similar images/signals/text or restore loss data.  It is not only limited to images by the way.   \n",
    "\n",
    "2. **Transfer Learning**: training data is a lengthy process.  Transfer learning is a technique related to understanding how we can use some pre-trained model and apply it to another totally different distribution *without* training again.  Mostly, it has to deal with shifting and transferring the *distribution*\n",
    "\n",
    "3. **Autoencoder**: neural network is not only limited to supervised learning.  We can also do unsupervised learning by creating a **encoder** which squeeze things into some middle form and a **decoder** that transform the middle form back to the original one.  Since the middle form is the best way to represent the data in a diminished form, we can say that the middle form actually holds **most important** information about the data so that it can be decoded to data that is as close to the original.   Autoencoder has been useful for noise cancelation, outlier detection, and even prediction task (just like how PCA can improve prediction accuracy)\n",
    "\n",
    "4. **Neural style transfer**: Given a picture of Chaky, and another picture of some comic, let's say Harry Potter, would it be cool to draw Chaky in a Harry Potter way?  This is called **style transfer** in which the network tries to learn the style from the comic and apply to Chaky image.\n",
    "\n",
    "5. **Transformers**.  This is a very recent trend.  Back then, if we are talking about sequential data, we would talk about LSTM, GRU, or TCN.  But right now, Transformers which is based on Attention-Mechanisms have shown great results and has been used to improve accuracy in NLP, and signal processing.  Some NLP pre-trained model like BERT or GBT2 using Transformers can perform really well in detecting fake news, fake reviews, or even write own reviews.\n",
    "\n",
    "Next week onward, some of my Ph.D./Master students will be coming to give their talk on some of these topics and how they apply to their brain/data science research.  This is a good time to take a glimpse how thesis looks like.  Please spare them some slack because they are also learning just like you, but I am sure they have plenty good things to share with you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
