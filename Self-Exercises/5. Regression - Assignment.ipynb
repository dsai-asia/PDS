{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load boston setting X as boston.data and y as boston.target\n",
    "\n",
    "- Attempt the grid search using polyregression + (linear, ridge, lasso, elastic net) \n",
    "\n",
    "- Does feature mechanisms on ridge/lasso/elastic helps here?\n",
    "\n",
    "- What is the optimal polynomial degree?  What does it mean?\n",
    "\n",
    "- Why do you think the result is like this?\n",
    "\n",
    "- What is the value of lambdas, and what does it means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use below data\n",
    "- Perform Ridge regression\n",
    "- Plot training and validation errors as function of lambda\n",
    "- Plot coefficients and coefficients error as function of lambda\n",
    "    - coefficients can be obtained simply using model.coef_\n",
    "    - coeffcient error can be computed using mean_squared_error(model.coef_, w)\n",
    "- Interpret what is happening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#only 10 out of 100 features are informative, add some noise to add to the difficulty of the problem\n",
    "X, y, coef = make_regression(n_samples = 1000, n_features = 100, coef=True,\n",
    "                         random_state=42, bias=6, noise=50, n_informative=10)\n",
    "\n",
    "#scaling does not really help with simple linear regression\n",
    "#since the coefficients can be multiply to certain order but\n",
    "#with same result.  However, regularized models will be\n",
    "#affected.  The idea is that the constraint is applied to the\n",
    "#sum of a function of coefficients.  If we inflate an attribute,\n",
    "#the coefficient will be deflated, which will affect\n",
    "#the penalization.  Thus it is best to scale for all regression\n",
    "#problems since it does not hurt\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using boston data, compare \n",
    "\n",
    "- OLS from scratch\n",
    "- pseudo OLS from scratch\n",
    "- gradient descent from scratch\n",
    "- LinearRegression() by sklearn  \n",
    "\n",
    "Measure which one is faster.  Try to vary the n_features.\n",
    "\n",
    "So what do you think, closed_form or gradient descent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement Ridge regression from scratch using \n",
    "\n",
    " - the closed form OLS\n",
    "\n",
    " - stochastic gradient descent\n",
    "\n",
    " - SGDRegressor() of sklearn using penalty as l2\n",
    "\n",
    " - Ridge() by sklearn\n",
    "\n",
    "Loop through several lambda and \n",
    " - Print the MSE\n",
    " - Compare the time as well\n",
    "\n",
    "Use this below data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_samples = 1000, n_features = 100, coef=True,\n",
    "                         random_state=42, bias=6, noise=10, n_informative=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
